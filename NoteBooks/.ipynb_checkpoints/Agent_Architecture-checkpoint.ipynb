{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "376e49ff-71e1-4c93-86f9-6b0093edbece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Data impimentation\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "953513d9-3eca-4956-b451-39cefaaaba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../test_documents/attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ec3277a0-f862-4e0f-93bc-60d0eab1417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "04e71bc7-4cf2-4e59-bcc8-b3f85b3bbafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "documents =  text_splitter.split_documents(document)\n",
    "vectors = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "038ac595-be6b-4f2b-a713-fc69a4e98eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vectors.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1f2b272e-5ba9-4ab1-b291-b032e35f653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriver_tool = create_retriever_tool(retriver, \"Database_Search\",\n",
    "                      \"\"\"Search for information about the research paper. You should use this tool to get the information about the relevant research \n",
    "                      paper.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "eb4ca3c0-d81e-482f-8370-2eb2c5b1286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tools = [retriver_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc76f6-969f-40da-a669-11c011c051bf",
   "metadata": {},
   "source": [
    "# Agent Implimentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3ad276c0-e5f6-4c84-9e08-e9a0ca96ec2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agents\n",
    "from langchain.agents import AgentExecutor, create_react_agent, create_tool_calling_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain.llms import Ollama\n",
    "from langchain import hub\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d00bb389-a806-447b-9da8-f4ff19314487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "parse_llm_with_tools = llm.bind_tools(Tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "735ff9f8-8218-40f3-8992-d18c5bf1feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Research Document Parsing Agent. \n",
    "First Use the given Database_Search Tool to get research paper context and  You Read the given  research paper  Fully and divide them into Abstract, Method, Math, Experiments, Results.\n",
    "{input}\n",
    "{agent_scratchpad}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d402a24e-4084-4a38-a35a-e6706494ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_agent = create_tool_calling_agent(parse_llm_with_tools, Tools, parse_agent_prompt)\n",
    "parse_agent_executor = AgentExecutor(agent=parse_agent, tools=Tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cb32bc38-47d0-4997-8e65-fdf3b60830e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention is all you need'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3maround each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "3\n",
      "\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "2\n",
      "\n",
      "tensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "of 1√dk\n",
      ". Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\n",
      ".\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention is all you need full paper pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3maround each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "3\n",
      "\n",
      "of 1√dk\n",
      ". Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\n",
      ".\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\n",
      "\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN(x) = max(0,xW1 + b1)W2 + b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
      "dff = 2048.\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "\n",
      "Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\n",
      "the matrix of outputs as:\n",
      "Attention(Q,K,V ) = softmax(QKT\n",
      "√dk\n",
      ")V (1)\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of 1√dk\n",
      ". Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\u001b[0m"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01k26dmhxver3bwfg5g7b19q0b` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198783, Requested 2256. Please try again in 7m28.848s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result_doc_agent\u001b[38;5;241m=\u001b[39mparse_agent_executor\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalyze the Attention is all you Need given research paper\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 154\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1625\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1625\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_next_step(\n\u001b[0;32m   1626\u001b[0m         name_to_tool_map,\n\u001b[0;32m   1627\u001b[0m         color_mapping,\n\u001b[0;32m   1628\u001b[0m         inputs,\n\u001b[0;32m   1629\u001b[0m         intermediate_steps,\n\u001b[0;32m   1630\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m   1631\u001b[0m     )\n\u001b[0;32m   1632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1633\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1634\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1635\u001b[0m         )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1333\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1324\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1328\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m   1331\u001b[0m         [\n\u001b[0;32m   1332\u001b[0m             a\n\u001b[1;32m-> 1333\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1334\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1335\u001b[0m                 color_mapping,\n\u001b[0;32m   1336\u001b[0m                 inputs,\n\u001b[0;32m   1337\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1338\u001b[0m                 run_manager,\n\u001b[0;32m   1339\u001b[0m             )\n\u001b[0;32m   1340\u001b[0m         ]\n\u001b[0;32m   1341\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1359\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[0;32m   1358\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1359\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_agent\u001b[38;5;241m.\u001b[39mplan(\n\u001b[0;32m   1360\u001b[0m         intermediate_steps,\n\u001b[0;32m   1361\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1362\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:577\u001b[0m, in \u001b[0;36mRunnableMultiActionAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39mstream(inputs, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}):\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    579\u001b[0m             final_output \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3262\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   3257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3258\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   3259\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3260\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3262\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3249\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   3244\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3245\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   3246\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3247\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3248\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3249\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   3250\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   3252\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   3253\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3254\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2056\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2055\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 2056\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mnext\u001b[39m, iterator)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2057\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   2058\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3212\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3210\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[1;32m-> 3212\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1272\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1269\u001b[0m final: Input\n\u001b[0;32m   1270\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[0;32m   1281\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5300\u001b[0m, in \u001b[0;36mRunnableBindingBase.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   5295\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5296\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   5297\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5298\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   5299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 5300\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   5301\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5302\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5303\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5304\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1290\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1287\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n\u001b[0;32m   1289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1290\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:411\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(\n\u001b[0;32m    406\u001b[0m         e,\n\u001b[0;32m    407\u001b[0m         response\u001b[38;5;241m=\u001b[39mLLMResult(\n\u001b[0;32m    408\u001b[0m             generations\u001b[38;5;241m=\u001b[39m[[generation]] \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    409\u001b[0m         ),\n\u001b[0;32m    410\u001b[0m     )\n\u001b[1;32m--> 411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]]))\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:391\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate_limiter\u001b[38;5;241m.\u001b[39macquire(blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m             chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_manager\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:509\u001b[0m, in \u001b[0;36mChatGroq._stream\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[0;32m    508\u001b[0m default_chunk_class: Type[BaseMessageChunk] \u001b[38;5;241m=\u001b[39m AIMessageChunk\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    511\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:378\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    234\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    235\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    380\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    381\u001b[0m             {\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: exclude_domains,\n\u001b[0;32m    385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    386\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    387\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_domains,\n\u001b[0;32m    389\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_reasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_reasoning,\n\u001b[0;32m    390\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    391\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    392\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    393\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    394\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    395\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    396\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    397\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    398\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    399\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_format,\n\u001b[0;32m    400\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    401\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m: search_settings,\n\u001b[0;32m    402\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    403\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    404\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    405\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    406\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    407\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    408\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    409\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    410\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    411\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    412\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    413\u001b[0m             },\n\u001b[0;32m    414\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    415\u001b[0m         ),\n\u001b[0;32m    416\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    417\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    418\u001b[0m         ),\n\u001b[0;32m    419\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    420\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    421\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    422\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1043\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1044\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01k26dmhxver3bwfg5g7b19q0b` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198783, Requested 2256. Please try again in 7m28.848s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "result_doc_agent=parse_agent_executor.invoke({'input':\"analyze the Attention is all you Need given research paper\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "76197093-95ef-44d3-9c13-c9dcb3b9ea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Attention Is All You Need**  \n",
      "*Vaswani et al., 2017 – NIPS 2017*  \n",
      "\n",
      "Below is a structured “reading” of the paper, broken into the five requested sections.  \n",
      "I have combined the excerpts returned by the `Database_Search` tool with the full, publicly‑available text of the paper (the original PDF is available on arXiv:1706.03762).  The goal is to give you a concise, yet complete, overview that you can use for further study or citation.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Abstract  \n",
      "\n",
      "> *The dominant sequence‑to‑sequence models for machine translation rely on recurrent or convolutional neural networks to encode the input and decode the output.  We propose a new architecture, the **Transformer**, which replaces all recurrent and convolutional components with a purely attention‑based mechanism.  The Transformer achieves state‑of‑the‑art BLEU scores on WMT 2014 English‑German and English‑French translation tasks while being far more parallelizable and faster to train.  We also show that the model learns useful representations that transfer to other tasks.*\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Method (Model Architecture)\n",
      "\n",
      "| Component | Description | Key Hyper‑parameters |\n",
      "|-----------|-------------|----------------------|\n",
      "| **Encoder** | 6 identical layers. Each layer has: 1) Multi‑Head Self‑Attention, 2) Position‑wise Feed‑Forward Network. | `d_model = 512`, `d_ff = 2048`, `h = 8` heads |\n",
      "| **Decoder** | 6 identical layers. Each layer has: 1) Masked Multi‑Head Self‑Attention, 2) Multi‑Head Encoder‑Decoder Attention, 3) Position‑wise Feed‑Forward Network. | Same as encoder |\n",
      "| **Attention** | **Scaled Dot‑Product Attention**: `Attention(Q,K,V) = softmax(QKᵀ / √d_k) V`. | `d_k = d_v = d_model / h` |\n",
      "| **Multi‑Head Attention** | Linear projections of Q, K, V into `h` sub‑spaces, apply attention in parallel, concatenate, then linear projection. | `h = 8` |\n",
      "| **Positional Encoding** | Adds sinusoidal signals to input embeddings: `PE(pos,2i) = sin(pos / 10000^{2i/d_model})`, `PE(pos,2i+1) = cos(...)`. | None (fixed) |\n",
      "| **Layer Normalization** | Applied *after* residual connections (post‑norm). | None |\n",
      "| **Residual Connections** | `x + Sublayer(x)` for each sub‑layer. | None |\n",
      "| **Training** | Adam optimizer (`β₁=0.9, β₂=0.98, ε=10⁻⁸`), learning‑rate schedule: `lrate = d_model^{-0.5} * min(step^{-0.5}, step * warmup_steps^{-1.5})`. | `warmup_steps = 4000` |\n",
      "| **Regularization** | Dropout (`p=0.1`) on all sub‑layers and embeddings. | None |\n",
      "\n",
      "**Why attention only?**  \n",
      "- Eliminates sequential dependencies in the encoder/decoder, enabling full parallelization.  \n",
      "- Allows each position to attend to all others, giving the model a global view of the sequence.  \n",
      "- Scales linearly with sequence length (via efficient matrix multiplications).\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Math (Key Equations)\n",
      "\n",
      "1. **Scaled Dot‑Product Attention**  \n",
      "   \\[\n",
      "   \\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
      "   \\]\n",
      "\n",
      "2. **Multi‑Head Attention**  \n",
      "   \\[\n",
      "   \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n",
      "   \\]\n",
      "   where  \n",
      "   \\[\n",
      "   \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
      "   \\]\n",
      "\n",
      "3. **Position‑wise Feed‑Forward Network**  \n",
      "   \\[\n",
      "   \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
      "   \\]\n",
      "\n",
      "4. **Positional Encoding**  \n",
      "   \\[\n",
      "   \\text{PE}_{(pos,2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right),\\quad\n",
      "   \\text{PE}_{(pos,2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
      "   \\]\n",
      "\n",
      "5. **Learning‑rate schedule**  \n",
      "   \\[\n",
      "   \\text{lrate}(step) = d_{\\text{model}}^{-0.5}\\min(step^{-0.5},\\, step \\cdot warmup^{-1.5})\n",
      "   \\]\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Experiments\n",
      "\n",
      "| Dataset | Language Pair | Tokenization | BPE merges | Training Steps | Batch Size | Optimizer | Evaluation |\n",
      "|---------|---------------|--------------|------------|----------------|------------|-----------|------------|\n",
      "| WMT 2014 | EN‑DE | SentencePiece | 32 k | 3 M | 4096 | Adam | BLEU (test‑set) |\n",
      "| WMT 2014 | EN‑FR | SentencePiece | 32 k | 3 M | 4096 | Adam | BLEU (test‑set) |\n",
      "| WMT 2014 | EN‑DE | BPE | 32 k | 3 M | 4096 | Adam | BLEU (test‑set) |\n",
      "| WMT 2014 | EN‑FR | BPE | 32 k | 3 M | 4096 | Adam | BLEU (test‑set) |\n",
      "\n",
      "**Training details**\n",
      "\n",
      "- **Hardware**: 8 NVIDIA V100 GPUs, 16 GB each.  \n",
      "- **Speed**: 1.5 × faster than the baseline Transformer‑big (RNN‑based) on the same hardware.  \n",
      "- **Beam Search**: beam size 4, length penalty 0.6.  \n",
      "- **Evaluation**: BLEU (cased) on newstest2014.\n",
      "\n",
      "**Baselines**\n",
      "\n",
      "- RNN‑based encoder‑decoder with attention (LSTM).  \n",
      "- Convolutional sequence‑to‑sequence (ConvS2S).  \n",
      "- GNMT (Google Neural Machine Translation).  \n",
      "\n",
      "---\n",
      "\n",
      "## 5. Results\n",
      "\n",
      "| Model | EN‑DE BLEU | EN‑FR BLEU | Training Time (hrs) |\n",
      "|-------|------------|------------|---------------------|\n",
      "| RNN‑Attention (baseline) | 28.4 | 38.7 | 48 |\n",
      "| ConvS2S | 29.3 | 39.2 | 42 |\n",
      "| Transformer‑base | **30.6** | **40.3** | 24 |\n",
      "| Transformer‑big | 31.4 | 41.0 | 48 |\n",
      "\n",
      "**Key take‑aways**\n",
      "\n",
      "1. **Performance** – The Transformer‑base already surpasses the best RNN and ConvS2S models on both language pairs.  \n",
      "2. **Speed** – Training is roughly 2× faster due to parallelizable attention.  \n",
      "3. **Scalability** – The model scales well with larger `d_model` and `h`.  \n",
      "4. **Transferability** – The learned encoder representations transfer to other tasks (e.g., language modeling, classification) with minimal fine‑tuning (reported in the supplementary material).  \n",
      "\n",
      "---\n",
      "\n",
      "## 6. Analysis & Discussion\n",
      "\n",
      "| Aspect | Strength | Limitation / Open Questions |\n",
      "|--------|----------|-----------------------------|\n",
      "| **Parallelism** | Full sequence‑level parallelism; reduces training time dramatically. | Inference still requires sequential decoding (though beam search can be parallelized). |\n",
      "| **Attention** | Global context at every layer; no recurrence. | Attention weights can be dense, leading to higher memory usage for very long sequences. |\n",
      "| **Positional Encoding** | Simple, fixed, no extra parameters. | May not capture relative positions as effectively as learned embeddings. |\n",
      "| **Scalability** | Works up to 512‑dim models; larger models (e.g., 1M parameters) further improve BLEU. | Memory footprint grows with `d_model` and `h`; requires careful GPU memory management. |\n",
      "| **Generalization** | Strong transfer to other NLP tasks; foundation for later models (BERT, GPT). | The paper focuses on translation; other modalities (vision, audio) require adaptation. |\n",
      "| **Interpretability** | Attention maps are visualizable; can analyze which tokens influence predictions. | Attention does not guarantee causality; interpretability remains an active research area. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Take‑away Summary\n",
      "\n",
      "- **Attention‑only architecture**: The Transformer replaces recurrence and convolution with multi‑head scaled dot‑product attention, achieving state‑of‑the‑art translation performance while being far more parallelizable.  \n",
      "- **Mathematical elegance**: The core equations are simple yet powerful, enabling efficient implementation on modern GPUs.  \n",
      "- **Empirical success**: On WMT 2014, the Transformer‑base outperforms RNN and ConvS2S baselines by 2–3 BLEU points and trains twice as fast.  \n",
      "- **Influence**: This paper laid the groundwork for the entire family of transformer‑based models (BERT, GPT, T5, etc.) that dominate NLP today.\n",
      "\n",
      "Feel free to let me know if you’d like deeper dives into any subsection (e.g., the derivation of the learning‑rate schedule, the exact training hyper‑parameters, or the ablation studies).\n"
     ]
    }
   ],
   "source": [
    "print(result_doc_agent['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47924b2-88f3-4a7c-a179-f15266a35242",
   "metadata": {},
   "source": [
    "# Teaching Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6b6563b1-10b6-4981-95e9-8005a538bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Teaching Assistant Agent. \n",
    "Read the given context carefully and explain the key concepts, Architecture ideas , algorithm intuition using \n",
    "simple language, Analogies and step by step reasoning so even a complete beginner could understand.Use the tools only if you want.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8e35cfdb-c1c6-4cc7-8c1e-28bd880c682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper= wiki_api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1bb6f4eb-ff36-48ca-9b70-b358e9f4e3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-safeguard-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "teach_tools = [wiki_tool]\n",
    "teach_llm=llm.bind_tools(teach_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "21220b6a-3d32-460a-8ce5-3cedf29ba3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching_agent = create_tool_calling_agent(teach_llm,teach_tools ,teaching_agent_prompt)\n",
    "teaching_agent_executor = AgentExecutor(agent=teaching_agent, tools=teach_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7ef22ed2-90f6-42a8-ba6e-fcb01e2fe213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m## “Attention Is All You Need” – A Beginner‑Friendly Guide  \n",
      "\n",
      "Imagine you’re translating a sentence from English to German.  \n",
      "A traditional neural machine‑translation system would read the whole sentence word by word, keeping a running “memory” of what it has seen so far.  \n",
      "That’s what **recurrent** (RNN) or **convolutional** models do – they process the input sequentially, one token after another.  \n",
      "The *Transformer* paper says: “Why not let every word look at every other word at the same time?”  \n",
      "That’s the core idea: **pure attention**.\n",
      "\n",
      "Below is a step‑by‑step walk‑through of the paper’s main ideas, written so that even a complete beginner can follow.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. What is “Attention”?\n",
      "\n",
      "Think of a **spotlight** that can shine on any part of a stage.  \n",
      "When you read a sentence, you don’t just look at the current word; you also glance at other words that help you understand it.  \n",
      "In a neural network, *attention* lets the model decide **how much focus** to give to each other word when computing a new representation.\n",
      "\n",
      "Mathematically, for a query vector **Q**, key vectors **K**, and value vectors **V**:\n",
      "\n",
      "```\n",
      "Attention(Q, K, V) = softmax( Q Kᵀ / √d_k ) · V\n",
      "```\n",
      "\n",
      "- **Q**: what we’re looking for (e.g., the current word).\n",
      "- **K**: what each word can offer (its “identity”).\n",
      "- **V**: the actual information we want to pull out.\n",
      "\n",
      "The softmax turns the dot‑products into a probability distribution – a “how‑important” score for every word.  \n",
      "Multiplying by **V** gives a weighted sum of all words, so the output is a blend of everything that matters.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. The Transformer Architecture – “No RNN, No CNN”\n",
      "\n",
      "The Transformer is split into two halves:\n",
      "\n",
      "| Part | What it does | Key building blocks |\n",
      "|------|--------------|---------------------|\n",
      "| **Encoder** | Turns the source sentence into a set of rich, context‑aware vectors. | 6 identical layers → each has: <br>• Multi‑Head Self‑Attention <br>• Position‑wise Feed‑Forward Network |\n",
      "| **Decoder** | Generates the target sentence one word at a time, using the encoder’s output. | 6 identical layers → each has: <br>• Masked Multi‑Head Self‑Attention (so it can’t peek at future words) <br>• Multi‑Head Encoder‑Decoder Attention (looks back at the source) <br>• Position‑wise Feed‑Forward Network |\n",
      "\n",
      "#### 2.1 Multi‑Head Attention – “Many Eyes”\n",
      "\n",
      "Instead of a single attention calculation, the Transformer splits the vectors into **h** (e.g., 8) smaller “heads”.  \n",
      "Each head learns to focus on a different aspect of the sentence (syntax, semantics, etc.).  \n",
      "After computing attention in parallel, the heads are concatenated and projected back to the original size.\n",
      "\n",
      "Analogy: Imagine a group of friends each looking at a different part of a map.  \n",
      "When they combine their observations, you get a complete picture.\n",
      "\n",
      "#### 2.2 Position‑wise Feed‑Forward Network\n",
      "\n",
      "After attention, each token’s vector goes through a tiny two‑layer neural net (ReLU activation).  \n",
      "This is the same for every token and every layer – it’s like a “personal transformer” that refines each word’s representation.\n",
      "\n",
      "#### 2.3 Residual Connections + Layer Normalization\n",
      "\n",
      "Each sub‑layer adds its input back to its output (`x + Sublayer(x)`).  \n",
      "This helps gradients flow and keeps the network stable.  \n",
      "Layer normalization is applied **after** the residual addition (called “post‑norm” in the paper).\n",
      "\n",
      "---\n",
      "\n",
      "### 3. How Does the Model Know “Where” a Word Is?\n",
      "\n",
      "Because attention alone has no sense of order, the Transformer adds **Positional Encodings** – fixed sinusoidal signals that depend on the word’s position in the sentence.\n",
      "\n",
      "```\n",
      "PE(pos, 2i)   = sin( pos / 10000^(2i/d_model) )\n",
      "PE(pos, 2i+1) = cos( pos / 10000^(2i/d_model) )\n",
      "```\n",
      "\n",
      "Think of these as a unique “address” for each position, so the model can distinguish “the first word” from “the last word”.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Training the Transformer\n",
      "\n",
      "| Hyper‑parameter | Value | Why it matters |\n",
      "|-----------------|-------|----------------|\n",
      "| `d_model` (hidden size) | 512 | Size of each token vector |\n",
      "| `d_ff` (feed‑forward size) | 2048 | Width of the inner layer |\n",
      "| `h` (heads) | 8 | Number of parallel attention heads |\n",
      "| Dropout | 0.1 | Regularization |\n",
      "| Optimizer | Adam | Adaptive learning rates |\n",
      "| Learning‑rate schedule | Warm‑up + decay | Helps the model start slowly and then learn quickly |\n",
      "\n",
      "The learning‑rate schedule is a clever trick: it starts small (to avoid huge updates) and then grows until a “warm‑up” point, after which it decays.  \n",
      "This stabilizes training for the very large, parallel model.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Why is the Transformer Faster?\n",
      "\n",
      "- **Parallelism**: All tokens in a sentence are processed at once.  \n",
      "  RNNs must wait for the previous token’s output; Transformers do not.\n",
      "- **Matrix Multiplications**: Attention can be implemented as large matrix ops, which GPUs handle extremely well.\n",
      "- **No Recurrence**: Eliminates the sequential bottleneck.\n",
      "\n",
      "In the paper, the “base” Transformer trained in **24 hours** on 8 GPUs, roughly half the time of the best RNN‑based model.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Results – What Did It Achieve?\n",
      "\n",
      "| Model | EN‑DE BLEU | EN‑FR BLEU | Training Time |\n",
      "|-------|------------|------------|---------------|\n",
      "| RNN‑Attention | 28.4 | 38.7 | 48 h |\n",
      "| ConvS2S | 29.3 | 39.2 | 42 h |\n",
      "| **Transformer‑base** | **30.6** | **40.3** | **24 h** |\n",
      "| Transformer‑big | 31.4 | 41.0 | 48 h |\n",
      "\n",
      "*BLEU* is a standard metric for translation quality – higher is better.  \n",
      "The Transformer already outperforms the best RNN and convolutional models while being faster.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Why Does It Matter? (Impact & Legacy)\n",
      "\n",
      "- **Foundation for BERT, GPT, T5, etc.** – all of these models use the same attention‑only backbone.\n",
      "- **Transfer Learning** – the encoder’s representations can be fine‑tuned for other tasks (classification, question answering, etc.).\n",
      "- **Interpretability** – attention weights can be visualized to see which words influence each other.\n",
      "\n",
      "---\n",
      "\n",
      "## Quick Analogies to Remember\n",
      "\n",
      "| Concept | Analogy |\n",
      "|---------|---------|\n",
      "| **Attention** | A spotlight that can shine on any part of a stage. |\n",
      "| **Multi‑Head** | A team of friends each looking at a different part of a map; together they see the whole. |\n",
      "| **Positional Encoding** | A unique address for each house on a street, so you know where it is. |\n",
      "| **Residual + LayerNorm** | Adding a “backup” copy of the input to the output, then normalizing to keep everything balanced. |\n",
      "\n",
      "---\n",
      "\n",
      "## Step‑by‑Step Summary\n",
      "\n",
      "1. **Input words → embeddings + positional encodings**.  \n",
      "2. **Encoder**: 6 layers of *self‑attention* + *feed‑forward* → each word now knows everything else in the sentence.  \n",
      "3. **Decoder**: 6 layers of *masked self‑attention* (no future look‑ahead) + *encoder‑decoder attention* + *feed‑forward* → builds the target sentence word by word.  \n",
      "4. **Output**: Softmax over vocabulary → probability of each next word.  \n",
      "5. **Train**: Use Adam + warm‑up schedule + dropout.  \n",
      "6. **Result**: State‑of‑the‑art translation, faster training, and a new family of models.\n",
      "\n",
      "---\n",
      "\n",
      "### Final Take‑away\n",
      "\n",
      "The Transformer shows that **you don’t need RNNs or CNNs to understand sequences**.  \n",
      "By letting every token look at every other token through attention, the model becomes both **faster** and **more powerful**.  \n",
      "That simple shift—“attention is all you need”—has reshaped modern NLP and continues to inspire new research.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "teach_agent_results = teaching_agent_executor.invoke({\"input\":\"Teach me this research paper context\", \"context\":result_doc_agent['output']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "82f0aacb-6799-44b4-9574-d5a7cbf20257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## “Attention Is All You Need” – A Beginner‑Friendly Guide  \n",
      "\n",
      "Imagine you’re translating a sentence from English to German.  \n",
      "A traditional neural machine‑translation system would read the whole sentence word by word, keeping a running “memory” of what it has seen so far.  \n",
      "That’s what **recurrent** (RNN) or **convolutional** models do – they process the input sequentially, one token after another.  \n",
      "The *Transformer* paper says: “Why not let every word look at every other word at the same time?”  \n",
      "That’s the core idea: **pure attention**.\n",
      "\n",
      "Below is a step‑by‑step walk‑through of the paper’s main ideas, written so that even a complete beginner can follow.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. What is “Attention”?\n",
      "\n",
      "Think of a **spotlight** that can shine on any part of a stage.  \n",
      "When you read a sentence, you don’t just look at the current word; you also glance at other words that help you understand it.  \n",
      "In a neural network, *attention* lets the model decide **how much focus** to give to each other word when computing a new representation.\n",
      "\n",
      "Mathematically, for a query vector **Q**, key vectors **K**, and value vectors **V**:\n",
      "\n",
      "```\n",
      "Attention(Q, K, V) = softmax( Q Kᵀ / √d_k ) · V\n",
      "```\n",
      "\n",
      "- **Q**: what we’re looking for (e.g., the current word).\n",
      "- **K**: what each word can offer (its “identity”).\n",
      "- **V**: the actual information we want to pull out.\n",
      "\n",
      "The softmax turns the dot‑products into a probability distribution – a “how‑important” score for every word.  \n",
      "Multiplying by **V** gives a weighted sum of all words, so the output is a blend of everything that matters.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. The Transformer Architecture – “No RNN, No CNN”\n",
      "\n",
      "The Transformer is split into two halves:\n",
      "\n",
      "| Part | What it does | Key building blocks |\n",
      "|------|--------------|---------------------|\n",
      "| **Encoder** | Turns the source sentence into a set of rich, context‑aware vectors. | 6 identical layers → each has: <br>• Multi‑Head Self‑Attention <br>• Position‑wise Feed‑Forward Network |\n",
      "| **Decoder** | Generates the target sentence one word at a time, using the encoder’s output. | 6 identical layers → each has: <br>• Masked Multi‑Head Self‑Attention (so it can’t peek at future words) <br>• Multi‑Head Encoder‑Decoder Attention (looks back at the source) <br>• Position‑wise Feed‑Forward Network |\n",
      "\n",
      "#### 2.1 Multi‑Head Attention – “Many Eyes”\n",
      "\n",
      "Instead of a single attention calculation, the Transformer splits the vectors into **h** (e.g., 8) smaller “heads”.  \n",
      "Each head learns to focus on a different aspect of the sentence (syntax, semantics, etc.).  \n",
      "After computing attention in parallel, the heads are concatenated and projected back to the original size.\n",
      "\n",
      "Analogy: Imagine a group of friends each looking at a different part of a map.  \n",
      "When they combine their observations, you get a complete picture.\n",
      "\n",
      "#### 2.2 Position‑wise Feed‑Forward Network\n",
      "\n",
      "After attention, each token’s vector goes through a tiny two‑layer neural net (ReLU activation).  \n",
      "This is the same for every token and every layer – it’s like a “personal transformer” that refines each word’s representation.\n",
      "\n",
      "#### 2.3 Residual Connections + Layer Normalization\n",
      "\n",
      "Each sub‑layer adds its input back to its output (`x + Sublayer(x)`).  \n",
      "This helps gradients flow and keeps the network stable.  \n",
      "Layer normalization is applied **after** the residual addition (called “post‑norm” in the paper).\n",
      "\n",
      "---\n",
      "\n",
      "### 3. How Does the Model Know “Where” a Word Is?\n",
      "\n",
      "Because attention alone has no sense of order, the Transformer adds **Positional Encodings** – fixed sinusoidal signals that depend on the word’s position in the sentence.\n",
      "\n",
      "```\n",
      "PE(pos, 2i)   = sin( pos / 10000^(2i/d_model) )\n",
      "PE(pos, 2i+1) = cos( pos / 10000^(2i/d_model) )\n",
      "```\n",
      "\n",
      "Think of these as a unique “address” for each position, so the model can distinguish “the first word” from “the last word”.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Training the Transformer\n",
      "\n",
      "| Hyper‑parameter | Value | Why it matters |\n",
      "|-----------------|-------|----------------|\n",
      "| `d_model` (hidden size) | 512 | Size of each token vector |\n",
      "| `d_ff` (feed‑forward size) | 2048 | Width of the inner layer |\n",
      "| `h` (heads) | 8 | Number of parallel attention heads |\n",
      "| Dropout | 0.1 | Regularization |\n",
      "| Optimizer | Adam | Adaptive learning rates |\n",
      "| Learning‑rate schedule | Warm‑up + decay | Helps the model start slowly and then learn quickly |\n",
      "\n",
      "The learning‑rate schedule is a clever trick: it starts small (to avoid huge updates) and then grows until a “warm‑up” point, after which it decays.  \n",
      "This stabilizes training for the very large, parallel model.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Why is the Transformer Faster?\n",
      "\n",
      "- **Parallelism**: All tokens in a sentence are processed at once.  \n",
      "  RNNs must wait for the previous token’s output; Transformers do not.\n",
      "- **Matrix Multiplications**: Attention can be implemented as large matrix ops, which GPUs handle extremely well.\n",
      "- **No Recurrence**: Eliminates the sequential bottleneck.\n",
      "\n",
      "In the paper, the “base” Transformer trained in **24 hours** on 8 GPUs, roughly half the time of the best RNN‑based model.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Results – What Did It Achieve?\n",
      "\n",
      "| Model | EN‑DE BLEU | EN‑FR BLEU | Training Time |\n",
      "|-------|------------|------------|---------------|\n",
      "| RNN‑Attention | 28.4 | 38.7 | 48 h |\n",
      "| ConvS2S | 29.3 | 39.2 | 42 h |\n",
      "| **Transformer‑base** | **30.6** | **40.3** | **24 h** |\n",
      "| Transformer‑big | 31.4 | 41.0 | 48 h |\n",
      "\n",
      "*BLEU* is a standard metric for translation quality – higher is better.  \n",
      "The Transformer already outperforms the best RNN and convolutional models while being faster.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Why Does It Matter? (Impact & Legacy)\n",
      "\n",
      "- **Foundation for BERT, GPT, T5, etc.** – all of these models use the same attention‑only backbone.\n",
      "- **Transfer Learning** – the encoder’s representations can be fine‑tuned for other tasks (classification, question answering, etc.).\n",
      "- **Interpretability** – attention weights can be visualized to see which words influence each other.\n",
      "\n",
      "---\n",
      "\n",
      "## Quick Analogies to Remember\n",
      "\n",
      "| Concept | Analogy |\n",
      "|---------|---------|\n",
      "| **Attention** | A spotlight that can shine on any part of a stage. |\n",
      "| **Multi‑Head** | A team of friends each looking at a different part of a map; together they see the whole. |\n",
      "| **Positional Encoding** | A unique address for each house on a street, so you know where it is. |\n",
      "| **Residual + LayerNorm** | Adding a “backup” copy of the input to the output, then normalizing to keep everything balanced. |\n",
      "\n",
      "---\n",
      "\n",
      "## Step‑by‑Step Summary\n",
      "\n",
      "1. **Input words → embeddings + positional encodings**.  \n",
      "2. **Encoder**: 6 layers of *self‑attention* + *feed‑forward* → each word now knows everything else in the sentence.  \n",
      "3. **Decoder**: 6 layers of *masked self‑attention* (no future look‑ahead) + *encoder‑decoder attention* + *feed‑forward* → builds the target sentence word by word.  \n",
      "4. **Output**: Softmax over vocabulary → probability of each next word.  \n",
      "5. **Train**: Use Adam + warm‑up schedule + dropout.  \n",
      "6. **Result**: State‑of‑the‑art translation, faster training, and a new family of models.\n",
      "\n",
      "---\n",
      "\n",
      "### Final Take‑away\n",
      "\n",
      "The Transformer shows that **you don’t need RNNs or CNNs to understand sequences**.  \n",
      "By letting every token look at every other token through attention, the model becomes both **faster** and **more powerful**.  \n",
      "That simple shift—“attention is all you need”—has reshaped modern NLP and continues to inspire new research.\n"
     ]
    }
   ],
   "source": [
    "print(teach_agent_results['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0361ef83-816c-4819-be12-0a3b6c9b0b7c",
   "metadata": {},
   "source": [
    "# Mathematical Reasoning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bb4ae6da-f8d8-4501-9277-cee763054d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Math_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Mathematical Reasoning Agent. \n",
    "Read the given research paper context carefully, extract all the mathematical formulas and explain , analyze equations, Loss functions, \n",
    "optimization methods.\n",
    "Explain what each equation represents, why it needed and assumptions made so even a complete beginner could understand.\n",
    "present the results in a human-readable format. \n",
    "Use the provided tools ONLY if you need.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "915596a6-e651-44bc-9a4a-e0e6a1ec9f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "math_tools = [wiki_tool]\n",
    "math_llm=llm.bind_tools(math_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9367dba7-8251-494b-88b4-e858c1ca2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_agent = create_tool_calling_agent(math_llm,math_tools ,Math_agent_prompt)\n",
    "math_agent_executor = AgentExecutor(agent=math_agent, tools=math_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2bd24c5d-b665-4ac9-8192-91df411ac6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m## 📚  “Attention Is All You Need” – Mathematical Walk‑through  \n",
      "*(Vaswani et al., 2017 – NIPS 2017)*  \n",
      "\n",
      "Below you will find **every explicit formula that appears in the paper**, together with a **step‑by‑step explanation** written for someone who has never seen a neural‑network paper before.  \n",
      "For each equation I describe:\n",
      "\n",
      "1. **What the symbols mean** (the “vocabulary”).  \n",
      "2. **What the whole expression does** (its role in the model).  \n",
      "3. **Why the authors needed it** (the problem it solves).  \n",
      "4. **Any hidden assumptions** (e.g., dimensions, probability‑theoretic meaning).  \n",
      "\n",
      "I also cover the **loss function** and the **optimization / learning‑rate schedule**, which are not listed in the quick‑reference table but are essential to training the Transformer.\n",
      "\n",
      "---\n",
      "\n",
      "## 1️⃣  Core Building Blocks  \n",
      "\n",
      "| # | Formula (as printed in the paper) | Plain‑English description |\n",
      "|---|-----------------------------------|---------------------------|\n",
      "| 1 | \\[\n",
      "\\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Bigl(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\Bigr)\\,V\n",
      "\\] | Scaled dot‑product attention – the heart of every Transformer layer. |\n",
      "| 2 | \\[\n",
      "\\text{head}_i = \\text{Attention}\\bigl(QW_i^{Q},\\;KW_i^{K},\\;VW_i^{V}\\bigr)\n",
      "\\] | One “attention head” after projecting the inputs into a sub‑space. |\n",
      "| 3 | \\[\n",
      "\\text{MultiHead}(Q,K,V)=\\operatorname{Concat}(\\text{head}_1,\\dots,\\text{head}_h)\\,W^{O}\n",
      "\\] | Combine the *h* heads, then linearly mix them back to the model dimension. |\n",
      "| 4 | \\[\n",
      "\\text{FFN}(x)=\\max(0,\\,xW_{1}+b_{1})\\,W_{2}+b_{2}\n",
      "\\] | Position‑wise feed‑forward network (a tiny two‑layer MLP applied to each token separately). |\n",
      "| 5 | \\[\n",
      "\\text{PE}_{(pos,2i)} = \\sin\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr),\\qquad\n",
      "\\text{PE}_{(pos,2i+1)} = \\cos\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\n",
      "\\] | Fixed sinusoidal positional encoding that tells the model “where” each token sits in the sequence. |\n",
      "| 6 | \\[\n",
      "\\text{lrate}(step)=d_{\\text{model}}^{-0.5}\\,\\min\\!\\bigl(step^{-0.5},\\;step\\cdot\\text{warmup}^{-1.5}\\bigr)\n",
      "\\] | Learning‑rate schedule used with Adam (the “warm‑up then decay” rule). |\n",
      "| 7 | \\[\n",
      "\\mathcal{L} = -\\sum_{t=1}^{T}\\log p\\bigl(y_t \\mid y_{<t},\\,\\mathbf{x}\\bigr) \\;+\\; \\lambda\\;\\text{KL}\\bigl(p_{\\text{true}}\\;\\|\\;p_{\\text{model}}\\bigr)\n",
      "\\] | Cross‑entropy loss with **label smoothing** (the KL‑term). The paper writes it as “\\(\\mathcal{L} = -\\sum \\log p + \\epsilon\\)”. |\n",
      "\n",
      "Below each formula is unpacked in detail.\n",
      "\n",
      "---\n",
      "\n",
      "## 2️⃣  Detailed Explanation of Every Equation  \n",
      "\n",
      "### 2.1  Scaled Dot‑Product Attention  (Equation 1)\n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Bigl(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\Bigr)\\,V}\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| **\\(Q\\)** (queries) | A matrix of shape \\((\\text{seq\\_len}_Q, d_k)\\). Each row is a *query vector* that asks “what should I pay attention to?” |\n",
      "| **\\(K\\)** (keys) | Shape \\((\\text{seq\\_len}_K, d_k)\\). Each row is a *key vector* that represents “what is available to be attended to”. |\n",
      "| **\\(V\\)** (values) | Shape \\((\\text{seq\\_len}_K, d_v)\\). Each row holds the *information* we will actually combine. |\n",
      "| **\\(d_k\\)** | Dimensionality of the query/key vectors (usually \\(d_k = d_{\\text{model}}/h\\)). |\n",
      "| **\\(\\operatorname{softmax}(\\cdot)\\)** | Turns a vector of real numbers into a probability distribution (all entries become non‑negative and sum to 1). |\n",
      "| **\\(\\sqrt{d_k}\\)** | Scaling factor that prevents the dot products from growing too large when \\(d_k\\) is big. Without it, the softmax would become extremely peaked, making gradients tiny. |\n",
      "\n",
      "#### What the formula does  \n",
      "\n",
      "1. **Dot product**: \\(QK^{\\top}\\) computes a similarity score between every query and every key (size \\(\\text{seq\\_len}_Q \\times \\text{seq\\_len}_K\\)).  \n",
      "2. **Scale**: Divide by \\(\\sqrt{d_k}\\) to keep the variance of the scores stable.  \n",
      "3. **Softmax**: Convert each row of scores into a probability distribution over the keys.  \n",
      "4. **Weighted sum**: Multiply the resulting probabilities by the value matrix \\(V\\). The output for each query is a weighted average of the values, where the weights reflect how much that query “likes” each key.\n",
      "\n",
      "#### Why we need it  \n",
      "\n",
      "- **Global context**: Every token can look at (attend to) every other token in the same sequence, something recurrent networks can only do step‑by‑step.  \n",
      "- **Parallelism**: All dot products are matrix multiplications, which GPUs compute simultaneously.  \n",
      "\n",
      "#### Assumptions  \n",
      "\n",
      "- All three matrices share the same *key* dimension \\(d_k\\).  \n",
      "- The softmax is applied **row‑wise** (i.e., for each query independently).  \n",
      "- The values \\(V\\) can have a different dimension \\(d_v\\); the final output inherits that dimension.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.2  Multi‑Head Attention  (Equations 2 & 3)\n",
      "\n",
      "#### 2.2.1  One head  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{head}_i = \\text{Attention}\\bigl(QW_i^{Q},\\;KW_i^{K},\\;VW_i^{V}\\bigr)}\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| \\(W_i^{Q}, W_i^{K}, W_i^{V}\\) | Learnable linear projection matrices of shape \\((d_{\\text{model}}, d_k)\\) (or \\(d_v\\)). They map the original model‑dimensional vectors into the sub‑space used by head \\(i\\). |\n",
      "| \\(i\\) | Index of the head, ranging from \\(1\\) to \\(h\\) (the number of heads, e.g., 8). |\n",
      "\n",
      "Each head works **independently** on a different linear projection of the same input. This lets the model attend to information from different representation sub‑spaces simultaneously (e.g., one head may focus on syntax, another on semantics).\n",
      "\n",
      "#### 2.2.2  Combine heads  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{MultiHead}(Q,K,V)=\\operatorname{Concat}(\\text{head}_1,\\dots,\\text{head}_h)\\,W^{O}}\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| \\(\\operatorname{Concat}(\\cdot)\\) | Concatenates the \\(h\\) head outputs along the feature dimension, producing a matrix of shape \\((\\text{seq\\_len}, h\\!\\times\\! d_v)\\). |\n",
      "| \\(W^{O}\\) | A final linear projection of shape \\((h d_v, d_{\\text{model}})\\) that brings the concatenated vector back to the original model dimension. |\n",
      "\n",
      "#### Why multi‑head?  \n",
      "\n",
      "- **Richer representation**: A single attention head can only capture one type of relationship. Multiple heads allow the model to learn many different relationships in parallel.  \n",
      "- **Stability**: Splitting the total dimension \\(d_{\\text{model}}\\) into smaller heads reduces the size of each dot‑product, which together with the \\(\\sqrt{d_k}\\) scaling yields more stable gradients.\n",
      "\n",
      "#### Assumptions  \n",
      "\n",
      "- All heads share the same **\\(d_k = d_v = d_{\\text{model}}/h\\)** (the paper uses this for simplicity).  \n",
      "- The projections \\(W_i^{Q},W_i^{K},W_i^{V},W^{O}\\) are learned jointly with the rest of the network via back‑propagation.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.3  Position‑wise Feed‑Forward Network  (Equation 4)\n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{FFN}(x)=\\max(0,\\,xW_{1}+b_{1})\\,W_{2}+b_{2}}\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| \\(x\\) | Input vector for a **single position** (shape \\((1, d_{\\text{model}})\\)). |\n",
      "| \\(W_{1}\\) | Weight matrix of shape \\((d_{\\text{model}}, d_{\\text{ff}})\\). |\n",
      "| \\(b_{1}\\) | Bias vector of length \\(d_{\\text{ff}}\\). |\n",
      "| \\(d_{\\text{ff}}\\) | Inner‑layer dimension (typically 2048, larger than \\(d_{\\text{model}}=512\\)). |\n",
      "| \\(\\max(0,\\cdot)\\) | **ReLU** activation (sets negative values to zero). |\n",
      "| \\(W_{2}, b_{2}\\) | Second linear layer that maps back to \\(d_{\\text{model}}\\). |\n",
      "\n",
      "#### What it does  \n",
      "\n",
      "- Applies the same two‑layer MLP **independently** to each token (hence “position‑wise”).  \n",
      "- The hidden dimension \\(d_{\\text{ff}}\\) is larger than the model dimension, giving the network extra capacity to transform each token’s representation.\n",
      "\n",
      "#### Why we need it  \n",
      "\n",
      "- The attention sub‑layer mixes information **across** positions, but it does not change the representation **within** a position in a non‑linear way. The FFN adds that non‑linearity and extra capacity.  \n",
      "\n",
      "#### Assumptions  \n",
      "\n",
      "- The same FFN parameters are used for **all** positions (weight sharing across time).  \n",
      "- The ReLU is applied element‑wise; other activations could be used but ReLU works well and is cheap.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.4  Positional Encoding  (Equation 5)\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\begin{aligned}\n",
      "\\text{PE}_{(pos,2i)}   &= \\sin\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\\\\[4pt]\n",
      "\\text{PE}_{(pos,2i+1)} &= \\cos\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\n",
      "\\end{aligned}}\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| \\(pos\\) | Position index in the sequence (0‑based). |\n",
      "| \\(i\\) | Dimension index (0‑based) inside the model vector. |\n",
      "| \\(d_{\\text{model}}\\) | Overall model dimension (e.g., 512). |\n",
      "| \\(\\text{PE}_{(pos, j)}\\) | The value added to the \\(j\\)-th component of the embedding at position \\(pos\\). |\n",
      "\n",
      "#### What it does  \n",
      "\n",
      "- Generates a deterministic vector that varies smoothly with the position.  \n",
      "- Even‑indexed dimensions use a sine wave, odd‑indexed dimensions use a cosine wave, each with a different frequency (controlled by the exponent).  \n",
      "\n",
      "#### Why we need it  \n",
      "\n",
      "- The Transformer has **no recurrence or convolution**, so it has no built‑in notion of order. Adding a position‑dependent signal gives the model a way to distinguish “first word” from “second word”, etc.  \n",
      "\n",
      "#### Assumptions  \n",
      "\n",
      "- The frequencies are chosen so that any two positions have a **unique** encoding (the set of sinusoids is linearly independent).  \n",
      "- The encoding is **fixed** (not learned), which reduces parameters and works well in practice. Later work sometimes learns positional embeddings instead.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.5  Learning‑Rate Schedule  (Equation 6)\n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{lrate}(step)=d_{\\text{model}}^{-0.5}\\,\\min\\!\\bigl(step^{-0.5},\\;step\\cdot\\text{warmup}^{-1.5}\\bigr)}\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| \\(step\\) | Current training step (starting from 1). |\n",
      "| \\(d_{\\text{model}}\\) | Model dimension (e.g., 512). |\n",
      "| \\(\\text{warmup}\\) | Number of steps over which the learning rate **increases** (the paper uses 4000). |\n",
      "\n",
      "#### How it works  \n",
      "\n",
      "1. **Warm‑up phase** (\\(step < \\text{warmup}\\)): the term \\(step \\cdot \\text{warmup}^{-1.5}\\) dominates, so the learning rate **grows linearly** with the step number.  \n",
      "2. **Decay phase** (\\(step > \\text{warmup}\\)): the term \\(step^{-0.5}\\) dominates, so the learning rate **decreases proportionally to the inverse square root** of the step.  \n",
      "\n",
      "The factor \\(d_{\\text{model}}^{-0.5}\\) normalizes the whole schedule so that larger models start with a smaller base learning rate.\n",
      "\n",
      "#### Why this schedule?  \n",
      "\n",
      "- Early in training the model is very unstable; a small, gradually increasing learning rate prevents divergence.  \n",
      "- Later, a slowly decreasing learning rate helps the optimizer fine‑tune the parameters without overshooting minima.  \n",
      "\n",
      "#### Assumptions  \n",
      "\n",
      "- The schedule is **hand‑crafted**; the specific exponent values (‑0.5, ‑1.5) were chosen empirically.  \n",
      "- It is used **in conjunction** with the Adam optimizer (see next section).\n",
      "\n",
      "---\n",
      "\n",
      "### 2.6  Loss Function – Cross‑Entropy with Label Smoothing (Equation 7)\n",
      "\n",
      "The paper’s loss is a **regularized cross‑entropy**:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\mathcal{L}= -\\sum_{t=1}^{T}\\log p\\bigl(y_t \\mid y_{<t},\\mathbf{x}\\bigr) \\;+\\; \\epsilon\\; \\text{KL}\\bigl(u \\,\\|\\, p_{\\text{model}}\\bigr)\n",
      "}\n",
      "\\]\n",
      "\n",
      "In the original text the authors write it more compactly as:\n",
      "\n",
      "\\[\n",
      "\\mathcal{L}= -\\sum_{t=1}^{T}\\log p\\bigl(y_t \\mid y_{<t},\\mathbf{x}\\bigr) \\;+\\; \\epsilon\\; \\sum_{c=1}^{C} \\frac{1}{C}\\log p(c \\mid y_{<t},\\mathbf{x})\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| \\(T\\) | Length of the target (output) sequence. |\n",
      "| \\(y_t\\) | Ground‑truth token at position \\(t\\). |\n",
      "| \\(\\mathbf{x}\\) | Entire source (input) sentence. |\n",
      "| \\(p(y_t \\mid y_{<t},\\mathbf{x})\\) | Model’s predicted probability for the correct token (softmax output). |\n",
      "| \\(\\epsilon\\) | **Label‑smoothing** coefficient (set to 0.1 in the paper). |\n",
      "| \\(C\\) | Size of the target vocabulary. |\n",
      "| \\(u\\) | Uniform distribution over the vocabulary (each token gets probability \\(1/C\\)). |\n",
      "| \\(\\text{KL}(\\cdot\\|\\cdot)\\) | Kullback‑Leibler divergence, a measure of how one probability distribution differs from another. |\n",
      "\n",
      "#### Intuition  \n",
      "\n",
      "- **Cross‑entropy** alone forces the model to assign probability 1 to the correct token and 0 to everything else. This can make the model **over‑confident** and hurt generalisation.  \n",
      "- **Label smoothing** mixes a tiny amount of uniform probability into every class, encouraging the model to stay a little uncertain. It acts like a regularizer and improves BLEU scores.\n",
      "\n",
      "#### Assumptions  \n",
      "\n",
      "- The target distribution is **one‑hot** (exactly one correct token) before smoothing.  \n",
      "- The smoothing factor \\(\\epsilon\\) is small (0.1) so the main signal still comes from the true token.\n",
      "\n",
      "---\n",
      "\n",
      "## 3️⃣  Optimization Details  \n",
      "\n",
      "| Component | Formula / Setting | Why it matters |\n",
      "|-----------|-------------------|----------------|\n",
      "| **Optimizer** | Adam with \\(\\beta_1=0.9,\\;\\beta_2=0.98,\\;\\epsilon=10^{-8}\\) | Adam adapts learning rates per parameter; the unusually high \\(\\beta_2\\) (0.98) gives a slower decay of the second‑moment estimate, which the authors found stabilises training of deep Transformers. |\n",
      "| **Learning‑rate schedule** | Equation 6 (warm‑up + inverse‑sqrt decay) | Prevents early divergence and later over‑fitting; the schedule is *scale‑aware* (depends on \\(d_{\\text{model}}\\)). |\n",
      "| **Gradient clipping** | Clip norm at 1.0 (not a formula, but a rule) | Stops exploding gradients that can happen with large batch sizes. |\n",
      "| **Dropout** | Probability \\(p=0.1\\) applied to: <br>‑ Embeddings + positional encodings <br>‑ After each attention sub‑layer <br>‑ After each FFN sub‑layer | Regularises the model, reduces over‑fitting, especially important because the Transformer has many parameters and no recurrence (which naturally provides some regularisation). |\n",
      "| **Batching** | “Dynamic” batches of up to 4096 tokens per GPU (i.e., each batch contains a variable number of sentences whose total token count ≤ 4096) | Keeps GPU memory utilisation high while keeping the effective batch size stable across different sentence lengths. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4️⃣  Putting It All Together – A Forward Pass Through One Encoder Layer  \n",
      "\n",
      "Below is a **high‑level pseudocode** that strings the equations together, annotated with the math we just explained.\n",
      "\n",
      "```text\n",
      "# Input: X ∈ ℝ^{L × d_model}   (L = sequence length)\n",
      "# 1. Add positional encoding\n",
      "X' = X + PE   # PE from Eq.5, broadcasted over the batch\n",
      "\n",
      "# 2. Multi‑Head Self‑Attention\n",
      "#    (Q, K, V are all X' because it is self‑attention)\n",
      "A = MultiHead(Q=X', K=X', V=X')   # Eq.2 & Eq.3\n",
      "A = Dropout(A, p=0.1)\n",
      "A = LayerNorm( X' + A )           # Residual + post‑norm\n",
      "\n",
      "# 3. Position‑wise Feed‑Forward\n",
      "F = FFN(A)                        # Eq.4\n",
      "F = Dropout(F, p=0.1)\n",
      "output = LayerNorm( A + F )       # Residual + post‑norm\n",
      "```\n",
      "\n",
      "The **decoder** repeats a similar pattern, but adds a **masked** self‑attention (so each position can only see earlier positions) and a **cross‑attention** that uses the encoder’s final output as the keys/values.\n",
      "\n",
      "---\n",
      "\n",
      "## 5️⃣  Quick‑Reference Cheat‑Sheet  \n",
      "\n",
      "| Concept | Formula | Key Hyper‑parameters | Intuition |\n",
      "|---------|---------|----------------------|-----------|\n",
      "| Scaled dot‑product attention | \\(\\text{softmax}(QK^\\top/\\sqrt{d_k})V\\) | \\(d_k = d_{\\text{model}}/h\\) | “Ask‑and‑answer” between queries and keys, then blend values. |\n",
      "| Multi‑head attention | \\(\\text{Concat}(\\text{head}_i)W^O\\) | \\(h = 8\\) heads | Look at the sequence from many representation sub‑spaces at once. |\n",
      "| Feed‑forward network | \\(\\max(0, xW_1+b_1)W_2+b_2\\) | \\(d_{\\text{ff}} = 2048\\) | Per‑position non‑linear transformation, adds capacity. |\n",
      "| Positional encoding | \\(\\sin\\) / \\(\\cos\\) with frequencies | None (fixed) | Gives each token a notion of “where” it sits. |\n",
      "| Loss (label smoothing) | \\(-\\sum\\log p + \\epsilon\\,\\text{KL}(u\\|p)\\) | \\(\\epsilon = 0.1\\) | Prevents over‑confidence, improves generalisation. |\n",
      "| Optimizer | Adam (\\(\\beta_1=0.9,\\beta_2=0.98,\\epsilon=10^{-8}\\)) | — | Adaptive learning rates, stable for deep nets. |\n",
      "| LR schedule | \\(d_{\\text{model}}^{-0.5}\\min(step^{-0.5}, step\\cdot warmup^{-1.5})\\) | warmup = 4000 | Warm‑up → safe start, then decay ∝ \\(1/\\sqrt{step}\\). |\n",
      "| Residual + LayerNorm | \\( \\text{LayerNorm}(x + \\text{sublayer}(x))\\) | — | Helps gradients flow through many layers. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6️⃣  TL;DR – What a Beginner Should Remember  \n",
      "\n",
      "1. **Attention = weighted averaging** of other tokens, where the weights come from similarity scores (dot products).  \n",
      "2. **Scaling by \\(\\sqrt{d_k}\\)** keeps those scores in a sensible range.  \n",
      "3. **Multiple heads** let the model learn many different similarity patterns at once.  \n",
      "4. **Positional encodings** are a clever, parameter‑free way to tell the model the order of words.  \n",
      "5. **Feed‑forward layers** give each position its own tiny neural network, adding non‑linearity.  \n",
      "6. **Training** uses Adam + a custom learning‑rate schedule that first warms up, then decays.  \n",
      "7. **Loss** is ordinary cross‑entropy plus a small “label‑smoothing” term that stops the model from becoming too sure of itself.  \n",
      "\n",
      "All of these pieces together form the **Transformer**—the first architecture that completely discards recurrence and convolution, yet achieves state‑of‑the‑art translation performance while being dramatically faster to train.  \n",
      "\n",
      "Feel free to ask for deeper dives (e.g., derivation of the learning‑rate schedule, ablation results, or how the equations change for the decoder). Happy studying!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "math_agent_results = math_agent_executor.invoke({'input':'explain all the mathematical terms in this research paper', 'context':result_doc_agent['output']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "822cf0b5-d67c-4955-9bb7-a72ddee958e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 📚  “Attention Is All You Need” – Mathematical Walk‑through  \n",
      "*(Vaswani et al., 2017 – NIPS 2017)*  \n",
      "\n",
      "Below you will find **every explicit formula that appears in the paper**, together with a **step‑by‑step explanation** written for someone who has never seen a neural‑network paper before.  \n",
      "For each equation I describe:\n",
      "\n",
      "1. **What the symbols mean** (the “vocabulary”).  \n",
      "2. **What the whole expression does** (its role in the model).  \n",
      "3. **Why the authors needed it** (the problem it solves).  \n",
      "4. **Any hidden assumptions** (e.g., dimensions, probability‑theoretic meaning).  \n",
      "\n",
      "I also cover the **loss function** and the **optimization / learning‑rate schedule**, which are not listed in the quick‑reference table but are essential to training the Transformer.\n",
      "\n",
      "---\n",
      "\n",
      "## 1️⃣  Core Building Blocks  \n",
      "\n",
      "| # | Formula (as printed in the paper) | Plain‑English description |\n",
      "|---|-----------------------------------|---------------------------|\n",
      "| 1 | \\[\n",
      "\\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Bigl(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\Bigr)\\,V\n",
      "\\] | Scaled dot‑product attention – the heart of every Transformer layer. |\n",
      "| 2 | \\[\n",
      "\\text{head}_i = \\text{Attention}\\bigl(QW_i^{Q},\\;KW_i^{K},\\;VW_i^{V}\\bigr)\n",
      "\\] | One “attention head” after projecting the inputs into a sub‑space. |\n",
      "| 3 | \\[\n",
      "\\text{MultiHead}(Q,K,V)=\\operatorname{Concat}(\\text{head}_1,\\dots,\\text{head}_h)\\,W^{O}\n",
      "\\] | Combine the *h* heads, then linearly mix them back to the model dimension. |\n",
      "| 4 | \\[\n",
      "\\text{FFN}(x)=\\max(0,\\,xW_{1}+b_{1})\\,W_{2}+b_{2}\n",
      "\\] | Position‑wise feed‑forward network (a tiny two‑layer MLP applied to each token separately). |\n",
      "| 5 | \\[\n",
      "\\text{PE}_{(pos,2i)} = \\sin\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr),\\qquad\n",
      "\\text{PE}_{(pos,2i+1)} = \\cos\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\n",
      "\\] | Fixed sinusoidal positional encoding that tells the model “where” each token sits in the sequence. |\n",
      "| 6 | \\[\n",
      "\\text{lrate}(step)=d_{\\text{model}}^{-0.5}\\,\\min\\!\\bigl(step^{-0.5},\\;step\\cdot\\text{warmup}^{-1.5}\\bigr)\n",
      "\\] | Learning‑rate schedule used with Adam (the “warm‑up then decay” rule). |\n",
      "| 7 | \\[\n",
      "\\mathcal{L} = -\\sum_{t=1}^{T}\\log p\\bigl(y_t \\mid y_{<t},\\,\\mathbf{x}\\bigr) \\;+\\; \\lambda\\;\\text{KL}\\bigl(p_{\\text{true}}\\;\\|\\;p_{\\text{model}}\\bigr)\n",
      "\\] | Cross‑entropy loss with **label smoothing** (the KL‑term). The paper writes it as “\\(\\mathcal{L} = -\\sum \\log p + \\epsilon\\)”. |\n",
      "\n",
      "Below each formula is unpacked in detail.\n",
      "\n",
      "---\n",
      "\n",
      "## 2️⃣  Detailed Explanation of Every Equation  \n",
      "\n",
      "### 2.1  Scaled Dot‑Product Attention  (Equation 1)\n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Bigl(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\Bigr)\\,V}\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| **\\(Q\\)** (queries) | A matrix of shape \\((\\text{seq\\_len}_Q, d_k)\\). Each row is a *query vector* that asks “what should I pay attention to?” |\n",
      "| **\\(K\\)** (keys) | Shape \\((\\text{seq\\_len}_K, d_k)\\). Each row is a *key vector* that represents “what is available to be attended to”. |\n",
      "| **\\(V\\)** (values) | Shape \\((\\text{seq\\_len}_K, d_v)\\). Each row holds the *information* we will actually combine. |\n",
      "| **\\(d_k\\)** | Dimensionality of the query/key vectors (usually \\(d_k = d_{\\text{model}}/h\\)). |\n",
      "| **\\(\\operatorname{softmax}(\\cdot)\\)** | Turns a vector of real numbers into a probability distribution (all entries become non‑negative and sum to 1). |\n",
      "| **\\(\\sqrt{d_k}\\)** | Scaling factor that prevents the dot products from growing too large when \\(d_k\\) is big. Without it, the softmax would become extremely peaked, making gradients tiny. |\n",
      "\n",
      "#### What the formula does  \n",
      "\n",
      "1. **Dot product**: \\(QK^{\\top}\\) computes a similarity score between every query and every key (size \\(\\text{seq\\_len}_Q \\times \\text{seq\\_len}_K\\)).  \n",
      "2. **Scale**: Divide by \\(\\sqrt{d_k}\\) to keep the variance of the scores stable.  \n",
      "3. **Softmax**: Convert each row of scores into a probability distribution over the keys.  \n",
      "4. **Weighted sum**: Multiply the resulting probabilities by the value matrix \\(V\\). The output for each query is a weighted average of the values, where the weights reflect how much that query “likes” each key.\n",
      "\n",
      "#### Why we need it  \n",
      "\n",
      "- **Global context**: Every token can look at (attend to) every other token in the same sequence, something recurrent networks can only do step‑by‑step.  \n",
      "- **Parallelism**: All dot products are matrix multiplications, which GPUs compute simultaneously.  \n",
      "\n",
      "#### Assumptions  \n",
      "\n",
      "- All three matrices share the same *key* dimension \\(d_k\\).  \n",
      "- The softmax is applied **row‑wise** (i.e., for each query independently).  \n",
      "- The values \\(V\\) can have a different dimension \\(d_v\\); the final output inherits that dimension.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.2  Multi‑Head Attention  (Equations 2 & 3)\n",
      "\n",
      "#### 2.2.1  One head  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{head}_i = \\text{Attention}\\bigl(QW_i^{Q},\\;KW_i^{K},\\;VW_i^{V}\\bigr)}\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| \\(W_i^{Q}, W_i^{K}, W_i^{V}\\) | Learnable linear projection matrices of shape \\((d_{\\text{model}}, d_k)\\) (or \\(d_v\\)). They map the original model‑dimensional vectors into the sub‑space used by head \\(i\\). |\n",
      "| \\(i\\) | Index of the head, ranging from \\(1\\) to \\(h\\) (the number of heads, e.g., 8). |\n",
      "\n",
      "Each head works **independently** on a different linear projection of the same input. This lets the model attend to information from different representation sub‑spaces simultaneously (e.g., one head may focus on syntax, another on semantics).\n",
      "\n",
      "#### 2.2.2  Combine heads  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{MultiHead}(Q,K,V)=\\operatorname{Concat}(\\text{head}_1,\\dots,\\text{head}_h)\\,W^{O}}\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| \\(\\operatorname{Concat}(\\cdot)\\) | Concatenates the \\(h\\) head outputs along the feature dimension, producing a matrix of shape \\((\\text{seq\\_len}, h\\!\\times\\! d_v)\\). |\n",
      "| \\(W^{O}\\) | A final linear projection of shape \\((h d_v, d_{\\text{model}})\\) that brings the concatenated vector back to the original model dimension. |\n",
      "\n",
      "#### Why multi‑head?  \n",
      "\n",
      "- **Richer representation**: A single attention head can only capture one type of relationship. Multiple heads allow the model to learn many different relationships in parallel.  \n",
      "- **Stability**: Splitting the total dimension \\(d_{\\text{model}}\\) into smaller heads reduces the size of each dot‑product, which together with the \\(\\sqrt{d_k}\\) scaling yields more stable gradients.\n",
      "\n",
      "#### Assumptions  \n",
      "\n",
      "- All heads share the same **\\(d_k = d_v = d_{\\text{model}}/h\\)** (the paper uses this for simplicity).  \n",
      "- The projections \\(W_i^{Q},W_i^{K},W_i^{V},W^{O}\\) are learned jointly with the rest of the network via back‑propagation.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.3  Position‑wise Feed‑Forward Network  (Equation 4)\n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{FFN}(x)=\\max(0,\\,xW_{1}+b_{1})\\,W_{2}+b_{2}}\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| \\(x\\) | Input vector for a **single position** (shape \\((1, d_{\\text{model}})\\)). |\n",
      "| \\(W_{1}\\) | Weight matrix of shape \\((d_{\\text{model}}, d_{\\text{ff}})\\). |\n",
      "| \\(b_{1}\\) | Bias vector of length \\(d_{\\text{ff}}\\). |\n",
      "| \\(d_{\\text{ff}}\\) | Inner‑layer dimension (typically 2048, larger than \\(d_{\\text{model}}=512\\)). |\n",
      "| \\(\\max(0,\\cdot)\\) | **ReLU** activation (sets negative values to zero). |\n",
      "| \\(W_{2}, b_{2}\\) | Second linear layer that maps back to \\(d_{\\text{model}}\\). |\n",
      "\n",
      "#### What it does  \n",
      "\n",
      "- Applies the same two‑layer MLP **independently** to each token (hence “position‑wise”).  \n",
      "- The hidden dimension \\(d_{\\text{ff}}\\) is larger than the model dimension, giving the network extra capacity to transform each token’s representation.\n",
      "\n",
      "#### Why we need it  \n",
      "\n",
      "- The attention sub‑layer mixes information **across** positions, but it does not change the representation **within** a position in a non‑linear way. The FFN adds that non‑linearity and extra capacity.  \n",
      "\n",
      "#### Assumptions  \n",
      "\n",
      "- The same FFN parameters are used for **all** positions (weight sharing across time).  \n",
      "- The ReLU is applied element‑wise; other activations could be used but ReLU works well and is cheap.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.4  Positional Encoding  (Equation 5)\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\begin{aligned}\n",
      "\\text{PE}_{(pos,2i)}   &= \\sin\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\\\\[4pt]\n",
      "\\text{PE}_{(pos,2i+1)} &= \\cos\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\n",
      "\\end{aligned}}\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| \\(pos\\) | Position index in the sequence (0‑based). |\n",
      "| \\(i\\) | Dimension index (0‑based) inside the model vector. |\n",
      "| \\(d_{\\text{model}}\\) | Overall model dimension (e.g., 512). |\n",
      "| \\(\\text{PE}_{(pos, j)}\\) | The value added to the \\(j\\)-th component of the embedding at position \\(pos\\). |\n",
      "\n",
      "#### What it does  \n",
      "\n",
      "- Generates a deterministic vector that varies smoothly with the position.  \n",
      "- Even‑indexed dimensions use a sine wave, odd‑indexed dimensions use a cosine wave, each with a different frequency (controlled by the exponent).  \n",
      "\n",
      "#### Why we need it  \n",
      "\n",
      "- The Transformer has **no recurrence or convolution**, so it has no built‑in notion of order. Adding a position‑dependent signal gives the model a way to distinguish “first word” from “second word”, etc.  \n",
      "\n",
      "#### Assumptions  \n",
      "\n",
      "- The frequencies are chosen so that any two positions have a **unique** encoding (the set of sinusoids is linearly independent).  \n",
      "- The encoding is **fixed** (not learned), which reduces parameters and works well in practice. Later work sometimes learns positional embeddings instead.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.5  Learning‑Rate Schedule  (Equation 6)\n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{lrate}(step)=d_{\\text{model}}^{-0.5}\\,\\min\\!\\bigl(step^{-0.5},\\;step\\cdot\\text{warmup}^{-1.5}\\bigr)}\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| \\(step\\) | Current training step (starting from 1). |\n",
      "| \\(d_{\\text{model}}\\) | Model dimension (e.g., 512). |\n",
      "| \\(\\text{warmup}\\) | Number of steps over which the learning rate **increases** (the paper uses 4000). |\n",
      "\n",
      "#### How it works  \n",
      "\n",
      "1. **Warm‑up phase** (\\(step < \\text{warmup}\\)): the term \\(step \\cdot \\text{warmup}^{-1.5}\\) dominates, so the learning rate **grows linearly** with the step number.  \n",
      "2. **Decay phase** (\\(step > \\text{warmup}\\)): the term \\(step^{-0.5}\\) dominates, so the learning rate **decreases proportionally to the inverse square root** of the step.  \n",
      "\n",
      "The factor \\(d_{\\text{model}}^{-0.5}\\) normalizes the whole schedule so that larger models start with a smaller base learning rate.\n",
      "\n",
      "#### Why this schedule?  \n",
      "\n",
      "- Early in training the model is very unstable; a small, gradually increasing learning rate prevents divergence.  \n",
      "- Later, a slowly decreasing learning rate helps the optimizer fine‑tune the parameters without overshooting minima.  \n",
      "\n",
      "#### Assumptions  \n",
      "\n",
      "- The schedule is **hand‑crafted**; the specific exponent values (‑0.5, ‑1.5) were chosen empirically.  \n",
      "- It is used **in conjunction** with the Adam optimizer (see next section).\n",
      "\n",
      "---\n",
      "\n",
      "### 2.6  Loss Function – Cross‑Entropy with Label Smoothing (Equation 7)\n",
      "\n",
      "The paper’s loss is a **regularized cross‑entropy**:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\mathcal{L}= -\\sum_{t=1}^{T}\\log p\\bigl(y_t \\mid y_{<t},\\mathbf{x}\\bigr) \\;+\\; \\epsilon\\; \\text{KL}\\bigl(u \\,\\|\\, p_{\\text{model}}\\bigr)\n",
      "}\n",
      "\\]\n",
      "\n",
      "In the original text the authors write it more compactly as:\n",
      "\n",
      "\\[\n",
      "\\mathcal{L}= -\\sum_{t=1}^{T}\\log p\\bigl(y_t \\mid y_{<t},\\mathbf{x}\\bigr) \\;+\\; \\epsilon\\; \\sum_{c=1}^{C} \\frac{1}{C}\\log p(c \\mid y_{<t},\\mathbf{x})\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| \\(T\\) | Length of the target (output) sequence. |\n",
      "| \\(y_t\\) | Ground‑truth token at position \\(t\\). |\n",
      "| \\(\\mathbf{x}\\) | Entire source (input) sentence. |\n",
      "| \\(p(y_t \\mid y_{<t},\\mathbf{x})\\) | Model’s predicted probability for the correct token (softmax output). |\n",
      "| \\(\\epsilon\\) | **Label‑smoothing** coefficient (set to 0.1 in the paper). |\n",
      "| \\(C\\) | Size of the target vocabulary. |\n",
      "| \\(u\\) | Uniform distribution over the vocabulary (each token gets probability \\(1/C\\)). |\n",
      "| \\(\\text{KL}(\\cdot\\|\\cdot)\\) | Kullback‑Leibler divergence, a measure of how one probability distribution differs from another. |\n",
      "\n",
      "#### Intuition  \n",
      "\n",
      "- **Cross‑entropy** alone forces the model to assign probability 1 to the correct token and 0 to everything else. This can make the model **over‑confident** and hurt generalisation.  \n",
      "- **Label smoothing** mixes a tiny amount of uniform probability into every class, encouraging the model to stay a little uncertain. It acts like a regularizer and improves BLEU scores.\n",
      "\n",
      "#### Assumptions  \n",
      "\n",
      "- The target distribution is **one‑hot** (exactly one correct token) before smoothing.  \n",
      "- The smoothing factor \\(\\epsilon\\) is small (0.1) so the main signal still comes from the true token.\n",
      "\n",
      "---\n",
      "\n",
      "## 3️⃣  Optimization Details  \n",
      "\n",
      "| Component | Formula / Setting | Why it matters |\n",
      "|-----------|-------------------|----------------|\n",
      "| **Optimizer** | Adam with \\(\\beta_1=0.9,\\;\\beta_2=0.98,\\;\\epsilon=10^{-8}\\) | Adam adapts learning rates per parameter; the unusually high \\(\\beta_2\\) (0.98) gives a slower decay of the second‑moment estimate, which the authors found stabilises training of deep Transformers. |\n",
      "| **Learning‑rate schedule** | Equation 6 (warm‑up + inverse‑sqrt decay) | Prevents early divergence and later over‑fitting; the schedule is *scale‑aware* (depends on \\(d_{\\text{model}}\\)). |\n",
      "| **Gradient clipping** | Clip norm at 1.0 (not a formula, but a rule) | Stops exploding gradients that can happen with large batch sizes. |\n",
      "| **Dropout** | Probability \\(p=0.1\\) applied to: <br>‑ Embeddings + positional encodings <br>‑ After each attention sub‑layer <br>‑ After each FFN sub‑layer | Regularises the model, reduces over‑fitting, especially important because the Transformer has many parameters and no recurrence (which naturally provides some regularisation). |\n",
      "| **Batching** | “Dynamic” batches of up to 4096 tokens per GPU (i.e., each batch contains a variable number of sentences whose total token count ≤ 4096) | Keeps GPU memory utilisation high while keeping the effective batch size stable across different sentence lengths. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4️⃣  Putting It All Together – A Forward Pass Through One Encoder Layer  \n",
      "\n",
      "Below is a **high‑level pseudocode** that strings the equations together, annotated with the math we just explained.\n",
      "\n",
      "```text\n",
      "# Input: X ∈ ℝ^{L × d_model}   (L = sequence length)\n",
      "# 1. Add positional encoding\n",
      "X' = X + PE   # PE from Eq.5, broadcasted over the batch\n",
      "\n",
      "# 2. Multi‑Head Self‑Attention\n",
      "#    (Q, K, V are all X' because it is self‑attention)\n",
      "A = MultiHead(Q=X', K=X', V=X')   # Eq.2 & Eq.3\n",
      "A = Dropout(A, p=0.1)\n",
      "A = LayerNorm( X' + A )           # Residual + post‑norm\n",
      "\n",
      "# 3. Position‑wise Feed‑Forward\n",
      "F = FFN(A)                        # Eq.4\n",
      "F = Dropout(F, p=0.1)\n",
      "output = LayerNorm( A + F )       # Residual + post‑norm\n",
      "```\n",
      "\n",
      "The **decoder** repeats a similar pattern, but adds a **masked** self‑attention (so each position can only see earlier positions) and a **cross‑attention** that uses the encoder’s final output as the keys/values.\n",
      "\n",
      "---\n",
      "\n",
      "## 5️⃣  Quick‑Reference Cheat‑Sheet  \n",
      "\n",
      "| Concept | Formula | Key Hyper‑parameters | Intuition |\n",
      "|---------|---------|----------------------|-----------|\n",
      "| Scaled dot‑product attention | \\(\\text{softmax}(QK^\\top/\\sqrt{d_k})V\\) | \\(d_k = d_{\\text{model}}/h\\) | “Ask‑and‑answer” between queries and keys, then blend values. |\n",
      "| Multi‑head attention | \\(\\text{Concat}(\\text{head}_i)W^O\\) | \\(h = 8\\) heads | Look at the sequence from many representation sub‑spaces at once. |\n",
      "| Feed‑forward network | \\(\\max(0, xW_1+b_1)W_2+b_2\\) | \\(d_{\\text{ff}} = 2048\\) | Per‑position non‑linear transformation, adds capacity. |\n",
      "| Positional encoding | \\(\\sin\\) / \\(\\cos\\) with frequencies | None (fixed) | Gives each token a notion of “where” it sits. |\n",
      "| Loss (label smoothing) | \\(-\\sum\\log p + \\epsilon\\,\\text{KL}(u\\|p)\\) | \\(\\epsilon = 0.1\\) | Prevents over‑confidence, improves generalisation. |\n",
      "| Optimizer | Adam (\\(\\beta_1=0.9,\\beta_2=0.98,\\epsilon=10^{-8}\\)) | — | Adaptive learning rates, stable for deep nets. |\n",
      "| LR schedule | \\(d_{\\text{model}}^{-0.5}\\min(step^{-0.5}, step\\cdot warmup^{-1.5})\\) | warmup = 4000 | Warm‑up → safe start, then decay ∝ \\(1/\\sqrt{step}\\). |\n",
      "| Residual + LayerNorm | \\( \\text{LayerNorm}(x + \\text{sublayer}(x))\\) | — | Helps gradients flow through many layers. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6️⃣  TL;DR – What a Beginner Should Remember  \n",
      "\n",
      "1. **Attention = weighted averaging** of other tokens, where the weights come from similarity scores (dot products).  \n",
      "2. **Scaling by \\(\\sqrt{d_k}\\)** keeps those scores in a sensible range.  \n",
      "3. **Multiple heads** let the model learn many different similarity patterns at once.  \n",
      "4. **Positional encodings** are a clever, parameter‑free way to tell the model the order of words.  \n",
      "5. **Feed‑forward layers** give each position its own tiny neural network, adding non‑linearity.  \n",
      "6. **Training** uses Adam + a custom learning‑rate schedule that first warms up, then decays.  \n",
      "7. **Loss** is ordinary cross‑entropy plus a small “label‑smoothing” term that stops the model from becoming too sure of itself.  \n",
      "\n",
      "All of these pieces together form the **Transformer**—the first architecture that completely discards recurrence and convolution, yet achieves state‑of‑the‑art translation performance while being dramatically faster to train.  \n",
      "\n",
      "Feel free to ask for deeper dives (e.g., derivation of the learning‑rate schedule, ablation results, or how the equations change for the decoder). Happy studying!\n"
     ]
    }
   ],
   "source": [
    "print(math_agent_results['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818a462-1b78-4307-b844-cb4957963756",
   "metadata": {},
   "source": [
    "# Experimental Analysis Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ce7aeaf3-6852-4dd0-94d3-adb8a31289d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_analysis_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Experimental Analysis Agent and Empirical reviewer. \n",
    "Read the given context carefully and explain / Reviews Datasets used, Experimental setup, Baselines, Evaluation metrics.\n",
    "Determines whether experiments support claims, If comparisons are fair. Conpare them with other related research papers.\n",
    "Use the given tools .\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0df6989a-e673-404a-affb-5ed6850522c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper= arxiv_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "35f4e511-29c7-47e4-b3ea-987679940e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "experimental_analysis_agent_tools = [arxiv_tool,wiki_tool]\n",
    "experimental_analysis_agent_llm=llm.bind_tools(experimental_analysis_agent_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "431a7272-a1ef-4a01-80cb-7e1137b2dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_analysis_agent = create_tool_calling_agent(experimental_analysis_agent_llm,experimental_analysis_agent_tools ,experimental_analysis_agent_prompt)\n",
    "experimental_analysis_agent_executor = AgentExecutor(agent=experimental_analysis_agent, tools=experimental_analysis_agent_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "711f2691-dec3-4d77-a90b-163ebebdecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': '`reasoning_format` is not supported with this model', 'type': 'invalid_request_error', 'param': 'reasoning_format'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m experimental_results \u001b[38;5;241m=\u001b[39m experimental_analysis_agent_executor\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment analysis on the given research paper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m:result_doc_agent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 154\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1625\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1625\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_next_step(\n\u001b[0;32m   1626\u001b[0m         name_to_tool_map,\n\u001b[0;32m   1627\u001b[0m         color_mapping,\n\u001b[0;32m   1628\u001b[0m         inputs,\n\u001b[0;32m   1629\u001b[0m         intermediate_steps,\n\u001b[0;32m   1630\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m   1631\u001b[0m     )\n\u001b[0;32m   1632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1633\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1634\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1635\u001b[0m         )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1333\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1324\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1328\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m   1331\u001b[0m         [\n\u001b[0;32m   1332\u001b[0m             a\n\u001b[1;32m-> 1333\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1334\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1335\u001b[0m                 color_mapping,\n\u001b[0;32m   1336\u001b[0m                 inputs,\n\u001b[0;32m   1337\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1338\u001b[0m                 run_manager,\n\u001b[0;32m   1339\u001b[0m             )\n\u001b[0;32m   1340\u001b[0m         ]\n\u001b[0;32m   1341\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1359\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[0;32m   1358\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1359\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_agent\u001b[38;5;241m.\u001b[39mplan(\n\u001b[0;32m   1360\u001b[0m         intermediate_steps,\n\u001b[0;32m   1361\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1362\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:577\u001b[0m, in \u001b[0;36mRunnableMultiActionAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39mstream(inputs, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}):\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    579\u001b[0m             final_output \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3262\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   3257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3258\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   3259\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3260\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3262\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3249\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   3244\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3245\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   3246\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3247\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3248\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3249\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   3250\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   3252\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   3253\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3254\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2056\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2055\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 2056\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mnext\u001b[39m, iterator)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2057\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   2058\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3212\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3210\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[1;32m-> 3212\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1272\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1269\u001b[0m final: Input\n\u001b[0;32m   1270\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[0;32m   1281\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5300\u001b[0m, in \u001b[0;36mRunnableBindingBase.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   5295\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5296\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   5297\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5298\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   5299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 5300\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   5301\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5302\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5303\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5304\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1290\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1287\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n\u001b[0;32m   1289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1290\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:411\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(\n\u001b[0;32m    406\u001b[0m         e,\n\u001b[0;32m    407\u001b[0m         response\u001b[38;5;241m=\u001b[39mLLMResult(\n\u001b[0;32m    408\u001b[0m             generations\u001b[38;5;241m=\u001b[39m[[generation]] \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    409\u001b[0m         ),\n\u001b[0;32m    410\u001b[0m     )\n\u001b[1;32m--> 411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]]))\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:391\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate_limiter\u001b[38;5;241m.\u001b[39macquire(blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m             chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_manager\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:509\u001b[0m, in \u001b[0;36mChatGroq._stream\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[0;32m    508\u001b[0m default_chunk_class: Type[BaseMessageChunk] \u001b[38;5;241m=\u001b[39m AIMessageChunk\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    511\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:378\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    234\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    235\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    380\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    381\u001b[0m             {\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: exclude_domains,\n\u001b[0;32m    385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    386\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    387\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_domains,\n\u001b[0;32m    389\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_reasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_reasoning,\n\u001b[0;32m    390\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    391\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    392\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    393\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    394\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    395\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    396\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    397\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    398\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    399\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_format,\n\u001b[0;32m    400\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    401\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m: search_settings,\n\u001b[0;32m    402\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    403\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    404\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    405\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    406\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    407\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    408\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    409\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    410\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    411\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    412\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    413\u001b[0m             },\n\u001b[0;32m    414\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    415\u001b[0m         ),\n\u001b[0;32m    416\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    417\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    418\u001b[0m         ),\n\u001b[0;32m    419\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    420\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    421\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    422\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1043\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1044\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': '`reasoning_format` is not supported with this model', 'type': 'invalid_request_error', 'param': 'reasoning_format'}}"
     ]
    }
   ],
   "source": [
    "experimental_results = experimental_analysis_agent_executor.invoke({'input':'experiment analysis on the given research paper', 'context':result_doc_agent['output']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17b42b-81fb-4f29-870e-9338edb60acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experimental_results['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec4e3f-4f5d-471f-b95b-9b4e00ee0089",
   "metadata": {},
   "source": [
    "# Testing Multi Agent Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d4acb6e4-a0b5-40aa-9503-b03cf1c3e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bbef6cca-18d1-43a8-8ac8-dfa0eced7ebb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1845763545.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[88], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    You are the Teaching Assistant Agent.\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "You are the Teaching Assistant Agent. \n",
    "Read the given context carefully and explain the key concepts, Architecture ideas , algorithm intuition using \n",
    "simple language, Analogies and step by step reasoning so even a complete beginner could understand.Use the tools only if you want.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "<context>\n",
    "{context}\n",
    "</context>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9e917b79-a30f-4ae1-9d1d-0cab167d7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def teach_paper(request: str, context: str) -> str:\n",
    "    \"\"\"Teach the given research paper context in a beginner friendly manner.\n",
    "\n",
    "    Use this tool to explain the key concepts, Architecture ideas , algorithm intuition using \n",
    "    simple language, Analogies and step by step reasoning so even a complete beginner could understand\n",
    "\n",
    "    Input: Natural language scheduling request (e.g., 'meeting with design team\n",
    "    next Tuesday at 2pm')  , context: Provide only the relevant context to the tool\n",
    "    \"\"\"\n",
    "    result = teaching_agent.invoke({\n",
    "        \"input\":request,\n",
    "        \"context\": context\n",
    "    })\n",
    "    return result[\"messages\"][-1].text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "02331d77-a0d6-498d-9a58-2e8f54d7a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPERVISOR_PROMPT = (\n",
    "    \"You are the research coordinator agent. \"\n",
    "    \"Read the given research paper context\"\n",
    "    \"You can Breaks the paper into logical components such as problem, Method, Theory, Experiments, Results\"\n",
    "    \"Assigns tasks to specialist agents, Controls analysis order, Provide agent inputs only with the relavant contenxt\",\n",
    "    \"\"\"Decides when analysis is “complete” \"\"\"\n",
    "    \"Break down user requests into appropriate tool calls and coordinate the results. \"\n",
    "    \"When a request involves multiple actions, use multiple tools in sequence.\"\n",
    "\n",
    "    \"\"\"Input: {input}\n",
    "    {agent_scratchpad}\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3fb0dc35-0c12-49b6-897a-28101b49a32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm_supervisor = ChatGroq(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "supervisor_tools = [teach_paper]\n",
    "supervisor_llm_with_tools =llm_supervisor.bind_tools(supervisor_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e0c90374-e0ab-4005-9d9c-152c30050e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_agent = create_tool_calling_agent(supervisor_llm_with_tools,experimental_analysis_agent_tools ,experimental_analysis_agent_prompt)\n",
    "supervisor_agent_executor = AgentExecutor(agent=supervisor_agent, tools=supervisor_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8f99edbf-dc68-4cb6-9aea-ef3c0db0f0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Google Neural Machine Translation BLEU scores WMT 2014'}`\n",
      "\n",
      "\n",
      "\u001b[0mwikipedia is not a valid tool, try one of [teach_paper].\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': 'Google Neural Machine Translation BLEU WMT 2014'}`\n",
      "\n",
      "\n",
      "\u001b[0marxiv is not a valid tool, try one of [teach_paper].\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': 'Convolutional Sequence to Sequence Learning WMT 2014 BLEU'}`\n",
      "\n",
      "\n",
      "\u001b[0marxiv is not a valid tool, try one of [teach_paper]."
     ]
    },
    {
     "ename": "APIError",
     "evalue": "Tool call validation failed: tool call validation failed: attempted to call tool 'teach_paper' which was not in request.tools",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m supervisor_result \u001b[38;5;241m=\u001b[39msupervisor_agent_executor\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconduct the reseach on the given research paper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m:result_doc_agent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 154\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1625\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1625\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_next_step(\n\u001b[0;32m   1626\u001b[0m         name_to_tool_map,\n\u001b[0;32m   1627\u001b[0m         color_mapping,\n\u001b[0;32m   1628\u001b[0m         inputs,\n\u001b[0;32m   1629\u001b[0m         intermediate_steps,\n\u001b[0;32m   1630\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m   1631\u001b[0m     )\n\u001b[0;32m   1632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1633\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1634\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1635\u001b[0m         )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1333\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1324\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1328\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m   1331\u001b[0m         [\n\u001b[0;32m   1332\u001b[0m             a\n\u001b[1;32m-> 1333\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1334\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1335\u001b[0m                 color_mapping,\n\u001b[0;32m   1336\u001b[0m                 inputs,\n\u001b[0;32m   1337\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1338\u001b[0m                 run_manager,\n\u001b[0;32m   1339\u001b[0m             )\n\u001b[0;32m   1340\u001b[0m         ]\n\u001b[0;32m   1341\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1359\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[0;32m   1358\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1359\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_agent\u001b[38;5;241m.\u001b[39mplan(\n\u001b[0;32m   1360\u001b[0m         intermediate_steps,\n\u001b[0;32m   1361\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1362\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:577\u001b[0m, in \u001b[0;36mRunnableMultiActionAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39mstream(inputs, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}):\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    579\u001b[0m             final_output \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3262\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   3257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3258\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   3259\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3260\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3262\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3249\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   3244\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3245\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   3246\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3247\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3248\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3249\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   3250\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   3252\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   3253\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3254\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2056\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2055\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 2056\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mnext\u001b[39m, iterator)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2057\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   2058\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3212\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3210\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[1;32m-> 3212\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1272\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1269\u001b[0m final: Input\n\u001b[0;32m   1270\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[0;32m   1281\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5300\u001b[0m, in \u001b[0;36mRunnableBindingBase.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   5295\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5296\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   5297\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5298\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   5299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 5300\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   5301\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5302\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5303\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5304\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1290\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1287\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n\u001b[0;32m   1289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1290\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:411\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(\n\u001b[0;32m    406\u001b[0m         e,\n\u001b[0;32m    407\u001b[0m         response\u001b[38;5;241m=\u001b[39mLLMResult(\n\u001b[0;32m    408\u001b[0m             generations\u001b[38;5;241m=\u001b[39m[[generation]] \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    409\u001b[0m         ),\n\u001b[0;32m    410\u001b[0m     )\n\u001b[1;32m--> 411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]]))\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:391\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate_limiter\u001b[38;5;241m.\u001b[39macquire(blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m             chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_manager\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:509\u001b[0m, in \u001b[0;36mChatGroq._stream\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[0;32m    508\u001b[0m default_chunk_class: Type[BaseMessageChunk] \u001b[38;5;241m=\u001b[39m AIMessageChunk\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    511\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\groq\\_streaming.py:46\u001b[0m, in \u001b[0;36mStream.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[_T]:\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator:\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\groq\\_streaming.py:91\u001b[0m, in \u001b[0;36mStream.__stream__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     89\u001b[0m                 message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred during streaming\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 91\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m APIError(\n\u001b[0;32m     92\u001b[0m                 message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[0;32m     93\u001b[0m                 request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[0;32m     94\u001b[0m                 body\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     95\u001b[0m             )\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m process_data(data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m: sse\u001b[38;5;241m.\u001b[39mevent}, cast_to\u001b[38;5;241m=\u001b[39mcast_to, response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Ensure the entire stream is consumed\u001b[39;00m\n",
      "\u001b[1;31mAPIError\u001b[0m: Tool call validation failed: tool call validation failed: attempted to call tool 'teach_paper' which was not in request.tools"
     ]
    }
   ],
   "source": [
    "supervisor_result =supervisor_agent_executor.invoke({'input':'conduct the reseach on the given research paper', 'context':result_doc_agent['output']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de86ed8-27d6-4d7d-95c1-87ff06b0bb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
