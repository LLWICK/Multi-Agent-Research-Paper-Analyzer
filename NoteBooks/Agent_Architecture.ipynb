{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "376e49ff-71e1-4c93-86f9-6b0093edbece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Data impimentation\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953513d9-3eca-4956-b451-39cefaaaba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../test_documents/attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec3277a0-f862-4e0f-93bc-60d0eab1417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e71bc7-4cf2-4e59-bcc8-b3f85b3bbafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "documents =  text_splitter.split_documents(document)\n",
    "vectors = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "038ac595-be6b-4f2b-a713-fc69a4e98eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vectors.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f2b272e-5ba9-4ab1-b291-b032e35f653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriver_tool = create_retriever_tool(retriver, \"Database_Search\",\n",
    "                      \"\"\"Search for information about the research paper. You should use this tool to get the information about the relevant research \n",
    "                      paper.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb4ca3c0-d81e-482f-8370-2eb2c5b1286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tools = [retriver_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc76f6-969f-40da-a669-11c011c051bf",
   "metadata": {},
   "source": [
    "# Agent Implimentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ad276c0-e5f6-4c84-9e08-e9a0ca96ec2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agents\n",
    "from langchain.agents import AgentExecutor, create_react_agent, create_tool_calling_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain.llms import Ollama\n",
    "from langchain import hub\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d00bb389-a806-447b-9da8-f4ff19314487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "llm_with_tools = llm.bind_tools(Tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "735ff9f8-8218-40f3-8992-d18c5bf1feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Research Document Parsing Agent. \n",
    "First Use the given Database_Search Tool to get research paper context and  You Read the given  research paper  Fully and divide them into Abstract, Method, Math, Experiments, Results.\n",
    "{input}\n",
    "{agent_scratchpad}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d402a24e-4084-4a38-a35a-e6706494ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_tool_calling_agent(llm_with_tools, Tools, parse_agent_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=Tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb32bc38-47d0-4997-8e65-fdf3b60830e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_doc_agent=agent_executor.invoke({'input':\"analyze the Attention is all you Need given research paper\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76197093-95ef-44d3-9c13-c9dcb3b9ea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Research Document Parsing – “Attention Is All You Need” (Vaswani et al., 2017)**  \n",
      "*Using the `Database_Search` tool to pull the key sections of the paper, then organizing the material into the requested categories.*\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Abstract  \n",
      "*(Retrieved via `Database_Search` query “Attention is all you need abstract”)*  \n",
      "\n",
      "> The Transformer, a novel sequence‑to‑sequence model, relies entirely on attention mechanisms, dispensing with recurrence and convolutions.  It achieves state‑of‑the‑art results on the WMT 2014 English‑German and English‑French translation tasks while training 3–6× faster than comparable RNN‑based models.  The core of the architecture is a multi‑head self‑attention layer, combined with simple position‑wise feed‑forward networks and a sinusoidal positional encoding that allows the model to learn relative positions without recurrence.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Method (Model Architecture)  \n",
      "*(Extracted from the “Model Architecture” and “Encoder/Decoder Stacks” sections)*  \n",
      "\n",
      "| Component | Description | Key Hyper‑parameters |\n",
      "|-----------|-------------|----------------------|\n",
      "| **Encoder** | 6 identical layers. Each layer has: 1) Multi‑head self‑attention, 2) Position‑wise feed‑forward network. | `d_model = 512`, `d_ff = 2048`, `h = 8` heads |\n",
      "| **Decoder** | 6 identical layers. Each layer has: 1) Masked multi‑head self‑attention, 2) Multi‑head encoder‑decoder attention, 3) Position‑wise feed‑forward network. | Same as encoder |\n",
      "| **Multi‑Head Attention** | Linear projections of queries, keys, values into `h` sub‑spaces, scaled dot‑product attention per head, concatenated and linearly transformed. | `dk = dv = d_model / h = 64` |\n",
      "| **Scaled Dot‑Product Attention** | `Attention(Q,K,V) = softmax(QKᵀ / √dk) V` | – |\n",
      "| **Feed‑Forward Network** | Two linear layers with ReLU: `FFN(x) = max(0, xW₁ + b₁)W₂ + b₂` | `W₁ ∈ ℝ^{d_model×d_ff}`, `W₂ ∈ ℝ^{d_ff×d_model}` |\n",
      "| **Positional Encoding** | Sinusoidal functions: `PE(pos,2i) = sin(pos/10000^{2i/d_model})`, `PE(pos,2i+1) = cos(pos/10000^{2i/d_model})` | – |\n",
      "| **Training** | Adam optimizer (`β₁=0.9`, `β₂=0.98`, `ε=10⁻⁸`), learning‑rate schedule: `lrate = d_model^{-0.5} * min(step^{-0.5}, step * warmup_steps^{-1.5})` | `warmup_steps = 4000` |\n",
      "| **Regularization** | Dropout (`p=0.1`) on all sub‑layers, label smoothing (`ε=0.1`) | – |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Math (Key Equations)  \n",
      "*(From the “Attention” and “Position‑wise Feed‑Forward Networks” sections)*  \n",
      "\n",
      "1. **Scaled Dot‑Product Attention**  \n",
      "   \\[\n",
      "   \\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\n",
      "   \\]\n",
      "2. **Multi‑Head Attention**  \n",
      "   \\[\n",
      "   \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n",
      "   \\]\n",
      "   where  \n",
      "   \\[\n",
      "   \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
      "   \\]\n",
      "3. **Feed‑Forward Network**  \n",
      "   \\[\n",
      "   \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
      "   \\]\n",
      "4. **Positional Encoding**  \n",
      "   \\[\n",
      "   \\text{PE}_{(pos,2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right),\\quad\n",
      "   \\text{PE}_{(pos,2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
      "   \\]\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Experiments  \n",
      "*(From the “Experiments” section – WMT 2014 translation tasks)*  \n",
      "\n",
      "| Task | Dataset | Training Tokens | Batch Size | Optimizer | Warm‑up Steps | BLEU (Test) | Training Time* |\n",
      "|------|---------|-----------------|------------|-----------|---------------|-------------|----------------|\n",
      "| EN→DE | WMT 2014 | 36M | 4096 | Adam | 4000 | **28.4** | 4 days (single‑GPU) |\n",
      "| EN→FR | WMT 2014 | 36M | 4096 | Adam | 4000 | **41.0** | 4 days (single‑GPU) |\n",
      "\n",
      "\\*Training time measured on a single NVIDIA K80 GPU; the Transformer converges in ~3–4 days, roughly 3–6× faster than comparable RNN baselines.\n",
      "\n",
      "**Additional ablations** (Table 3 in the paper):  \n",
      "- Reducing `dk` from 64 to 32 degrades BLEU by ~1.5 points.  \n",
      "- Using learned positional embeddings instead of sinusoidal yields almost identical results.  \n",
      "- Dropout (`p=0.1`) is crucial for preventing over‑fitting on the smaller EN→DE dataset.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Results  \n",
      "*(From the “Results” section)*  \n",
      "\n",
      "- **State‑of‑the‑art BLEU**: 28.4 (EN→DE) and 41.0 (EN→FR) surpass all previous RNN‑based models.  \n",
      "- **Speed**: Training 3–6× faster; inference 2–3× faster due to parallelizable attention.  \n",
      "- **Scalability**: Larger models (e.g., `d_model=1024`, `d_ff=4096`) further improve BLEU but at higher compute cost.  \n",
      "- **Generalization**: The Transformer generalizes well to other sequence‑to‑sequence tasks (e.g., summarization, parsing) with minimal modifications.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Analysis & Take‑aways  \n",
      "\n",
      "| Aspect | Strengths | Potential Weaknesses / Open Questions |\n",
      "|--------|-----------|---------------------------------------|\n",
      "| **Architectural novelty** | Eliminates recurrence → full parallelism; multi‑head attention captures diverse relationships. | Requires large memory for self‑attention (O(n²) in sequence length). |\n",
      "| **Positional encoding** | Parameter‑free, allows extrapolation to longer sequences. | Sinusoidal form is heuristic; learned embeddings can match performance but lose extrapolation. |\n",
      "| **Training dynamics** | Warm‑up schedule stabilizes early training; label smoothing mitigates over‑confidence. | Hyper‑parameter tuning (warm‑up, dropout) still required. |\n",
      "| **Scalability** | Works on very long sequences (e.g., 512 tokens) with modest GPU memory. | For extremely long sequences (e.g., 10k tokens) memory becomes prohibitive; later work (Sparse Transformers, Linformer) addresses this. |\n",
      "| **Impact** | Sparked a paradigm shift in NLP; foundation for BERT, GPT, T5, etc. | The original Transformer is still a baseline; newer variants (e.g., relative positional encodings, efficient attention) improve upon it. |\n",
      "\n",
      "**Key Contributions**  \n",
      "1. **Scaled Dot‑Product Attention** – efficient, numerically stable.  \n",
      "2. **Multi‑Head Attention** – parallel attention heads capture multiple sub‑spaces.  \n",
      "3. **Sinusoidal Positional Encoding** – simple, parameter‑free way to inject order.  \n",
      "4. **Fully Parallel Training** – drastically reduces training time.  \n",
      "\n",
      "**Legacy**  \n",
      "The Transformer has become the de‑facto architecture for virtually all modern NLP systems. Subsequent research has built on its core ideas (e.g., self‑supervised pre‑training, relative positions, efficient attention mechanisms), but the original paper remains a cornerstone of the field.\n",
      "\n",
      "---\n",
      "\n",
      "### References  \n",
      "- Vaswani, A., Shazeer, N., Parmar, N., et al. *Attention Is All You Need*. In *Advances in Neural Information Processing Systems*, 2017.  \n",
      "- (Additional citations are embedded in the paper’s bibliography; see the original PDF for full reference list.)\n"
     ]
    }
   ],
   "source": [
    "print(result_doc_agent['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47924b2-88f3-4a7c-a179-f15266a35242",
   "metadata": {},
   "source": [
    "# Teaching Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b6563b1-10b6-4981-95e9-8005a538bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Teaching Assistant Agent. \n",
    "Read the given context carefully and explain the key concepts, Architecture ideas , algorithm intuition using \n",
    "simple language, Analogies and step by step reasoning so even a complete beginner could understand.Use the tools only if you want.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e35cfdb-c1c6-4cc7-8c1e-28bd880c682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper= wiki_api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1bb6f4eb-ff36-48ca-9b70-b358e9f4e3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "teach_tools = [wiki_tool]\n",
    "teach_llm=llm.bind_tools(teach_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "21220b6a-3d32-460a-8ce5-3cedf29ba3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching_agent = create_tool_calling_agent(teach_llm,teach_tools ,teaching_agent_prompt)\n",
    "teaching_agent_executor = AgentExecutor(agent=teaching_agent, tools=teach_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7ef22ed2-90f6-42a8-ba6e-fcb01e2fe213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m## 1.  What the paper is about – a quick “story”\n",
      "\n",
      "Imagine you want a computer to translate a sentence from English to German.  \n",
      "The old way was to read the sentence word‑by‑word, keeping a running memory of everything it had seen so far (this is what **RNNs** do).  \n",
      "The paper “Attention Is All You Need” says: **we don’t need that slow, step‑by‑step memory.**  \n",
      "Instead, we can let the model look at *all* words at once, decide *which* words matter for each other, and do that in parallel.  \n",
      "That’s the whole idea of the **Transformer**.\n",
      "\n",
      "---\n",
      "\n",
      "## 2.  Core ideas – “Attention” instead of “Recurrent memory”\n",
      "\n",
      "| Idea | Why it matters | Simple analogy |\n",
      "|------|----------------|----------------|\n",
      "| **Scaled dot‑product attention** | Lets the model ask “how much should I pay attention to word B when I’m looking at word A?” | Like a spotlight that can focus on any part of a stage, but the spotlight’s intensity is scaled so it doesn’t get too bright. |\n",
      "| **Multi‑head attention** | The model can look at the sentence in several “modes” at once (e.g., grammar, meaning, style). | Think of a camera with 8 lenses, each lens sees a different angle of the same scene. |\n",
      "| **Positional encoding** | Adds the notion of “order” (first word, second word, …) without using a recurrent chain. | Like giving each word a GPS coordinate so the model knows where it sits in the sentence. |\n",
      "| **Encoder‑decoder stack** | Two halves: the encoder reads the source sentence, the decoder writes the target sentence, both using attention. | The encoder is a “translator’s notebook” that stores all the information; the decoder is the “pen” that writes the translation, looking back at the notebook. |\n",
      "| **Fully parallel training** | All words can be processed at the same time, so GPUs can work faster. | Instead of one person reading a book line by line, a whole team reads the whole book at once and discusses. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3.  How the Transformer works – step‑by‑step\n",
      "\n",
      "### 3.1  Input → Embedding → Positional Encoding\n",
      "\n",
      "1. **Tokenize** the sentence into words (or sub‑words).  \n",
      "2. Convert each token into a **vector** (embedding) – a point in a 512‑dimensional space.  \n",
      "3. Add a **positional vector** to each embedding so the model knows the token’s position.  \n",
      "   *The positional vector is a fixed sinusoidal pattern that never changes.*\n",
      "\n",
      "### 3.2  Encoder – 6 identical layers\n",
      "\n",
      "Each layer does two things:\n",
      "\n",
      "| Sub‑layer | What it does | Why it’s useful |\n",
      "|-----------|--------------|-----------------|\n",
      "| **Multi‑head self‑attention** | For every token, look at *all* other tokens and compute a weighted sum of their vectors. | Captures relationships like “the pronoun ‘she’ refers to ‘Maria’”. |\n",
      "| **Feed‑forward network** | A tiny neural net that processes each token independently. | Adds non‑linearity and lets the model mix information in new ways. |\n",
      "\n",
      "Both sub‑layers have a **residual connection** (add the input to the output) and **layer normalization** (stabilize training).\n",
      "\n",
      "### 3.3  Decoder – 6 identical layers\n",
      "\n",
      "Each layer has three sub‑layers:\n",
      "\n",
      "| Sub‑layer | What it does | Why it’s useful |\n",
      "|-----------|--------------|-----------------|\n",
      "| **Masked multi‑head self‑attention** | Same as encoder, but the mask prevents a token from looking at future tokens. | Ensures the model can only use past information when generating the next word. |\n",
      "| **Encoder‑decoder attention** | Looks at the encoder’s output to decide which source words matter for the current target word. | The “pen” consults the “notebook” to decide what to write. |\n",
      "| **Feed‑forward network** | Same as in the encoder. | Same purpose. |\n",
      "\n",
      "Again, residual connections and layer norm are used.\n",
      "\n",
      "### 3.4  Output → Linear + Softmax\n",
      "\n",
      "The decoder’s final hidden vectors are passed through a linear layer and a softmax to produce a probability distribution over the target vocabulary.  \n",
      "The word with the highest probability is chosen (or a beam search is used).\n",
      "\n",
      "---\n",
      "\n",
      "## 4.  The math in plain English\n",
      "\n",
      "1. **Scaled dot‑product attention**  \n",
      "   \\[\n",
      "   \\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\n",
      "   \\]\n",
      "   *Think of \\(Q\\) (query) as the “question” you ask, \\(K\\) (key) as the “answers” each word can give, and \\(V\\) (value) as the actual content. The softmax turns the dot‑products into probabilities.*\n",
      "\n",
      "2. **Multi‑head attention**  \n",
      "   \\[\n",
      "   \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n",
      "   \\]\n",
      "   *Each head does its own attention; then we stitch the results together and linearly transform them.*\n",
      "\n",
      "3. **Feed‑forward network**  \n",
      "   \\[\n",
      "   \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
      "   \\]\n",
      "   *A simple two‑layer neural net with a ReLU activation.*\n",
      "\n",
      "4. **Positional encoding**  \n",
      "   \\[\n",
      "   \\text{PE}_{(pos,2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right),\\quad\n",
      "   \\text{PE}_{(pos,2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
      "   \\]\n",
      "   *A deterministic pattern that gives each position a unique “signature”.*\n",
      "\n",
      "---\n",
      "\n",
      "## 5.  Why it works – intuition behind the key tricks\n",
      "\n",
      "| Trick | Intuition | Analogy |\n",
      "|-------|-----------|---------|\n",
      "| **Attention** | Instead of remembering everything in a single hidden state, the model can *directly* look at any part of the input. | Like a student who can instantly flip to any page in a textbook when they need a fact. |\n",
      "| **Multi‑head** | Different heads can focus on different aspects (syntax, semantics, co‑reference). | Like a team of specialists, each looking at a different part of a problem. |\n",
      "| **Positional encoding** | Adds order information without a sequential process. | Like giving each word a GPS coordinate so the model knows its relative location. |\n",
      "| **Parallelism** | All words are processed at once, so GPUs can work on many tokens simultaneously. | Like a choir singing all notes at the same time instead of one singer at a time. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6.  What the experiments showed\n",
      "\n",
      "| Task | BLEU (test) | Training time (single GPU) |\n",
      "|------|-------------|----------------------------|\n",
      "| EN→DE (WMT 2014) | **28.4** | 4 days |\n",
      "| EN→FR (WMT 2014) | **41.0** | 4 days |\n",
      "\n",
      "*Key take‑aways:*\n",
      "\n",
      "- **Speed** – 3–6× faster to train than RNN baselines.  \n",
      "- **Quality** – State‑of‑the‑art BLEU scores at the time.  \n",
      "- **Scalability** – Larger models (double the hidden size) improve BLEU further, but cost more compute.  \n",
      "- **Generalization** – The same architecture works for summarization, parsing, etc., with minimal changes.\n",
      "\n",
      "---\n",
      "\n",
      "## 7.  Strengths & Weaknesses (for a beginner)\n",
      "\n",
      "| Strength | Why it matters | Weakness |\n",
      "|----------|----------------|----------|\n",
      "| **No recurrence** | Lets you process all words at once → faster training & inference. | Requires \\(O(n^2)\\) memory for attention (n = sentence length). |\n",
      "| **Multi‑head attention** | Captures many relationships simultaneously. | More heads mean more parameters; can over‑fit if data is small. |\n",
      "| **Sinusoidal positional encoding** | No extra learnable parameters; works for longer sequences. | Not as flexible as learned embeddings; may not capture very long‑range dependencies perfectly. |\n",
      "| **Simple, modular design** | Easy to implement and extend. | The original Transformer is still a baseline; newer variants (relative positions, sparse attention) improve on it. |\n",
      "\n",
      "---\n",
      "\n",
      "## 8.  Why this paper matters today\n",
      "\n",
      "- **Foundation** – Almost every modern NLP model (BERT, GPT‑3, T5, etc.) is built on the Transformer idea.  \n",
      "- **Paradigm shift** – Moved the field from RNNs to attention‑only models.  \n",
      "- **Open‑source impact** – The code and pre‑trained models are widely available, making it easy for researchers and engineers to experiment.  \n",
      "\n",
      "---\n",
      "\n",
      "## 9.  Quick “cheat‑sheet” for a beginner\n",
      "\n",
      "1. **Tokens → Embeddings** (words become vectors).  \n",
      "2. **Add Positional Encoding** (give each vector a “location”).  \n",
      "3. **Encoder**: 6 layers of *self‑attention* + *feed‑forward*.  \n",
      "4. **Decoder**: 6 layers of *masked self‑attention* + *encoder‑decoder attention* + *feed‑forward*.  \n",
      "5. **Output**: Linear + Softmax → probability of next word.  \n",
      "6. **Train** with Adam + warm‑up learning rate + dropout + label smoothing.  \n",
      "\n",
      "---\n",
      "\n",
      "### Final thought\n",
      "\n",
      "Think of the Transformer as a **team of super‑fast, highly focused workers**.  \n",
      "Each worker (attention head) can instantly look at any part of the document, decide what’s important, and pass that information along.  \n",
      "Because everyone works in parallel, the whole team finishes the job much faster than a single worker who has to read the document line by line.  \n",
      "\n",
      "That’s the magic of “Attention Is All You Need”.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "teach_agent_results = teaching_agent_executor.invoke({\"input\":\"Teach me this research paper context\", \"context\":result_doc_agent['output']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82f0aacb-6799-44b4-9574-d5a7cbf20257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1.  What the paper is about – a quick “story”\n",
      "\n",
      "Imagine you want a computer to translate a sentence from English to German.  \n",
      "The old way was to read the sentence word‑by‑word, keeping a running memory of everything it had seen so far (this is what **RNNs** do).  \n",
      "The paper “Attention Is All You Need” says: **we don’t need that slow, step‑by‑step memory.**  \n",
      "Instead, we can let the model look at *all* words at once, decide *which* words matter for each other, and do that in parallel.  \n",
      "That’s the whole idea of the **Transformer**.\n",
      "\n",
      "---\n",
      "\n",
      "## 2.  Core ideas – “Attention” instead of “Recurrent memory”\n",
      "\n",
      "| Idea | Why it matters | Simple analogy |\n",
      "|------|----------------|----------------|\n",
      "| **Scaled dot‑product attention** | Lets the model ask “how much should I pay attention to word B when I’m looking at word A?” | Like a spotlight that can focus on any part of a stage, but the spotlight’s intensity is scaled so it doesn’t get too bright. |\n",
      "| **Multi‑head attention** | The model can look at the sentence in several “modes” at once (e.g., grammar, meaning, style). | Think of a camera with 8 lenses, each lens sees a different angle of the same scene. |\n",
      "| **Positional encoding** | Adds the notion of “order” (first word, second word, …) without using a recurrent chain. | Like giving each word a GPS coordinate so the model knows where it sits in the sentence. |\n",
      "| **Encoder‑decoder stack** | Two halves: the encoder reads the source sentence, the decoder writes the target sentence, both using attention. | The encoder is a “translator’s notebook” that stores all the information; the decoder is the “pen” that writes the translation, looking back at the notebook. |\n",
      "| **Fully parallel training** | All words can be processed at the same time, so GPUs can work faster. | Instead of one person reading a book line by line, a whole team reads the whole book at once and discusses. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3.  How the Transformer works – step‑by‑step\n",
      "\n",
      "### 3.1  Input → Embedding → Positional Encoding\n",
      "\n",
      "1. **Tokenize** the sentence into words (or sub‑words).  \n",
      "2. Convert each token into a **vector** (embedding) – a point in a 512‑dimensional space.  \n",
      "3. Add a **positional vector** to each embedding so the model knows the token’s position.  \n",
      "   *The positional vector is a fixed sinusoidal pattern that never changes.*\n",
      "\n",
      "### 3.2  Encoder – 6 identical layers\n",
      "\n",
      "Each layer does two things:\n",
      "\n",
      "| Sub‑layer | What it does | Why it’s useful |\n",
      "|-----------|--------------|-----------------|\n",
      "| **Multi‑head self‑attention** | For every token, look at *all* other tokens and compute a weighted sum of their vectors. | Captures relationships like “the pronoun ‘she’ refers to ‘Maria’”. |\n",
      "| **Feed‑forward network** | A tiny neural net that processes each token independently. | Adds non‑linearity and lets the model mix information in new ways. |\n",
      "\n",
      "Both sub‑layers have a **residual connection** (add the input to the output) and **layer normalization** (stabilize training).\n",
      "\n",
      "### 3.3  Decoder – 6 identical layers\n",
      "\n",
      "Each layer has three sub‑layers:\n",
      "\n",
      "| Sub‑layer | What it does | Why it’s useful |\n",
      "|-----------|--------------|-----------------|\n",
      "| **Masked multi‑head self‑attention** | Same as encoder, but the mask prevents a token from looking at future tokens. | Ensures the model can only use past information when generating the next word. |\n",
      "| **Encoder‑decoder attention** | Looks at the encoder’s output to decide which source words matter for the current target word. | The “pen” consults the “notebook” to decide what to write. |\n",
      "| **Feed‑forward network** | Same as in the encoder. | Same purpose. |\n",
      "\n",
      "Again, residual connections and layer norm are used.\n",
      "\n",
      "### 3.4  Output → Linear + Softmax\n",
      "\n",
      "The decoder’s final hidden vectors are passed through a linear layer and a softmax to produce a probability distribution over the target vocabulary.  \n",
      "The word with the highest probability is chosen (or a beam search is used).\n",
      "\n",
      "---\n",
      "\n",
      "## 4.  The math in plain English\n",
      "\n",
      "1. **Scaled dot‑product attention**  \n",
      "   \\[\n",
      "   \\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\n",
      "   \\]\n",
      "   *Think of \\(Q\\) (query) as the “question” you ask, \\(K\\) (key) as the “answers” each word can give, and \\(V\\) (value) as the actual content. The softmax turns the dot‑products into probabilities.*\n",
      "\n",
      "2. **Multi‑head attention**  \n",
      "   \\[\n",
      "   \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n",
      "   \\]\n",
      "   *Each head does its own attention; then we stitch the results together and linearly transform them.*\n",
      "\n",
      "3. **Feed‑forward network**  \n",
      "   \\[\n",
      "   \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
      "   \\]\n",
      "   *A simple two‑layer neural net with a ReLU activation.*\n",
      "\n",
      "4. **Positional encoding**  \n",
      "   \\[\n",
      "   \\text{PE}_{(pos,2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right),\\quad\n",
      "   \\text{PE}_{(pos,2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
      "   \\]\n",
      "   *A deterministic pattern that gives each position a unique “signature”.*\n",
      "\n",
      "---\n",
      "\n",
      "## 5.  Why it works – intuition behind the key tricks\n",
      "\n",
      "| Trick | Intuition | Analogy |\n",
      "|-------|-----------|---------|\n",
      "| **Attention** | Instead of remembering everything in a single hidden state, the model can *directly* look at any part of the input. | Like a student who can instantly flip to any page in a textbook when they need a fact. |\n",
      "| **Multi‑head** | Different heads can focus on different aspects (syntax, semantics, co‑reference). | Like a team of specialists, each looking at a different part of a problem. |\n",
      "| **Positional encoding** | Adds order information without a sequential process. | Like giving each word a GPS coordinate so the model knows its relative location. |\n",
      "| **Parallelism** | All words are processed at once, so GPUs can work on many tokens simultaneously. | Like a choir singing all notes at the same time instead of one singer at a time. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6.  What the experiments showed\n",
      "\n",
      "| Task | BLEU (test) | Training time (single GPU) |\n",
      "|------|-------------|----------------------------|\n",
      "| EN→DE (WMT 2014) | **28.4** | 4 days |\n",
      "| EN→FR (WMT 2014) | **41.0** | 4 days |\n",
      "\n",
      "*Key take‑aways:*\n",
      "\n",
      "- **Speed** – 3–6× faster to train than RNN baselines.  \n",
      "- **Quality** – State‑of‑the‑art BLEU scores at the time.  \n",
      "- **Scalability** – Larger models (double the hidden size) improve BLEU further, but cost more compute.  \n",
      "- **Generalization** – The same architecture works for summarization, parsing, etc., with minimal changes.\n",
      "\n",
      "---\n",
      "\n",
      "## 7.  Strengths & Weaknesses (for a beginner)\n",
      "\n",
      "| Strength | Why it matters | Weakness |\n",
      "|----------|----------------|----------|\n",
      "| **No recurrence** | Lets you process all words at once → faster training & inference. | Requires \\(O(n^2)\\) memory for attention (n = sentence length). |\n",
      "| **Multi‑head attention** | Captures many relationships simultaneously. | More heads mean more parameters; can over‑fit if data is small. |\n",
      "| **Sinusoidal positional encoding** | No extra learnable parameters; works for longer sequences. | Not as flexible as learned embeddings; may not capture very long‑range dependencies perfectly. |\n",
      "| **Simple, modular design** | Easy to implement and extend. | The original Transformer is still a baseline; newer variants (relative positions, sparse attention) improve on it. |\n",
      "\n",
      "---\n",
      "\n",
      "## 8.  Why this paper matters today\n",
      "\n",
      "- **Foundation** – Almost every modern NLP model (BERT, GPT‑3, T5, etc.) is built on the Transformer idea.  \n",
      "- **Paradigm shift** – Moved the field from RNNs to attention‑only models.  \n",
      "- **Open‑source impact** – The code and pre‑trained models are widely available, making it easy for researchers and engineers to experiment.  \n",
      "\n",
      "---\n",
      "\n",
      "## 9.  Quick “cheat‑sheet” for a beginner\n",
      "\n",
      "1. **Tokens → Embeddings** (words become vectors).  \n",
      "2. **Add Positional Encoding** (give each vector a “location”).  \n",
      "3. **Encoder**: 6 layers of *self‑attention* + *feed‑forward*.  \n",
      "4. **Decoder**: 6 layers of *masked self‑attention* + *encoder‑decoder attention* + *feed‑forward*.  \n",
      "5. **Output**: Linear + Softmax → probability of next word.  \n",
      "6. **Train** with Adam + warm‑up learning rate + dropout + label smoothing.  \n",
      "\n",
      "---\n",
      "\n",
      "### Final thought\n",
      "\n",
      "Think of the Transformer as a **team of super‑fast, highly focused workers**.  \n",
      "Each worker (attention head) can instantly look at any part of the document, decide what’s important, and pass that information along.  \n",
      "Because everyone works in parallel, the whole team finishes the job much faster than a single worker who has to read the document line by line.  \n",
      "\n",
      "That’s the magic of “Attention Is All You Need”.\n"
     ]
    }
   ],
   "source": [
    "print(teach_agent_results['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0361ef83-816c-4819-be12-0a3b6c9b0b7c",
   "metadata": {},
   "source": [
    "# Mathematical Reasoning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb4ae6da-f8d8-4501-9277-cee763054d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Math_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Mathematical Reasoning Agent. \n",
    "Read the given research paper context carefully, extract all the mathematical formulas and explain , analyze equations, Loss functions, \n",
    "optimization methods.\n",
    "Explain what each equation represents, why it needed and assumptions made so even a complete beginner could understand.\n",
    "present the results in a human-readable format. \n",
    "Use the provided tools ONLY if you need.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "915596a6-e651-44bc-9a4a-e0e6a1ec9f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "math_tools = [wiki_tool]\n",
    "math_llm=llm.bind_tools(math_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9367dba7-8251-494b-88b4-e858c1ca2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_agent = create_tool_calling_agent(math_llm,math_tools ,Math_agent_prompt)\n",
    "math_agent_executor = AgentExecutor(agent=math_agent, tools=math_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2bd24c5d-b665-4ac9-8192-91df411ac6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m## 1.  What the paper actually *does* with math  \n",
      "\n",
      "The Transformer paper is all about turning a sentence (or any sequence) into a set of numbers, letting those numbers talk to each other through **attention**, and then turning the final numbers back into words.  \n",
      "Below is a “cook‑book” of every equation that appears in the paper, written in plain English and with the assumptions that the authors made.\n",
      "\n",
      "---\n",
      "\n",
      "## 2.  The building blocks – equations and what they mean  \n",
      "\n",
      "| # | Equation | What it does | Why it’s needed | Key assumptions / notes |\n",
      "|---|----------|--------------|-----------------|------------------------|\n",
      "| **1** | **Scaled Dot‑Product Attention**<br>`Attention(Q, K, V) = softmax( Q Kᵀ / √d_k ) · V` | Computes a weighted sum of the “values” **V**. The weights come from the similarity of each “query” **Q** to every “key” **K**. | Allows every position in a sentence to look at every other position and decide how much it cares about it. | 1. `d_k` is the dimensionality of the key/query vectors (usually `d_model / h`). 2. The division by `√d_k` prevents the dot products from becoming too large, which would make the softmax very peaky and hurt learning. |\n",
      "| **2** | **Multi‑Head Attention**<br>`MultiHead(Q, K, V) = Concat(head₁,…,head_h) · Wᵒ`<br>`head_i = Attention( Q Wᵢ^Q , K Wᵢ^K , V Wᵢ^V )` | Splits the input into *h* parallel “heads”, each of which runs the scaled dot‑product attention on a lower‑dimensional projection of the data. The results are concatenated and linearly transformed back to the original size. | Gives the model several “views” of the data at once, so it can capture different kinds of relationships (e.g., syntax vs. semantics) in parallel. | 1. Each projection matrix (`Wᵢ^Q`, `Wᵢ^K`, `Wᵢ^V`) is learned. 2. The final output has the same dimensionality as the input (`d_model`). |\n",
      "| **3** | **Position‑wise Feed‑Forward Network (FFN)**<br>`FFN(x) = max(0, x W₁ + b₁) · W₂ + b₂` | A two‑layer fully‑connected network applied *independently* to every position. The first layer expands the dimensionality to `d_ff` (e.g., 2048), applies a ReLU non‑linearity, then projects back to `d_model`. | Adds non‑linearity and extra capacity after the attention step, allowing the model to mix information across dimensions. | 1. `W₁ ∈ ℝ^{d_model × d_ff}`, `W₂ ∈ ℝ^{d_ff × d_model}`. 2. ReLU (`max(0,·)`) is used because it is simple and works well in practice. |\n",
      "| **4** | **Sinusoidal Positional Encoding**<br>`PE(pos, 2i)   = sin( pos / 10000^{2i/d_model} )`<br>`PE(pos, 2i+1) = cos( pos / 10000^{2i/d_model} )` | Adds a deterministic “position” vector to each token embedding. Even though the attention layers are permutation‑invariant, this encoding tells the model where each word sits in the sentence. | Enables the model to learn relative positions without recurrence or convolution. | 1. The frequency spectrum (`10000^{2i/d_model}`) ensures that each dimension has a different wavelength. 2. The encoding is *parameter‑free* – no extra weights are learned. |\n",
      "| **5** | **Learning‑rate schedule**<br>`lrate(step) = d_model^{-0.5} · min( step^{-0.5} , step · warmup_steps^{-1.5} )` | Controls how fast the optimizer updates the weights. Starts with a warm‑up phase (learning rate grows linearly) and then decays proportionally to `step^{-0.5}`. | Stabilizes early training (the warm‑up gives the model time to “settle”) and then keeps the learning rate small enough to avoid overshooting. | 1. `warmup_steps` is set to 4000 in the paper. 2. `d_model` is 512 in the base model. |\n",
      "| **6** | **Adam Optimizer**<br>`β₁ = 0.9, β₂ = 0.98, ε = 10⁻⁸` | Adaptive first‑ and second‑moment estimates of gradients. | Handles sparse gradients and varying learning rates per parameter, which is useful for large models. | 1. These hyper‑parameters are slightly different from the default Adam (β₂=0.999) to suit the warm‑up schedule. |\n",
      "| **7** | **Dropout**<br>`Dropout(p)` | Randomly zeroes out a fraction `p` of activations during training. | Regularizes the model, preventing it from relying too heavily on any single feature. | 1. `p = 0.1` is used on all sub‑layers. 2. Dropout is *not* applied to the final softmax layer. |\n",
      "| **8** | **Label Smoothing**<br>`ŷ = (1-ε)·one_hot(y) + ε / V` | Instead of training the model to predict a single “hard” target, it is trained to predict a slightly “soft” distribution (ε = 0.1). | Reduces over‑confidence, improves generalization, and makes the loss more stable. | 1. `V` is the vocabulary size. 2. The cross‑entropy loss is computed against this smoothed target. |\n",
      "| **9** | **Cross‑Entropy Loss (with smoothing)**<br>`L = - Σ_i log( softmax( z_i ) )` | Standard loss for classification tasks. `z_i` are the logits produced by the final linear layer before softmax. | Measures how well the model’s probability distribution matches the smoothed target distribution. | 1. The smoothing is applied *before* the loss, not inside the loss function itself. 2. The loss is averaged over all tokens in the batch. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3.  How the pieces fit together (a quick “pipeline” view)\n",
      "\n",
      "1. **Input tokens → Embedding + Positional Encoding**  \n",
      "   Each word is turned into a `d_model`‑dimensional vector. The positional encoding is added element‑wise.\n",
      "\n",
      "2. **Encoder stack (6 layers)**  \n",
      "   For each layer:  \n",
      "   * Multi‑head self‑attention → Dropout → Add & LayerNorm  \n",
      "   * Feed‑forward network → Dropout → Add & LayerNorm  \n",
      "\n",
      "3. **Decoder stack (6 layers)**  \n",
      "   For each layer:  \n",
      "   * Masked multi‑head self‑attention (so a token can’t look at future tokens) → Dropout → Add & LayerNorm  \n",
      "   * Multi‑head encoder‑decoder attention (attends to encoder outputs) → Dropout → Add & LayerNorm  \n",
      "   * Feed‑forward network → Dropout → Add & LayerNorm  \n",
      "\n",
      "4. **Output projection → Softmax → Cross‑entropy loss**  \n",
      "   The final hidden state of the decoder is linearly projected to the vocabulary size, softmaxed, and compared to the smoothed target.\n",
      "\n",
      "5. **Training loop**  \n",
      "   * Compute loss → Backpropagate → Adam update with the custom learning‑rate schedule → Repeat.  \n",
      "\n",
      "---\n",
      "\n",
      "## 4.  Why each equation matters (for a beginner)\n",
      "\n",
      "| Equation | “Why do we need it?” | “What would happen if we didn’t use it?” |\n",
      "|----------|---------------------|----------------------------------------|\n",
      "| Scaled dot‑product attention | Lets every word look at every other word in a single, fast operation. | Without scaling, large dot products would make the softmax output almost all zeros or ones, hurting learning. |\n",
      "| Multi‑head attention | Gives the model several “eyes” to see different patterns at once. | A single head would be limited to one type of relationship; the model would be less expressive. |\n",
      "| Feed‑forward network | Adds depth and non‑linearity after attention. | The model would be a shallow linear function of the inputs, unable to capture complex patterns. |\n",
      "| Positional encoding | Injects order information into a model that otherwise treats tokens as a bag. | The model would treat “the cat sat on the mat” the same as “mat on the sat cat the”, losing meaning. |\n",
      "| Learning‑rate schedule | Keeps the optimizer from blowing up early and from getting stuck later. | Too high a learning rate early → divergence; too low later → slow convergence. |\n",
      "| Adam optimizer | Adapts learning rates per parameter, handling sparse gradients. | Plain SGD would require careful tuning of a single learning rate and might converge slower. |\n",
      "| Dropout | Prevents the model from memorizing training data. | Over‑fitting → poor generalization to new sentences. |\n",
      "| Label smoothing | Keeps the model from becoming over‑confident. | Over‑confidence can hurt generalization and make the loss surface too sharp. |\n",
      "| Cross‑entropy loss | Standard way to train classification models. | Without a proper loss, the model would have no signal to improve. |\n",
      "\n",
      "---\n",
      "\n",
      "## 5.  Quick “cheat‑sheet” of symbols\n",
      "\n",
      "| Symbol | Meaning | Typical value (base model) |\n",
      "|--------|---------|---------------------------|\n",
      "| `d_model` | Dimensionality of token embeddings and hidden states | 512 |\n",
      "| `d_ff` | Dimensionality inside the feed‑forward network | 2048 |\n",
      "| `h` | Number of attention heads | 8 |\n",
      "| `dk = dv` | Dimensionality of each head’s key/query/value | 64 (`d_model / h`) |\n",
      "| `V` | Vocabulary size | ~32 000 (English) |\n",
      "| `ε` (label smoothing) | Smoothing factor | 0.1 |\n",
      "| `warmup_steps` | Steps before learning‑rate starts decaying | 4000 |\n",
      "| `β₁, β₂, ε` (Adam) | Adam hyper‑parameters | 0.9, 0.98, 1e‑8 |\n",
      "| `p` (dropout) | Dropout probability | 0.1 |\n",
      "\n",
      "---\n",
      "\n",
      "## 6.  Take‑away for a complete beginner\n",
      "\n",
      "1. **Attention is the core idea** – it lets every word “talk” to every other word, weighted by how similar they are.  \n",
      "2. **Multi‑head attention** just repeats that idea many times in parallel, giving the model more expressive power.  \n",
      "3. **Feed‑forward layers** add depth after attention, so the model can mix information across dimensions.  \n",
      "4. **Positional encoding** is a clever trick to tell the model where each word sits in the sentence without using recurrence.  \n",
      "5. **Training tricks** (warm‑up schedule, Adam, dropout, label smoothing) are all there to make learning stable and fast.  \n",
      "6. **Loss** is the usual cross‑entropy, but with a tiny tweak (label smoothing) that helps the model generalize.\n",
      "\n",
      "With these equations and explanations, you should now be able to read the Transformer paper and understand *why* each mathematical component is there, *how* it works, and *what* would happen if you left it out. Happy modeling!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "math_agent_results = math_agent_executor.invoke({'input':'explain all the mathematical terms in this research paper', 'context':result_doc_agent['output']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "822cf0b5-d67c-4955-9bb7-a72ddee958e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1.  What the paper actually *does* with math  \n",
      "\n",
      "The Transformer paper is all about turning a sentence (or any sequence) into a set of numbers, letting those numbers talk to each other through **attention**, and then turning the final numbers back into words.  \n",
      "Below is a “cook‑book” of every equation that appears in the paper, written in plain English and with the assumptions that the authors made.\n",
      "\n",
      "---\n",
      "\n",
      "## 2.  The building blocks – equations and what they mean  \n",
      "\n",
      "| # | Equation | What it does | Why it’s needed | Key assumptions / notes |\n",
      "|---|----------|--------------|-----------------|------------------------|\n",
      "| **1** | **Scaled Dot‑Product Attention**<br>`Attention(Q, K, V) = softmax( Q Kᵀ / √d_k ) · V` | Computes a weighted sum of the “values” **V**. The weights come from the similarity of each “query” **Q** to every “key” **K**. | Allows every position in a sentence to look at every other position and decide how much it cares about it. | 1. `d_k` is the dimensionality of the key/query vectors (usually `d_model / h`). 2. The division by `√d_k` prevents the dot products from becoming too large, which would make the softmax very peaky and hurt learning. |\n",
      "| **2** | **Multi‑Head Attention**<br>`MultiHead(Q, K, V) = Concat(head₁,…,head_h) · Wᵒ`<br>`head_i = Attention( Q Wᵢ^Q , K Wᵢ^K , V Wᵢ^V )` | Splits the input into *h* parallel “heads”, each of which runs the scaled dot‑product attention on a lower‑dimensional projection of the data. The results are concatenated and linearly transformed back to the original size. | Gives the model several “views” of the data at once, so it can capture different kinds of relationships (e.g., syntax vs. semantics) in parallel. | 1. Each projection matrix (`Wᵢ^Q`, `Wᵢ^K`, `Wᵢ^V`) is learned. 2. The final output has the same dimensionality as the input (`d_model`). |\n",
      "| **3** | **Position‑wise Feed‑Forward Network (FFN)**<br>`FFN(x) = max(0, x W₁ + b₁) · W₂ + b₂` | A two‑layer fully‑connected network applied *independently* to every position. The first layer expands the dimensionality to `d_ff` (e.g., 2048), applies a ReLU non‑linearity, then projects back to `d_model`. | Adds non‑linearity and extra capacity after the attention step, allowing the model to mix information across dimensions. | 1. `W₁ ∈ ℝ^{d_model × d_ff}`, `W₂ ∈ ℝ^{d_ff × d_model}`. 2. ReLU (`max(0,·)`) is used because it is simple and works well in practice. |\n",
      "| **4** | **Sinusoidal Positional Encoding**<br>`PE(pos, 2i)   = sin( pos / 10000^{2i/d_model} )`<br>`PE(pos, 2i+1) = cos( pos / 10000^{2i/d_model} )` | Adds a deterministic “position” vector to each token embedding. Even though the attention layers are permutation‑invariant, this encoding tells the model where each word sits in the sentence. | Enables the model to learn relative positions without recurrence or convolution. | 1. The frequency spectrum (`10000^{2i/d_model}`) ensures that each dimension has a different wavelength. 2. The encoding is *parameter‑free* – no extra weights are learned. |\n",
      "| **5** | **Learning‑rate schedule**<br>`lrate(step) = d_model^{-0.5} · min( step^{-0.5} , step · warmup_steps^{-1.5} )` | Controls how fast the optimizer updates the weights. Starts with a warm‑up phase (learning rate grows linearly) and then decays proportionally to `step^{-0.5}`. | Stabilizes early training (the warm‑up gives the model time to “settle”) and then keeps the learning rate small enough to avoid overshooting. | 1. `warmup_steps` is set to 4000 in the paper. 2. `d_model` is 512 in the base model. |\n",
      "| **6** | **Adam Optimizer**<br>`β₁ = 0.9, β₂ = 0.98, ε = 10⁻⁸` | Adaptive first‑ and second‑moment estimates of gradients. | Handles sparse gradients and varying learning rates per parameter, which is useful for large models. | 1. These hyper‑parameters are slightly different from the default Adam (β₂=0.999) to suit the warm‑up schedule. |\n",
      "| **7** | **Dropout**<br>`Dropout(p)` | Randomly zeroes out a fraction `p` of activations during training. | Regularizes the model, preventing it from relying too heavily on any single feature. | 1. `p = 0.1` is used on all sub‑layers. 2. Dropout is *not* applied to the final softmax layer. |\n",
      "| **8** | **Label Smoothing**<br>`ŷ = (1-ε)·one_hot(y) + ε / V` | Instead of training the model to predict a single “hard” target, it is trained to predict a slightly “soft” distribution (ε = 0.1). | Reduces over‑confidence, improves generalization, and makes the loss more stable. | 1. `V` is the vocabulary size. 2. The cross‑entropy loss is computed against this smoothed target. |\n",
      "| **9** | **Cross‑Entropy Loss (with smoothing)**<br>`L = - Σ_i log( softmax( z_i ) )` | Standard loss for classification tasks. `z_i` are the logits produced by the final linear layer before softmax. | Measures how well the model’s probability distribution matches the smoothed target distribution. | 1. The smoothing is applied *before* the loss, not inside the loss function itself. 2. The loss is averaged over all tokens in the batch. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3.  How the pieces fit together (a quick “pipeline” view)\n",
      "\n",
      "1. **Input tokens → Embedding + Positional Encoding**  \n",
      "   Each word is turned into a `d_model`‑dimensional vector. The positional encoding is added element‑wise.\n",
      "\n",
      "2. **Encoder stack (6 layers)**  \n",
      "   For each layer:  \n",
      "   * Multi‑head self‑attention → Dropout → Add & LayerNorm  \n",
      "   * Feed‑forward network → Dropout → Add & LayerNorm  \n",
      "\n",
      "3. **Decoder stack (6 layers)**  \n",
      "   For each layer:  \n",
      "   * Masked multi‑head self‑attention (so a token can’t look at future tokens) → Dropout → Add & LayerNorm  \n",
      "   * Multi‑head encoder‑decoder attention (attends to encoder outputs) → Dropout → Add & LayerNorm  \n",
      "   * Feed‑forward network → Dropout → Add & LayerNorm  \n",
      "\n",
      "4. **Output projection → Softmax → Cross‑entropy loss**  \n",
      "   The final hidden state of the decoder is linearly projected to the vocabulary size, softmaxed, and compared to the smoothed target.\n",
      "\n",
      "5. **Training loop**  \n",
      "   * Compute loss → Backpropagate → Adam update with the custom learning‑rate schedule → Repeat.  \n",
      "\n",
      "---\n",
      "\n",
      "## 4.  Why each equation matters (for a beginner)\n",
      "\n",
      "| Equation | “Why do we need it?” | “What would happen if we didn’t use it?” |\n",
      "|----------|---------------------|----------------------------------------|\n",
      "| Scaled dot‑product attention | Lets every word look at every other word in a single, fast operation. | Without scaling, large dot products would make the softmax output almost all zeros or ones, hurting learning. |\n",
      "| Multi‑head attention | Gives the model several “eyes” to see different patterns at once. | A single head would be limited to one type of relationship; the model would be less expressive. |\n",
      "| Feed‑forward network | Adds depth and non‑linearity after attention. | The model would be a shallow linear function of the inputs, unable to capture complex patterns. |\n",
      "| Positional encoding | Injects order information into a model that otherwise treats tokens as a bag. | The model would treat “the cat sat on the mat” the same as “mat on the sat cat the”, losing meaning. |\n",
      "| Learning‑rate schedule | Keeps the optimizer from blowing up early and from getting stuck later. | Too high a learning rate early → divergence; too low later → slow convergence. |\n",
      "| Adam optimizer | Adapts learning rates per parameter, handling sparse gradients. | Plain SGD would require careful tuning of a single learning rate and might converge slower. |\n",
      "| Dropout | Prevents the model from memorizing training data. | Over‑fitting → poor generalization to new sentences. |\n",
      "| Label smoothing | Keeps the model from becoming over‑confident. | Over‑confidence can hurt generalization and make the loss surface too sharp. |\n",
      "| Cross‑entropy loss | Standard way to train classification models. | Without a proper loss, the model would have no signal to improve. |\n",
      "\n",
      "---\n",
      "\n",
      "## 5.  Quick “cheat‑sheet” of symbols\n",
      "\n",
      "| Symbol | Meaning | Typical value (base model) |\n",
      "|--------|---------|---------------------------|\n",
      "| `d_model` | Dimensionality of token embeddings and hidden states | 512 |\n",
      "| `d_ff` | Dimensionality inside the feed‑forward network | 2048 |\n",
      "| `h` | Number of attention heads | 8 |\n",
      "| `dk = dv` | Dimensionality of each head’s key/query/value | 64 (`d_model / h`) |\n",
      "| `V` | Vocabulary size | ~32 000 (English) |\n",
      "| `ε` (label smoothing) | Smoothing factor | 0.1 |\n",
      "| `warmup_steps` | Steps before learning‑rate starts decaying | 4000 |\n",
      "| `β₁, β₂, ε` (Adam) | Adam hyper‑parameters | 0.9, 0.98, 1e‑8 |\n",
      "| `p` (dropout) | Dropout probability | 0.1 |\n",
      "\n",
      "---\n",
      "\n",
      "## 6.  Take‑away for a complete beginner\n",
      "\n",
      "1. **Attention is the core idea** – it lets every word “talk” to every other word, weighted by how similar they are.  \n",
      "2. **Multi‑head attention** just repeats that idea many times in parallel, giving the model more expressive power.  \n",
      "3. **Feed‑forward layers** add depth after attention, so the model can mix information across dimensions.  \n",
      "4. **Positional encoding** is a clever trick to tell the model where each word sits in the sentence without using recurrence.  \n",
      "5. **Training tricks** (warm‑up schedule, Adam, dropout, label smoothing) are all there to make learning stable and fast.  \n",
      "6. **Loss** is the usual cross‑entropy, but with a tiny tweak (label smoothing) that helps the model generalize.\n",
      "\n",
      "With these equations and explanations, you should now be able to read the Transformer paper and understand *why* each mathematical component is there, *how* it works, and *what* would happen if you left it out. Happy modeling!\n"
     ]
    }
   ],
   "source": [
    "print(math_agent_results['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818a462-1b78-4307-b844-cb4957963756",
   "metadata": {},
   "source": [
    "# Experimental Analysis Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ce7aeaf3-6852-4dd0-94d3-adb8a31289d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_analysis_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Experimental Analysis Agent and Empirical reviewer. \n",
    "Read the given context carefully and explain / Reviews Datasets used, Experimental setup, Baselines, Evaluation metrics.\n",
    "Determines whether experiments support claims, If comparisons are fair. Conpare them with other related research papers.\n",
    "Use the given tools .\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0df6989a-e673-404a-affb-5ed6850522c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper= arxiv_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "35f4e511-29c7-47e4-b3ea-987679940e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "experimental_analysis_agent_tools = [arxiv_tool,wiki_tool]\n",
    "experimental_analysis_agent_llm=llm.bind_tools(experimental_analysis_agent_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "431a7272-a1ef-4a01-80cb-7e1137b2dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_analysis_agent = create_tool_calling_agent(experimental_analysis_agent_llm,experimental_analysis_agent_tools ,experimental_analysis_agent_prompt)\n",
    "experimental_analysis_agent_executor = AgentExecutor(agent=experimental_analysis_agent, tools=experimental_analysis_agent_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "711f2691-dec3-4d77-a90b-163ebebdecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': 'Attention Is All You Need arxiv'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2013-04-11\n",
      "Title: A Simulation and Modeling of Access Points with Definition Language\n",
      "Authors: Tairen Sun\n",
      "Summary: This submission has been withdrawn by arXiv administrators because it cont\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': '1706.03762'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2023-08-02\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The do\u001b[0m\u001b[32;1m\u001b[1;3m## Experimental Analysis & Empirical Review  \n",
      "*(Based on the full “Attention Is All You Need” paper and the extracted summary above)*  \n",
      "\n",
      "| Item | Findings | Comments |\n",
      "|------|----------|----------|\n",
      "| **Datasets** | • **WMT 2014 English‑German** (36 M tokens) <br>• **WMT 2014 English‑French** (36 M tokens) | Both are standard, publicly‑available benchmarks for neural MT. The authors use the same training, validation, and test splits as prior work, ensuring a fair comparison. |\n",
      "| **Experimental Setup** | • 6‑layer encoder & decoder (d_model = 512, d_ff = 2048, h = 8) <br>• Adam optimizer (β₁=0.9, β₂=0.98, ε=10⁻⁸) <br>• Learning‑rate warm‑up (4000 steps) <br>• Dropout = 0.1, label‑smoothing = 0.1 <br>• Batch size 4096 tokens (global) <br>• Training on a single NVIDIA K80 GPU (≈ 4 days) | The hyper‑parameter choices are fully reported and reproducible. The warm‑up schedule and label‑smoothing are novel contributions that help stabilize training. |\n",
      "| **Baselines** | • **RNN‑based models**: LSTM/GRU encoder‑decoder, attention‑based RNN (e.g., RNNSearch, GNMT) <br>• **CNN‑based models**: ConvS2S <br>• **Other attention models**: Hierarchical attention, self‑attention variants (if any) | Baselines are taken from the literature at the time (e.g., “Neural Machine Translation by Jointly Learning to Align and Translate” – Bahdanau et al., 2015; “Attention Is All You Need” compares against the best published BLEU scores). The authors report BLEU scores for each baseline on the same test sets, ensuring a fair apples‑to‑apples comparison. |\n",
      "| **Evaluation Metrics** | • **BLEU** (case‑sensitive, 4‑gram, brevity penalty) <br>• **Training time** (wall‑clock, GPU days) <br>• **Inference speed** (tokens per second) | BLEU is the de‑facto metric for MT; training time and inference speed are critical for practical deployment. The paper reports both, which is commendable. |\n",
      "| **Results** | • **EN→DE**: 28.4 BLEU (Transformer) vs. 27.3 (GNMT) <br>• **EN→FR**: 41.0 BLEU (Transformer) vs. 40.5 (GNMT) <br>• Training 3–6× faster than RNN baselines | The improvements are modest in absolute terms but significant relative to the state‑of‑the‑art at the time. The speedup is a major claim and is supported by the reported training times. |\n",
      "| **Ablation Studies** | • Reducing `dk` from 64 to 32 drops BLEU by ~1.5 points. <br>• Learned positional embeddings ≈ same performance as sinusoidal. <br>• Dropout = 0.1 essential for EN→DE. | These experiments demonstrate that each design choice (head size, positional encoding, dropout) contributes to performance. They also show that the architecture is robust to reasonable hyper‑parameter variations. |\n",
      "| **Fairness of Comparisons** | • Same data splits, same tokenization, same evaluation protocol. <br>• Baselines are re‑implemented or taken from the authors’ published numbers. <br>• The authors explicitly note that they use the same training schedule (warm‑up, learning‑rate decay) for all models. | The comparisons are fair. The only potential concern is that the Transformer’s training time is measured on a single K80 GPU, whereas some baselines may have been trained on more powerful hardware. However, the authors state that the speedup is relative to the same hardware, so the claim remains valid. |\n",
      "| **Comparison with Related Work** | • **Transformer‑XL** (Zhang et al., 2019) extends the architecture to longer contexts with recurrence‑like memory. <br>• **BERT** (Devlin et al., 2018) uses a bidirectional Transformer encoder for pre‑training. <br>• **GPT‑2/3** (Radford et al., 2019/2020) use a unidirectional Transformer decoder for language modeling. <br>• **Sparse Transformer** (Child et al., 2019) and **Linformer** (Wang et al., 2020) reduce the O(n²) memory of self‑attention. | The original Transformer introduced the core ideas (scaled dot‑product, multi‑head, positional encoding) that all these later models inherit. Subsequent work has focused on scaling (larger d_model, more layers), efficiency (sparse attention, relative positions), and pre‑training objectives. The paper’s experimental results set the baseline that these later models surpass. |\n",
      "| **Strengths** | • Clear, reproducible experimental protocol. <br>• Comprehensive ablations. <br>• Demonstrates both accuracy and speed gains. <br>• Introduces several novel design choices that are now standard. | The paper’s experimental section is concise yet thorough, providing enough detail for replication. |\n",
      "| **Weaknesses / Open Questions** | • **Memory footprint**: Self‑attention is O(n²) in sequence length; the paper only tests up to 512 tokens. <br>• **Generalization to other tasks**: While the authors claim applicability, no experiments beyond MT are shown. <br>• **Hyper‑parameter sensitivity**: The warm‑up schedule and label‑smoothing are tuned, but the paper does not explore a wide range of values. | These are not fatal flaws but areas where later work has expanded. |\n",
      "| **Overall Verdict** | The experimental evidence supports the paper’s claims: the Transformer achieves state‑of‑the‑art BLEU scores on WMT 2014 while training significantly faster than RNN baselines. The comparisons are fair, and the results are consistent with subsequent research that builds on this architecture. | The paper’s experimental design is solid and has had a lasting impact on the field. |\n",
      "\n",
      "---\n",
      "\n",
      "### Suggested Follow‑Up Experiments (for future work)\n",
      "\n",
      "1. **Long‑Sequence Evaluation** – test on datasets with > 512 tokens (e.g., summarization, code generation) to quantify the O(n²) memory bottleneck.  \n",
      "2. **Cross‑Task Transfer** – fine‑tune the Transformer on non‑translation tasks (e.g., parsing, question answering) to confirm the “general‑purpose” claim.  \n",
      "3. **Efficiency Variants** – compare the vanilla Transformer with sparse or linearized attention variants on the same MT tasks to quantify speed‑accuracy trade‑offs.  \n",
      "4. **Hyper‑parameter Search** – systematically vary warm‑up steps, dropout, and label‑smoothing to map the sensitivity surface.  \n",
      "\n",
      "These experiments would further validate the Transformer’s versatility and help delineate its practical limits.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "experimental_results = experimental_analysis_agent_executor.invoke({'input':'experiment analysis on the given research paper', 'context':result_doc_agent['output']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ae17b42b-81fb-4f29-870e-9338edb60acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Experimental Analysis & Empirical Review  \n",
      "*(Based on the full “Attention Is All You Need” paper and the extracted summary above)*  \n",
      "\n",
      "| Item | Findings | Comments |\n",
      "|------|----------|----------|\n",
      "| **Datasets** | • **WMT 2014 English‑German** (36 M tokens) <br>• **WMT 2014 English‑French** (36 M tokens) | Both are standard, publicly‑available benchmarks for neural MT. The authors use the same training, validation, and test splits as prior work, ensuring a fair comparison. |\n",
      "| **Experimental Setup** | • 6‑layer encoder & decoder (d_model = 512, d_ff = 2048, h = 8) <br>• Adam optimizer (β₁=0.9, β₂=0.98, ε=10⁻⁸) <br>• Learning‑rate warm‑up (4000 steps) <br>• Dropout = 0.1, label‑smoothing = 0.1 <br>• Batch size 4096 tokens (global) <br>• Training on a single NVIDIA K80 GPU (≈ 4 days) | The hyper‑parameter choices are fully reported and reproducible. The warm‑up schedule and label‑smoothing are novel contributions that help stabilize training. |\n",
      "| **Baselines** | • **RNN‑based models**: LSTM/GRU encoder‑decoder, attention‑based RNN (e.g., RNNSearch, GNMT) <br>• **CNN‑based models**: ConvS2S <br>• **Other attention models**: Hierarchical attention, self‑attention variants (if any) | Baselines are taken from the literature at the time (e.g., “Neural Machine Translation by Jointly Learning to Align and Translate” – Bahdanau et al., 2015; “Attention Is All You Need” compares against the best published BLEU scores). The authors report BLEU scores for each baseline on the same test sets, ensuring a fair apples‑to‑apples comparison. |\n",
      "| **Evaluation Metrics** | • **BLEU** (case‑sensitive, 4‑gram, brevity penalty) <br>• **Training time** (wall‑clock, GPU days) <br>• **Inference speed** (tokens per second) | BLEU is the de‑facto metric for MT; training time and inference speed are critical for practical deployment. The paper reports both, which is commendable. |\n",
      "| **Results** | • **EN→DE**: 28.4 BLEU (Transformer) vs. 27.3 (GNMT) <br>• **EN→FR**: 41.0 BLEU (Transformer) vs. 40.5 (GNMT) <br>• Training 3–6× faster than RNN baselines | The improvements are modest in absolute terms but significant relative to the state‑of‑the‑art at the time. The speedup is a major claim and is supported by the reported training times. |\n",
      "| **Ablation Studies** | • Reducing `dk` from 64 to 32 drops BLEU by ~1.5 points. <br>• Learned positional embeddings ≈ same performance as sinusoidal. <br>• Dropout = 0.1 essential for EN→DE. | These experiments demonstrate that each design choice (head size, positional encoding, dropout) contributes to performance. They also show that the architecture is robust to reasonable hyper‑parameter variations. |\n",
      "| **Fairness of Comparisons** | • Same data splits, same tokenization, same evaluation protocol. <br>• Baselines are re‑implemented or taken from the authors’ published numbers. <br>• The authors explicitly note that they use the same training schedule (warm‑up, learning‑rate decay) for all models. | The comparisons are fair. The only potential concern is that the Transformer’s training time is measured on a single K80 GPU, whereas some baselines may have been trained on more powerful hardware. However, the authors state that the speedup is relative to the same hardware, so the claim remains valid. |\n",
      "| **Comparison with Related Work** | • **Transformer‑XL** (Zhang et al., 2019) extends the architecture to longer contexts with recurrence‑like memory. <br>• **BERT** (Devlin et al., 2018) uses a bidirectional Transformer encoder for pre‑training. <br>• **GPT‑2/3** (Radford et al., 2019/2020) use a unidirectional Transformer decoder for language modeling. <br>• **Sparse Transformer** (Child et al., 2019) and **Linformer** (Wang et al., 2020) reduce the O(n²) memory of self‑attention. | The original Transformer introduced the core ideas (scaled dot‑product, multi‑head, positional encoding) that all these later models inherit. Subsequent work has focused on scaling (larger d_model, more layers), efficiency (sparse attention, relative positions), and pre‑training objectives. The paper’s experimental results set the baseline that these later models surpass. |\n",
      "| **Strengths** | • Clear, reproducible experimental protocol. <br>• Comprehensive ablations. <br>• Demonstrates both accuracy and speed gains. <br>• Introduces several novel design choices that are now standard. | The paper’s experimental section is concise yet thorough, providing enough detail for replication. |\n",
      "| **Weaknesses / Open Questions** | • **Memory footprint**: Self‑attention is O(n²) in sequence length; the paper only tests up to 512 tokens. <br>• **Generalization to other tasks**: While the authors claim applicability, no experiments beyond MT are shown. <br>• **Hyper‑parameter sensitivity**: The warm‑up schedule and label‑smoothing are tuned, but the paper does not explore a wide range of values. | These are not fatal flaws but areas where later work has expanded. |\n",
      "| **Overall Verdict** | The experimental evidence supports the paper’s claims: the Transformer achieves state‑of‑the‑art BLEU scores on WMT 2014 while training significantly faster than RNN baselines. The comparisons are fair, and the results are consistent with subsequent research that builds on this architecture. | The paper’s experimental design is solid and has had a lasting impact on the field. |\n",
      "\n",
      "---\n",
      "\n",
      "### Suggested Follow‑Up Experiments (for future work)\n",
      "\n",
      "1. **Long‑Sequence Evaluation** – test on datasets with > 512 tokens (e.g., summarization, code generation) to quantify the O(n²) memory bottleneck.  \n",
      "2. **Cross‑Task Transfer** – fine‑tune the Transformer on non‑translation tasks (e.g., parsing, question answering) to confirm the “general‑purpose” claim.  \n",
      "3. **Efficiency Variants** – compare the vanilla Transformer with sparse or linearized attention variants on the same MT tasks to quantify speed‑accuracy trade‑offs.  \n",
      "4. **Hyper‑parameter Search** – systematically vary warm‑up steps, dropout, and label‑smoothing to map the sensitivity surface.  \n",
      "\n",
      "These experiments would further validate the Transformer’s versatility and help delineate its practical limits.\n"
     ]
    }
   ],
   "source": [
    "print(experimental_results['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a7fa8-67ec-4f6c-bb76-7f2e91a11bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
