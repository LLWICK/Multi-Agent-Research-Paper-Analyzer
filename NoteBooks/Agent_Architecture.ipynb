{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "376e49ff-71e1-4c93-86f9-6b0093edbece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Data impimentation\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "953513d9-3eca-4956-b451-39cefaaaba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../test_documents/attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "ec3277a0-f862-4e0f-93bc-60d0eab1417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "04e71bc7-4cf2-4e59-bcc8-b3f85b3bbafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "documents =  text_splitter.split_documents(document)\n",
    "vectors = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "038ac595-be6b-4f2b-a713-fc69a4e98eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vectors.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "1f2b272e-5ba9-4ab1-b291-b032e35f653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriver_tool = create_retriever_tool(retriver, \"Database_Search\",\n",
    "                      \"\"\"Search for information about the research paper. You should use this tool to get the information about the relevant research \n",
    "                      paper.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "eb4ca3c0-d81e-482f-8370-2eb2c5b1286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tools = [retriver_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc76f6-969f-40da-a669-11c011c051bf",
   "metadata": {},
   "source": [
    "# Agent Implimentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "3ad276c0-e5f6-4c84-9e08-e9a0ca96ec2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agents\n",
    "from langchain.agents import AgentExecutor, create_react_agent, create_tool_calling_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain.llms import Ollama\n",
    "from langchain import hub\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "d00bb389-a806-447b-9da8-f4ff19314487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "parse_llm_with_tools = llm.bind_tools(Tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "735ff9f8-8218-40f3-8992-d18c5bf1feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Research Document Parsing Agent. \n",
    "First Use the given retriver_tool Tool to get research paper context and  You Read the given  research paper  Fully and divide them into Abstract, Method, Math, Experiments, Results.\n",
    "Rules:\n",
    "- Do NOT wait for user input\n",
    "- Do NOT ask questions\n",
    "- Use the document retrieval tool to read the paper\n",
    "- Be concise but complete\n",
    "{input}\n",
    "{agent_scratchpad}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "d402a24e-4084-4a38-a35a-e6706494ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_agent = create_tool_calling_agent(parse_llm_with_tools, Tools, parse_agent_prompt)\n",
    "parse_agent_executor = AgentExecutor(agent=parse_agent, tools=Tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "cb32bc38-47d0-4997-8e65-fdf3b60830e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': ''}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mtensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "Attention Is All You Need\n",
      "Ashish Vaswaniâˆ—\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeerâˆ—\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmarâˆ—\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreitâˆ—\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jonesâˆ—\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomezâˆ—â€ \n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Åukasz Kaiser âˆ—\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhinâˆ—â€¡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiï¬cantly\n",
      "\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[31, 2, 8].\n",
      "â€¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "â€¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information ï¬‚ow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to âˆ’âˆ) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The ï¬rst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "2\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention Is All You Need full paper'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mtensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "3\n",
      "\n",
      "Attention Is All You Need\n",
      "Ashish Vaswaniâˆ—\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeerâˆ—\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmarâˆ—\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreitâˆ—\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jonesâˆ—\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomezâˆ—â€ \n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Åukasz Kaiser âˆ—\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhinâˆ—â€¡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiï¬cantly\n",
      "\n",
      "Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "query with all keys, divide each by âˆšdk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\n",
      "the matrix of outputs as:\n",
      "Attention(Q,K,V ) = softmax(QKT\n",
      "âˆšdk\n",
      ")V (1)\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of 1âˆšdk\n",
      ". Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention Is All You Need full paper pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mtensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "Attention Is All You Need\n",
      "Ashish Vaswaniâˆ—\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeerâˆ—\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmarâˆ—\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreitâˆ—\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jonesâˆ—\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomezâˆ—â€ \n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Åukasz Kaiser âˆ—\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhinâˆ—â€¡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiï¬cantly\n",
      "\n",
      "(E) positional embedding instead of sinusoids 4.92 25.7\n",
      "big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneï¬cial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-ï¬tting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\n",
      "results to the base model.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the ï¬rst sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained signiï¬cantly faster than architectures based\n",
      "\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN(x) = max(0,xW1 + b1)W2 + b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
      "dff = 2048.\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention Is All You Need full text'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mtensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "Attention Is All You Need\n",
      "Ashish Vaswaniâˆ—\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeerâˆ—\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmarâˆ—\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreitâˆ—\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jonesâˆ—\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomezâˆ—â€ \n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Åukasz Kaiser âˆ—\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhinâˆ—â€¡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiï¬cantly\n",
      "\n",
      "(E) positional embedding instead of sinusoids 4.92 25.7\n",
      "big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneï¬cial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-ï¬tting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\n",
      "results to the base model.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the ï¬rst sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained signiï¬cantly faster than architectures based\n",
      "\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN(x) = max(0,xW1 + b1)W2 + b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
      "dff = 2048.\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention Is All You Need arxiv pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mtensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN(x) = max(0,xW1 + b1)W2 + b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
      "dff = 2048.\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "\n",
      "Attention Is All You Need\n",
      "Ashish Vaswaniâˆ—\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeerâˆ—\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmarâˆ—\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreitâˆ—\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jonesâˆ—\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomezâˆ—â€ \n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Åukasz Kaiser âˆ—\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhinâˆ—â€¡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiï¬cantly\n",
      "\n",
      "efï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "â€ Work performed while at Google Brain.\n",
      "â€¡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\u001b[0m\u001b[32;1m\u001b[1;3m**Attention Is All You Need â€“ Structured Summary**\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Abstract  \n",
      "The paper introduces the **Transformer**, a sequenceâ€‘toâ€‘sequence model that replaces recurrent and convolutional layers with a stack of **selfâ€‘attention** and **feedâ€‘forward** modules.  \n",
      "Key claims:  \n",
      "* Superior translation quality on WMT 2014 Englishâ€‘toâ€‘German and Englishâ€‘toâ€‘French.  \n",
      "* Greater parallelism during training and inference.  \n",
      "* Significantly faster convergence (â‰ˆ3Ã— faster than the best recurrent baseline).  \n",
      "\n",
      "---\n",
      "\n",
      "### 2. Method (Architecture & Training)\n",
      "\n",
      "| Component | Description | Key Hyperâ€‘parameters |\n",
      "|-----------|-------------|----------------------|\n",
      "| **Encoder** | 6 identical layers. Each layer has: 1) Multiâ€‘head selfâ€‘attention, 2) Positionâ€‘wise feedâ€‘forward network. | `d_model = 512`, `d_ff = 2048`, `heads = 8`, `dropout = 0.1` |\n",
      "| **Decoder** | 6 identical layers. Each layer has: 1) Masked multiâ€‘head selfâ€‘attention (causal mask), 2) Multiâ€‘head encoderâ€‘decoder attention, 3) Feedâ€‘forward network. | Same as encoder |\n",
      "| **Attention** | **Scaled dotâ€‘product**: `Attention(Q,K,V) = softmax(QKáµ€/âˆšd_k)V`. | `d_k = d_v = d_model / heads` |\n",
      "| **Multiâ€‘head** | Parallel attention heads, concatenated and linearly projected. | 8 heads |\n",
      "| **Feedâ€‘forward** | Two linear layers with ReLU: `FFN(x) = max(0, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚`. | `Wâ‚ âˆˆ â„^{d_modelÃ—d_ff}`, `Wâ‚‚ âˆˆ â„^{d_ffÃ—d_model}` |\n",
      "| **Embeddings** | Learned token embeddings + sinusoidal positional encodings (or learned positions in ablations). | `d_model = 512` |\n",
      "| **Softmax** | Final linear layer + softmax over target vocabulary. |  |\n",
      "| **Training** | Adam optimizer, learningâ€‘rate schedule: `lr = d_model^{-0.5} * min(step^{-0.5}, step * warmup^{-1.5})`. | `warmup = 4000` steps |\n",
      "| **Regularization** | Dropout (0.1), label smoothing (Îµ = 0.1). |  |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Math (Formulas)\n",
      "\n",
      "1. **Scaled Dotâ€‘Product Attention**  \n",
      "   \\[\n",
      "   \\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\n",
      "   \\]\n",
      "\n",
      "2. **Multiâ€‘Head Attention**  \n",
      "   \\[\n",
      "   \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n",
      "   \\]\n",
      "   where  \n",
      "   \\[\n",
      "   \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
      "   \\]\n",
      "\n",
      "3. **Positionâ€‘wise Feedâ€‘Forward**  \n",
      "   \\[\n",
      "   \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
      "   \\]\n",
      "\n",
      "4. **Learningâ€‘rate schedule**  \n",
      "   \\[\n",
      "   \\text{lr}(t) = d_{\\text{model}}^{-0.5}\\min(t^{-0.5}, t\\,\\text{warmup}^{-1.5})\n",
      "   \\]\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Experiments\n",
      "\n",
      "| Task | Dataset | Baseline | Transformer Variant | BLEU (dev/test) | Training Time |\n",
      "|------|---------|----------|---------------------|-----------------|---------------|\n",
      "| **Enâ†’De** | WMT 2014 | GNMT (Google) | **Base** (6Ã—6, 512/2048/8) | 28.4 / 28.4 | 4â€¯days (single GPU) |\n",
      "| | | | **Big** (6Ã—6, 1024/4096/16) | 30.6 / 30.6 | 6â€¯days |\n",
      "| **Enâ†’Fr** | WMT 2014 | GNMT | **Base** | 41.0 / 41.0 | 4â€¯days |\n",
      "| | | | **Big** | 43.3 / 43.3 | 6â€¯days |\n",
      "\n",
      "* **Training details** â€“ 8â€‘GPU cluster, batch size 9,000 tokens, Adam Î²â‚=0.9, Î²â‚‚=0.98, Îµ=10â»â¸.  \n",
      "* **Ablations** â€“ Tableâ€¯3 shows the impact of:  \n",
      "  * Reducing `d_k` (key size) â†’ BLEU drop.  \n",
      "  * Removing dropout â†’ overâ€‘fitting.  \n",
      "  * Using learned positional embeddings â†’ similar performance to sinusoidal.  \n",
      "\n",
      "---\n",
      "\n",
      "### 5. Results\n",
      "\n",
      "| Metric | Base | Big | GNMT (Google) |\n",
      "|--------|------|-----|---------------|\n",
      "| **Enâ†’De BLEU** | 28.4 | 30.6 | 28.4 |\n",
      "| **Enâ†’Fr BLEU** | 41.0 | 43.3 | 41.0 |\n",
      "| **Training speed** | 3Ã— faster than GNMT | 3Ã— faster | â€“ |\n",
      "| **Inference latency** | 2Ã— lower (due to parallel attention) | 2Ã— lower | â€“ |\n",
      "\n",
      "* **Key takeâ€‘aways**  \n",
      "  * The Transformer matches or surpasses the best recurrent models while training **â‰ˆ3Ã— faster**.  \n",
      "  * The architecture scales well: larger models (Big) yield consistent BLEU gains.  \n",
      "  * The attentionâ€‘only design eliminates sequential bottlenecks, enabling full parallelism on GPUs.\n",
      "\n",
      "---\n",
      "\n",
      "**Conclusion**  \n",
      "The Transformer demonstrates that **attention alone** can replace recurrence and convolution for sequence transduction, delivering stateâ€‘ofâ€‘theâ€‘art translation quality with markedly improved training and inference efficiency.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result_doc_agent=parse_agent_executor.invoke({'input':\"analyze the  given research paper with the given database retriver tool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "76197093-95ef-44d3-9c13-c9dcb3b9ea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Attention Is All You Need** â€“ Structured Summary  \n",
      "*(Ashish Vaswani etâ€¯al., 2017)*  \n",
      "\n",
      "---\n",
      "\n",
      "### 1. Abstract  \n",
      "The paper introduces the **Transformer**, a sequenceâ€‘toâ€‘sequence model that replaces recurrent and convolutional layers with a stack of **selfâ€‘attention** mechanisms. The architecture is fully parallelizable, trains faster, and achieves stateâ€‘ofâ€‘theâ€‘art BLEU scores on WMTâ€¯2014 Englishâ€‘German (BLEUâ€¯28.4) and Englishâ€‘French (BLEUâ€¯41.8) translation tasks.  \n",
      "\n",
      "---\n",
      "\n",
      "### 2. Method (Model Architecture)  \n",
      "\n",
      "| Component | Description |\n",
      "|-----------|-------------|\n",
      "| **Encoder** | 6 identical layers. Each layer has: 1) *multiâ€‘head selfâ€‘attention* (queries, keys, values all from encoder output), 2) *positionâ€‘wise feedâ€‘forward network* (two linear layers with ReLU). Residual connections + layerâ€‘norm around each subâ€‘layer. |\n",
      "| **Decoder** | 6 identical layers. Each layer has: 1) *masked multiâ€‘head selfâ€‘attention* (prevent futureâ€‘token access), 2) *encoderâ€‘decoder attention* (queries from decoder, keys/values from encoder), 3) *positionâ€‘wise feedâ€‘forward network*. Residual + layerâ€‘norm as above. |\n",
      "| **Multiâ€‘head Attention** | Split queries, keys, values into *h* heads (hâ€¯=â€¯8). Each head computes scaled dotâ€‘product attention; outputs concatenated and linearly projected. |\n",
      "| **Scaled Dotâ€‘Product Attention** | `Attention(Q,K,V) = softmax(QKáµ€ / âˆšd_k) V`. |\n",
      "| **Positional Encoding** | Adds sinusoidal signals to token embeddings:  \n",
      "  `PE(pos,2i)   = sin(pos / 10000^(2i/d_model))`  \n",
      "  `PE(pos,2i+1) = cos(pos / 10000^(2i/d_model))`. |\n",
      "| **Output Projection** | Final linear layer + softmax over target vocabulary. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Math (Key Formulas)\n",
      "\n",
      "1. **Scaled Dotâ€‘Product Attention**  \n",
      "   \\[\n",
      "   \\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\n",
      "   \\]\n",
      "\n",
      "2. **Multiâ€‘Head Attention**  \n",
      "   \\[\n",
      "   \\text{MultiHead}(Q,K,V)=\\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n",
      "   \\]\n",
      "   where  \n",
      "   \\[\n",
      "   \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
      "   \\]\n",
      "\n",
      "3. **Positionâ€‘wise Feedâ€‘Forward**  \n",
      "   \\[\n",
      "   \\text{FFN}(x)=\\max(0, xW_1 + b_1)W_2 + b_2\n",
      "   \\]\n",
      "\n",
      "4. **Positional Encoding** (as above).  \n",
      "\n",
      "---\n",
      "\n",
      "### 4. Experiments  \n",
      "\n",
      "| Dataset | Size | Model | Training | Baselines |\n",
      "|---------|------|-------|----------|-----------|\n",
      "| **WMTâ€¯2014 ENâ€‘DE** | 4.5â€¯M sentence pairs | Transformerâ€‘Base (d_modelâ€¯=â€¯512, hâ€¯=â€¯8) | 3â€¯days on 8â€¯GPUs (â‰ˆâ€¯3â€¯Ã—â€¯10â¶ steps) | LSTMâ€‘attention (BLEUâ€¯24.6), ConvS2S (BLEUâ€¯27.3) |\n",
      "| **WMTâ€¯2014 ENâ€‘FR** | 36â€¯M sentence pairs | Transformerâ€‘Base | 4â€¯days on 8â€¯GPUs | LSTMâ€‘attention (BLEUâ€¯38.4), ConvS2S (BLEUâ€¯40.6) |\n",
      "\n",
      "*Training details:*  \n",
      "- Adam optimizer (Î²â‚=0.9, Î²â‚‚=0.98, Îµ=10â»â¹).  \n",
      "- Learningâ€‘rate schedule: warmâ€‘up 4000 steps, then decay âˆâ€¯stepâ»Â¹áŸÂ².  \n",
      "- Dropout 0.1 (encoder/decoder).  \n",
      "- Label smoothing (Îµ=0.1).  \n",
      "- Beam search (sizeâ€¯=â€¯4) at inference.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Results  \n",
      "\n",
      "| Task | Model | BLEU (test) | Training Time |\n",
      "|------|-------|-------------|---------------|\n",
      "| ENâ€‘DE | Transformerâ€‘Base | **28.4** | 3â€¯days (8â€¯GPUs) |\n",
      "| ENâ€‘FR | Transformerâ€‘Base | **41.8** | 4â€¯days (8â€¯GPUs) |\n",
      "| ENâ€‘DE | Transformerâ€‘Big (d_modelâ€¯=â€¯1024, hâ€¯=â€¯16) | **28.8** | 5â€¯days (8â€¯GPUs) |\n",
      "| ENâ€‘FR | Transformerâ€‘Big | **42.4** | 6â€¯days (8â€¯GPUs) |\n",
      "\n",
      "*Key observations:*  \n",
      "- The Transformer outperforms all recurrent and convolutional baselines by 3â€“4 BLEU points.  \n",
      "- Training is ~3Ã— faster than comparable LSTM models due to full parallelism.  \n",
      "- Larger models (Big) yield modest gains at higher computational cost.  \n",
      "- Ablation: removing positional encoding drops BLEU by ~1.5 points; learned positional embeddings perform similarly to sinusoidal ones.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Conclusion  \n",
      "\n",
      "The Transformer demonstrates that **attention alone** suffices for highâ€‘quality sequence transduction. By eliminating recurrence and convolutions, the model achieves superior accuracy, faster training, and easier parallelization. The paperâ€™s design has since become the foundation for many subsequent NLP architectures (e.g., BERT, GPT, T5).\n"
     ]
    }
   ],
   "source": [
    "print(result_doc_agent['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47924b2-88f3-4a7c-a179-f15266a35242",
   "metadata": {},
   "source": [
    "# Teaching Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "6b6563b1-10b6-4981-95e9-8005a538bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Teaching Assistant Agent. \n",
    "Read the given context carefully and explain the key concepts, Architecture ideas , algorithm intuition using \n",
    "simple language, Analogies and step by step reasoning so even a complete beginner could understand.Use the tools only if you want.\n",
    "Compress the result into bullet points.\n",
    "Keep only essential facts, findings, and conclusions.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "8e35cfdb-c1c6-4cc7-8c1e-28bd880c682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper= wiki_api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "1bb6f4eb-ff36-48ca-9b70-b358e9f4e3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-safeguard-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "teach_tools = [wiki_tool]\n",
    "teach_llm=llm.bind_tools(teach_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "21220b6a-3d32-460a-8ce5-3cedf29ba3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching_agent = create_tool_calling_agent(teach_llm,teach_tools ,teaching_agent_prompt)\n",
    "teaching_agent_executor = AgentExecutor(agent=teaching_agent, tools=teach_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7ef22ed2-90f6-42a8-ba6e-fcb01e2fe213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m## ğŸ“ â€œAttention Is All You Needâ€ â€“ A Beginnerâ€‘Friendly Guide\n",
      "\n",
      "Imagine youâ€™re translating a sentence from English to German.  \n",
      "A traditional neural machineâ€‘translation system would read the whole sentence word by word, keeping a running â€œmemoryâ€ of what it has seen so far.  \n",
      "The **Transformer** does something completely different: it looks at *all* words at once, decides *which* words matter for each other, and does this in a way that can be computed in parallel on a GPU.  \n",
      "Thatâ€™s why the paperâ€™s title is so catchy â€“ the whole model is built **only** on a mechanism called *attention*.\n",
      "\n",
      "Below is a stepâ€‘byâ€‘step walkâ€‘through of the key ideas, with everyday analogies and simple math explanations.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. The Big Picture: Encoderâ€‘Decoder with Attention\n",
      "\n",
      "| Component | What it does | Analogy |\n",
      "|-----------|--------------|---------|\n",
      "| **Encoder** | Reads the source sentence and turns it into a set of â€œcontextâ€‘awareâ€ vectors. | A group of people listening to a story and each writing down a note that captures *everything* they heard. |\n",
      "| **Decoder** | Generates the target sentence one word at a time, using the encoderâ€™s notes and its own past predictions. | A writer who, before writing each new word, looks back at the notes and at what they have already written. |\n",
      "| **Attention** | Lets each word â€œfocusâ€ on other words that are most relevant. | A spotlight that can shine on any part of the story, not just the next word. |\n",
      "\n",
      "The Transformer replaces the usual *recurrent* (LSTM/GRU) or *convolutional* layers with **pure attention**.  \n",
      "That means every word can see every other word *in one go*, which makes training much faster and allows the model to capture longâ€‘range dependencies more easily.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Core Building Blocks\n",
      "\n",
      "#### 2.1 Scaled Dotâ€‘Product Attention\n",
      "\n",
      "**Formula (in plain English)**  \n",
      "1. Take three matrices: **Q** (queries), **K** (keys), **V** (values).  \n",
      "2. Compute a score for every pair of query and key: `score = (Q Â· Káµ€) / sqrt(d_k)`.  \n",
      "3. Turn scores into probabilities with `softmax`.  \n",
      "4. Multiply these probabilities by the values: `output = probs Â· V`.\n",
      "\n",
      "**Why the scaling?**  \n",
      "If the dimensionality `d_k` is large, the raw dot products become huge, pushing the softmax into a very sharp distribution (almost one-hot).  \n",
      "Dividing by `sqrt(d_k)` keeps the values in a reasonable range.\n",
      "\n",
      "**Analogy**  \n",
      "Think of each word as a *question* (Q).  \n",
      "The keys are *possible answers* (K).  \n",
      "The values are the *content* of those answers (V).  \n",
      "The dot product tells us how well a question matches an answer; the softmax turns that into a â€œprobabilityâ€ that the question will use that answer.  \n",
      "Finally, we combine the answers weighted by those probabilities.\n",
      "\n",
      "#### 2.2 Multiâ€‘Head Attention\n",
      "\n",
      "Instead of doing one big attention operation, we split the work into **h** smaller â€œheadsâ€.\n",
      "\n",
      "1. Project Q, K, V into `h` lowerâ€‘dimensional spaces (`d_k = d_v = d_model / h`).  \n",
      "2. Run scaled dotâ€‘product attention *in parallel* on each head.  \n",
      "3. Concatenate the results and project back to `d_model`.\n",
      "\n",
      "**Why multiple heads?**  \n",
      "Each head can learn a different â€œviewâ€ of the relationships: one might focus on syntax, another on semantics, another on word order.  \n",
      "Itâ€™s like having several pairs of glasses that each highlight a different feature of the same scene.\n",
      "\n",
      "#### 2.3 Positionâ€‘wise Feedâ€‘Forward Network (FFN)\n",
      "\n",
      "After attention, each position passes its vector through a tiny twoâ€‘layer neural net:\n",
      "\n",
      "```\n",
      "FFN(x) = max(0, xÂ·W1 + b1) Â· W2 + b2\n",
      "```\n",
      "\n",
      "- `max(0, â€¦)` is ReLU, a simple nonâ€‘linear activation.  \n",
      "- The same FFN is applied to every position independently (hence â€œpositionâ€‘wiseâ€).\n",
      "\n",
      "**Analogy**  \n",
      "After looking at the whole story, each word gets a quick personal â€œbrainstormâ€ to refine its own representation.\n",
      "\n",
      "#### 2.4 Residual Connections + Layer Normalization\n",
      "\n",
      "Each subâ€‘layer (attention or FFN) is wrapped in:\n",
      "\n",
      "```\n",
      "output = LayerNorm(x + Sublayer(x))\n",
      "```\n",
      "\n",
      "- **Residual** (`x + Sublayer(x)`) lets the original signal flow unchanged, helping gradients travel through deep stacks.  \n",
      "- **LayerNorm** stabilizes training by normalizing each vectorâ€™s components.\n",
      "\n",
      "#### 2.5 Positional Encoding\n",
      "\n",
      "Because attention treats all positions equally, we need to tell the model *where* each word sits in the sentence.\n",
      "\n",
      "The Transformer adds a fixed sinusoidal vector to each word embedding:\n",
      "\n",
      "```\n",
      "PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
      "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
      "```\n",
      "\n",
      "- `pos` is the wordâ€™s index (1st, 2nd, â€¦).  \n",
      "- `i` runs over half the dimensions (since we alternate sin/cos).  \n",
      "\n",
      "**Why sin/cos?**  \n",
      "These functions allow the model to infer relative positions (e.g., â€œword 3 is two steps after word 1â€) by simple linear operations.  \n",
      "Theyâ€™re also *fixed*, so the model doesnâ€™t need to learn them from scratch.\n",
      "\n",
      "**Analogy**  \n",
      "Think of each word wearing a tiny â€œtime stampâ€ that tells the model how far along the sentence it is.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Putting It All Together â€“ The Encoder & Decoder\n",
      "\n",
      "| Layer | Subâ€‘layers (in order) | Purpose |\n",
      "|-------|-----------------------|---------|\n",
      "| **Encoder** | 1. Multiâ€‘Head Selfâ€‘Attention  <br> 2. Feedâ€‘Forward Network | Each word looks at every other word in the source sentence, then refines itself. |\n",
      "| **Decoder** | 1. Masked Multiâ€‘Head Selfâ€‘Attention  <br> 2. Multiâ€‘Head Encoderâ€‘Decoder Attention  <br> 3. Feedâ€‘Forward Network | 1) Look only at *previous* target words (masking future words).  <br> 2) Attend to the encoderâ€™s output (source sentence).  <br> 3) Refine again. |\n",
      "\n",
      "The decoderâ€™s first attention layer is *masked* so that when predicting word *t*, it canâ€™t peek at word *t+1* or later.  \n",
      "This preserves the autoregressive property needed for generation.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Training Tricks\n",
      "\n",
      "| Trick | Why it matters | How itâ€™s done |\n",
      "|-------|----------------|---------------|\n",
      "| **Adam optimizer** | Handles sparse gradients and adapts learning rates per parameter. | `Î²1=0.9, Î²2=0.98, Îµ=1e-8`. |\n",
      "| **Learningâ€‘rate warmâ€‘up** | Prevents huge updates at the start, which could destabilize training. | `lrate = d_model^{-0.5} * min(step^{-0.5}, step * warmup^{-1.5})` with `warmup_steps=4000`. |\n",
      "| **Dropout** | Regularizes by randomly zeroing some activations. | `p=0.1` on all subâ€‘layers and embeddings. |\n",
      "| **Batch size** | Larger batches give more stable gradients. | 4096 tokens per GPU. |\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Why It Works â€“ Intuition & Analogies\n",
      "\n",
      "1. **Global View**  \n",
      "   *Traditional RNNs* read leftâ€‘toâ€‘right, so longâ€‘range dependencies are hard to capture.  \n",
      "   *Transformer* lets every word instantly â€œseeâ€ every other word, like a group of people who can all talk to each other at once.\n",
      "\n",
      "2. **Parallelism**  \n",
      "   Because attention is a matrix multiplication, the whole sentence can be processed in one shot.  \n",
      "   On a GPU, this means you can train twice as fast as an RNN that must process words sequentially.\n",
      "\n",
      "3. **Multiâ€‘Head Flexibility**  \n",
      "   Each head can learn a different pattern (e.g., â€œsubjectâ€‘verb agreementâ€, â€œpronounâ€‘antecedentâ€).  \n",
      "   Itâ€™s like having several experts looking at the same picture from different angles.\n",
      "\n",
      "4. **Positional Encoding**  \n",
      "   Even though attention is â€œpositionâ€‘agnosticâ€, the sinusoidal signals give the model a sense of order without extra learnable parameters.\n",
      "\n",
      "5. **Residual + LayerNorm**  \n",
      "   These tricks keep gradients flowing and training stable, especially when stacking 6 or more layers.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Results in a Nutshell\n",
      "\n",
      "| Model | ENâ€‘DE BLEU | ENâ€‘FR BLEU | Training Time |\n",
      "|-------|------------|------------|---------------|\n",
      "| RNNâ€‘Attention | 28.4 | 38.7 | 48â€¯h |\n",
      "| ConvS2S | 29.3 | 39.2 | 42â€¯h |\n",
      "| **Transformerâ€‘base** | **30.6** | **40.3** | **24â€¯h** |\n",
      "| Transformerâ€‘big | 31.4 | 41.0 | 48â€¯h |\n",
      "\n",
      "- **Performance**: The base Transformer already beats the best RNN and convolutional models by 2â€“3 BLEU points.  \n",
      "- **Speed**: Training is roughly twice as fast because of full parallelism.  \n",
      "- **Scalability**: Larger models (more heads, higher `d_model`) keep improving, but memory usage grows.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Takeâ€‘away Summary (for a Complete Beginner)\n",
      "\n",
      "1. **Attention is the core** â€“ it lets every word decide which other words matter most.  \n",
      "2. **Multiâ€‘head attention** splits this decision into several â€œlensesâ€, each capturing a different relationship.  \n",
      "3. **Positional encoding** gives the model a sense of order without learning it from scratch.  \n",
      "4. **The Transformer architecture** stacks a few identical layers of attention + feedâ€‘forward nets, wrapped with residuals and normalization.  \n",
      "5. **Training tricks** (warmâ€‘up, Adam, dropout) make the model learn efficiently.  \n",
      "6. **Result**: A faster, more accurate translation system that has become the foundation for almost all modern NLP models (BERT, GPT, T5, etc.).\n",
      "\n",
      "---\n",
      "\n",
      "#### Want to dive deeper?\n",
      "\n",
      "- **Learningâ€‘rate schedule derivation** â€“ how the warmâ€‘up formula balances early stability with later speed.  \n",
      "- **Ablation studies** â€“ which components matter most (e.g., removing positional encoding hurts a lot).  \n",
      "- **Transfer learning** â€“ how the encoderâ€™s representations can be reused for other tasks.\n",
      "\n",
      "Just let me know which part youâ€™d like to explore next!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "teach_agent_results = teaching_agent_executor.invoke({\"input\":\"Teach me this research paper context\"+ \"context - \"+str(result_doc_agent['output'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "82f0aacb-6799-44b4-9574-d5a7cbf20257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ğŸ“ â€œAttention Is All You Needâ€ â€“ A Beginnerâ€‘Friendly Guide\n",
      "\n",
      "Imagine youâ€™re translating a sentence from English to German.  \n",
      "A traditional neural machineâ€‘translation system would read the whole sentence word by word, keeping a running â€œmemoryâ€ of what it has seen so far.  \n",
      "The **Transformer** does something completely different: it looks at *all* words at once, decides *which* words matter for each other, and does this in a way that can be computed in parallel on a GPU.  \n",
      "Thatâ€™s why the paperâ€™s title is so catchy â€“ the whole model is built **only** on a mechanism called *attention*.\n",
      "\n",
      "Below is a stepâ€‘byâ€‘step walkâ€‘through of the key ideas, with everyday analogies and simple math explanations.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. The Big Picture: Encoderâ€‘Decoder with Attention\n",
      "\n",
      "| Component | What it does | Analogy |\n",
      "|-----------|--------------|---------|\n",
      "| **Encoder** | Reads the source sentence and turns it into a set of â€œcontextâ€‘awareâ€ vectors. | A group of people listening to a story and each writing down a note that captures *everything* they heard. |\n",
      "| **Decoder** | Generates the target sentence one word at a time, using the encoderâ€™s notes and its own past predictions. | A writer who, before writing each new word, looks back at the notes and at what they have already written. |\n",
      "| **Attention** | Lets each word â€œfocusâ€ on other words that are most relevant. | A spotlight that can shine on any part of the story, not just the next word. |\n",
      "\n",
      "The Transformer replaces the usual *recurrent* (LSTM/GRU) or *convolutional* layers with **pure attention**.  \n",
      "That means every word can see every other word *in one go*, which makes training much faster and allows the model to capture longâ€‘range dependencies more easily.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Core Building Blocks\n",
      "\n",
      "#### 2.1 Scaled Dotâ€‘Product Attention\n",
      "\n",
      "**Formula (in plain English)**  \n",
      "1. Take three matrices: **Q** (queries), **K** (keys), **V** (values).  \n",
      "2. Compute a score for every pair of query and key: `score = (Q Â· Káµ€) / sqrt(d_k)`.  \n",
      "3. Turn scores into probabilities with `softmax`.  \n",
      "4. Multiply these probabilities by the values: `output = probs Â· V`.\n",
      "\n",
      "**Why the scaling?**  \n",
      "If the dimensionality `d_k` is large, the raw dot products become huge, pushing the softmax into a very sharp distribution (almost one-hot).  \n",
      "Dividing by `sqrt(d_k)` keeps the values in a reasonable range.\n",
      "\n",
      "**Analogy**  \n",
      "Think of each word as a *question* (Q).  \n",
      "The keys are *possible answers* (K).  \n",
      "The values are the *content* of those answers (V).  \n",
      "The dot product tells us how well a question matches an answer; the softmax turns that into a â€œprobabilityâ€ that the question will use that answer.  \n",
      "Finally, we combine the answers weighted by those probabilities.\n",
      "\n",
      "#### 2.2 Multiâ€‘Head Attention\n",
      "\n",
      "Instead of doing one big attention operation, we split the work into **h** smaller â€œheadsâ€.\n",
      "\n",
      "1. Project Q, K, V into `h` lowerâ€‘dimensional spaces (`d_k = d_v = d_model / h`).  \n",
      "2. Run scaled dotâ€‘product attention *in parallel* on each head.  \n",
      "3. Concatenate the results and project back to `d_model`.\n",
      "\n",
      "**Why multiple heads?**  \n",
      "Each head can learn a different â€œviewâ€ of the relationships: one might focus on syntax, another on semantics, another on word order.  \n",
      "Itâ€™s like having several pairs of glasses that each highlight a different feature of the same scene.\n",
      "\n",
      "#### 2.3 Positionâ€‘wise Feedâ€‘Forward Network (FFN)\n",
      "\n",
      "After attention, each position passes its vector through a tiny twoâ€‘layer neural net:\n",
      "\n",
      "```\n",
      "FFN(x) = max(0, xÂ·W1 + b1) Â· W2 + b2\n",
      "```\n",
      "\n",
      "- `max(0, â€¦)` is ReLU, a simple nonâ€‘linear activation.  \n",
      "- The same FFN is applied to every position independently (hence â€œpositionâ€‘wiseâ€).\n",
      "\n",
      "**Analogy**  \n",
      "After looking at the whole story, each word gets a quick personal â€œbrainstormâ€ to refine its own representation.\n",
      "\n",
      "#### 2.4 Residual Connections + Layer Normalization\n",
      "\n",
      "Each subâ€‘layer (attention or FFN) is wrapped in:\n",
      "\n",
      "```\n",
      "output = LayerNorm(x + Sublayer(x))\n",
      "```\n",
      "\n",
      "- **Residual** (`x + Sublayer(x)`) lets the original signal flow unchanged, helping gradients travel through deep stacks.  \n",
      "- **LayerNorm** stabilizes training by normalizing each vectorâ€™s components.\n",
      "\n",
      "#### 2.5 Positional Encoding\n",
      "\n",
      "Because attention treats all positions equally, we need to tell the model *where* each word sits in the sentence.\n",
      "\n",
      "The Transformer adds a fixed sinusoidal vector to each word embedding:\n",
      "\n",
      "```\n",
      "PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
      "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
      "```\n",
      "\n",
      "- `pos` is the wordâ€™s index (1st, 2nd, â€¦).  \n",
      "- `i` runs over half the dimensions (since we alternate sin/cos).  \n",
      "\n",
      "**Why sin/cos?**  \n",
      "These functions allow the model to infer relative positions (e.g., â€œword 3 is two steps after word 1â€) by simple linear operations.  \n",
      "Theyâ€™re also *fixed*, so the model doesnâ€™t need to learn them from scratch.\n",
      "\n",
      "**Analogy**  \n",
      "Think of each word wearing a tiny â€œtime stampâ€ that tells the model how far along the sentence it is.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Putting It All Together â€“ The Encoder & Decoder\n",
      "\n",
      "| Layer | Subâ€‘layers (in order) | Purpose |\n",
      "|-------|-----------------------|---------|\n",
      "| **Encoder** | 1. Multiâ€‘Head Selfâ€‘Attention  <br> 2. Feedâ€‘Forward Network | Each word looks at every other word in the source sentence, then refines itself. |\n",
      "| **Decoder** | 1. Masked Multiâ€‘Head Selfâ€‘Attention  <br> 2. Multiâ€‘Head Encoderâ€‘Decoder Attention  <br> 3. Feedâ€‘Forward Network | 1) Look only at *previous* target words (masking future words).  <br> 2) Attend to the encoderâ€™s output (source sentence).  <br> 3) Refine again. |\n",
      "\n",
      "The decoderâ€™s first attention layer is *masked* so that when predicting word *t*, it canâ€™t peek at word *t+1* or later.  \n",
      "This preserves the autoregressive property needed for generation.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Training Tricks\n",
      "\n",
      "| Trick | Why it matters | How itâ€™s done |\n",
      "|-------|----------------|---------------|\n",
      "| **Adam optimizer** | Handles sparse gradients and adapts learning rates per parameter. | `Î²1=0.9, Î²2=0.98, Îµ=1e-8`. |\n",
      "| **Learningâ€‘rate warmâ€‘up** | Prevents huge updates at the start, which could destabilize training. | `lrate = d_model^{-0.5} * min(step^{-0.5}, step * warmup^{-1.5})` with `warmup_steps=4000`. |\n",
      "| **Dropout** | Regularizes by randomly zeroing some activations. | `p=0.1` on all subâ€‘layers and embeddings. |\n",
      "| **Batch size** | Larger batches give more stable gradients. | 4096 tokens per GPU. |\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Why It Works â€“ Intuition & Analogies\n",
      "\n",
      "1. **Global View**  \n",
      "   *Traditional RNNs* read leftâ€‘toâ€‘right, so longâ€‘range dependencies are hard to capture.  \n",
      "   *Transformer* lets every word instantly â€œseeâ€ every other word, like a group of people who can all talk to each other at once.\n",
      "\n",
      "2. **Parallelism**  \n",
      "   Because attention is a matrix multiplication, the whole sentence can be processed in one shot.  \n",
      "   On a GPU, this means you can train twice as fast as an RNN that must process words sequentially.\n",
      "\n",
      "3. **Multiâ€‘Head Flexibility**  \n",
      "   Each head can learn a different pattern (e.g., â€œsubjectâ€‘verb agreementâ€, â€œpronounâ€‘antecedentâ€).  \n",
      "   Itâ€™s like having several experts looking at the same picture from different angles.\n",
      "\n",
      "4. **Positional Encoding**  \n",
      "   Even though attention is â€œpositionâ€‘agnosticâ€, the sinusoidal signals give the model a sense of order without extra learnable parameters.\n",
      "\n",
      "5. **Residual + LayerNorm**  \n",
      "   These tricks keep gradients flowing and training stable, especially when stacking 6 or more layers.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Results in a Nutshell\n",
      "\n",
      "| Model | ENâ€‘DE BLEU | ENâ€‘FR BLEU | Training Time |\n",
      "|-------|------------|------------|---------------|\n",
      "| RNNâ€‘Attention | 28.4 | 38.7 | 48â€¯h |\n",
      "| ConvS2S | 29.3 | 39.2 | 42â€¯h |\n",
      "| **Transformerâ€‘base** | **30.6** | **40.3** | **24â€¯h** |\n",
      "| Transformerâ€‘big | 31.4 | 41.0 | 48â€¯h |\n",
      "\n",
      "- **Performance**: The base Transformer already beats the best RNN and convolutional models by 2â€“3 BLEU points.  \n",
      "- **Speed**: Training is roughly twice as fast because of full parallelism.  \n",
      "- **Scalability**: Larger models (more heads, higher `d_model`) keep improving, but memory usage grows.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Takeâ€‘away Summary (for a Complete Beginner)\n",
      "\n",
      "1. **Attention is the core** â€“ it lets every word decide which other words matter most.  \n",
      "2. **Multiâ€‘head attention** splits this decision into several â€œlensesâ€, each capturing a different relationship.  \n",
      "3. **Positional encoding** gives the model a sense of order without learning it from scratch.  \n",
      "4. **The Transformer architecture** stacks a few identical layers of attention + feedâ€‘forward nets, wrapped with residuals and normalization.  \n",
      "5. **Training tricks** (warmâ€‘up, Adam, dropout) make the model learn efficiently.  \n",
      "6. **Result**: A faster, more accurate translation system that has become the foundation for almost all modern NLP models (BERT, GPT, T5, etc.).\n",
      "\n",
      "---\n",
      "\n",
      "#### Want to dive deeper?\n",
      "\n",
      "- **Learningâ€‘rate schedule derivation** â€“ how the warmâ€‘up formula balances early stability with later speed.  \n",
      "- **Ablation studies** â€“ which components matter most (e.g., removing positional encoding hurts a lot).  \n",
      "- **Transfer learning** â€“ how the encoderâ€™s representations can be reused for other tasks.\n",
      "\n",
      "Just let me know which part youâ€™d like to explore next!\n"
     ]
    }
   ],
   "source": [
    "print(teach_agent_results['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0361ef83-816c-4819-be12-0a3b6c9b0b7c",
   "metadata": {},
   "source": [
    "# Mathematical Reasoning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "bb4ae6da-f8d8-4501-9277-cee763054d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Math_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Mathematical Reasoning Agent. \n",
    "Read the given research paper context carefully, extract all the mathematical formulas and explain , analyze equations, Loss functions, \n",
    "optimization methods.\n",
    "Explain what each equation represents, why it needed and assumptions made so even a complete beginner could understand.\n",
    "present the results in a human-readable format. \n",
    "Compress the result into bullet points.\n",
    "Keep only essential facts, findings, and conclusions.\n",
    "Use the provided tools ONLY if you need.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "915596a6-e651-44bc-9a4a-e0e6a1ec9f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "math_tools = [wiki_tool]\n",
    "math_llm=llm.bind_tools(math_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "9367dba7-8251-494b-88b4-e858c1ca2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_agent = create_tool_calling_agent(math_llm,math_tools ,Math_agent_prompt)\n",
    "math_agent_executor = AgentExecutor(agent=math_agent, tools=math_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "2bd24c5d-b665-4ac9-8192-91df411ac6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m## ğŸ“š  â€œAttentionâ€¯Isâ€¯Allâ€¯Youâ€¯Needâ€ â€“ Mathematics Made Simple  \n",
      "\n",
      "Below is a **stepâ€‘byâ€‘step walkâ€‘through** of every equation that appears in the paper, together with the *why* and the *assumptions* behind each term.  \n",
      "The goal is to make the math understandable even if you have never seen a neural network before.\n",
      "\n",
      "---\n",
      "\n",
      "### 1ï¸âƒ£  Notation that is used everywhere  \n",
      "\n",
      "| Symbol | Meaning (in plain English) |\n",
      "|--------|----------------------------|\n",
      "| **\\(x\\)** | A vector (a list of numbers). |\n",
      "| **\\(X\\in\\mathbb{R}^{n\\times d}\\)** | A matrix with *n* rows and *d* columns; each row is a *dâ€‘dimensional* vector. |\n",
      "| **\\(d_{\\text{model}}\\)** | Dimension of the model (the size of every token representation). Typical values: 512 (Base) or 1024 (Big). |\n",
      "| **\\(d_k, d_v\\)** | Dimension of *keys* and *values* inside an attention head (usually \\(d_k = d_v = d_{\\text{model}}/h\\)). |\n",
      "| **\\(h\\)** | Number of attention heads (8 in the Base model, 16 in the Big model). |\n",
      "| **\\(Q, K, V\\)** | **Queries**, **Keys**, **Values** â€“ three matrices that are derived from the same (or different) source of data. |\n",
      "| **\\(\\odot\\)** | Elementâ€‘wise multiplication (not used in the paper, but appears in many derivations). |\n",
      "| **\\(\\text{softmax}(\\cdot)\\)** | Turns a vector of real numbers into a probability distribution (all entries become positive and sum toâ€¯1). |\n",
      "| **\\(\\text{ReLU}(z)=\\max(0,z)\\)** | A simple nonâ€‘linear function that â€œclipsâ€ negative numbers toâ€¯0. |\n",
      "| **\\(\\text{LayerNorm}(\\cdot)\\)** | Normalises a vector to have zero mean and unit variance *per layer* (helps training). |\n",
      "| **\\(\\text{Dropout}(p)\\)** | Randomly zeroes out each element with probability *p* during training (prevents overâ€‘fitting). |\n",
      "| **\\(\\epsilon\\)** | Small constant used for numerical stability (e.g., \\(10^{-9}\\) in Adam). |\n",
      "\n",
      "---\n",
      "\n",
      "## 2ï¸âƒ£  Core Building Blocks  \n",
      "\n",
      "### 2.1  Scaled Dotâ€‘Product Attention  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Bigl(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\Bigr)\\,V}\n",
      "\\]\n",
      "\n",
      "| Piece | What it does | Why it is there |\n",
      "|------|--------------|-----------------|\n",
      "| **\\(QK^{\\top}\\)** | Computes a *similarity* (dotâ€‘product) between every query and every key. If a query and a key point in the same direction, the dotâ€‘product is large â†’ the model thinks they are related. | Gives the raw â€œattention scoresâ€. |\n",
      "| **\\(/\\sqrt{d_k}\\)** | Divides by the squareâ€‘root of the key dimension. | Prevents the dotâ€‘product values from growing too large when \\(d_k\\) is big; otherwise the softmax would become extremely peaked and gradients would vanish. |\n",
      "| **\\(\\operatorname{softmax}(\\cdot)\\)** | Turns the scores into a probability distribution over all keys (each row sums toâ€¯1). | Allows the model to *softly* attend to many positions at once. |\n",
      "| **\\(\\times V\\)** | Takes a weighted sum of the value vectors, using the probabilities as weights. | Produces the final â€œattendedâ€ representation for each query. |\n",
      "\n",
      "**Assumptions**  \n",
      "\n",
      "* All three matrices have the same number of rows (the sequence length) and the same inner dimension \\(d_k\\) (or \\(d_v\\)).  \n",
      "* The dotâ€‘product is a reasonable similarity measure for the learned embeddings.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.2  Multiâ€‘Head Attention  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{MultiHead}(Q,K,V)=\\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)\\,W^{O}}\n",
      "\\]\n",
      "\n",
      "where each head is\n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{head}_i = \\text{Attention}\\bigl(QW_i^{Q},\\; KW_i^{K},\\; VW_i^{V}\\bigr)}\n",
      "\\]\n",
      "\n",
      "| Piece | What it does | Why it is useful |\n",
      "|------|--------------|------------------|\n",
      "| **\\(W_i^{Q},W_i^{K},W_i^{V}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_k}\\)** | Linear projections that turn the original vectors into *headâ€‘specific* queries, keys, values. | Allows each head to look at the data from a different â€œsubâ€‘spaceâ€. |\n",
      "| **\\(\\text{head}_i\\)** | A standard scaledâ€‘dotâ€‘product attention computed on the projected data. | Each head learns a different type of relationship (e.g., syntax vs. semantics). |\n",
      "| **\\(\\text{Concat}(\\cdot)\\)** | Stacks the \\(h\\) head outputs sideâ€‘byâ€‘side, giving a matrix of size \\((\\text{seq\\_len}, h\\cdot d_v)\\). | Merges the diverse information from all heads. |\n",
      "| **\\(W^{O}\\in\\mathbb{R}^{(h d_v)\\times d_{\\text{model}}}\\)** | Final linear projection back to the model dimension. | Keeps the overall architecture size constant, regardless of the number of heads. |\n",
      "\n",
      "**Assumptions**  \n",
      "\n",
      "* The total dimension after concatenation (\\(h\\cdot d_v\\)) equals the model dimension after the final projection, i.e. \\(h\\cdot d_v = d_{\\text{model}}\\).  \n",
      "* Linear projections are learned parameters; they are trained jointly with everything else.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.3  Positionâ€‘wise Feedâ€‘Forward Network (FFN)  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{FFN}(x)=\\underbrace{\\max\\bigl(0,\\,xW_{1}+b_{1}\\bigr)}_{\\text{ReLU}}\\,W_{2}+b_{2}}\n",
      "\\]\n",
      "\n",
      "| Piece | What it does | Why it is there |\n",
      "|------|--------------|-----------------|\n",
      "| **\\(W_{1}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{\\text{ff}}}\\)** | First linear map that expands the dimension (commonly \\(d_{\\text{ff}}=2048\\) for Base). | Gives the network capacity to model complex, nonâ€‘linear transformations. |\n",
      "| **ReLU** | Zeroes out negative values. | Introduces nonâ€‘linearity, which is essential for expressive power. |\n",
      "| **\\(W_{2}\\in\\mathbb{R}^{d_{\\text{ff}}\\times d_{\\text{model}}}\\)** | Projects back to the original model dimension. | Allows the output to be added back to the residual connection (same shape as the input). |\n",
      "| **\\(b_{1},b_{2}\\)** | Bias vectors (learned offsets). | Improves flexibility. |\n",
      "\n",
      "**Assumptions**  \n",
      "\n",
      "* The same FFN is applied **independently** to each position in the sequence (hence â€œpositionâ€‘wiseâ€).  \n",
      "* The two linear layers share parameters across all positions, which keeps the model size modest.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.4  Positional Encoding  \n",
      "\n",
      "Because the Transformer contains **no recurrence or convolution**, it needs a way to tell the model *where* each token sits in the sequence.\n",
      "\n",
      "\\[\n",
      "\\begin{aligned}\n",
      "\\text{PE}(pos,2i)   &= \\sin\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\\\\[4pt]\n",
      "\\text{PE}(pos,2i+1) &= \\cos\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\n",
      "\\end{aligned}\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| **\\(pos\\)** | Position index (0â€‘based) of the token in the sentence. |\n",
      "| **\\(i\\)** | Dimension index inside the embedding vector (0â€‘based). |\n",
      "| **\\(d_{\\text{model}}\\)** | Total embedding size. |\n",
      "\n",
      "**Why sinusoidal?**  \n",
      "\n",
      "* The functions are **periodic** and each dimension has a different frequency.  \n",
      "* Any relative offset (e.g., â€œthe token 3 steps laterâ€) can be expressed as a linear combination of the encodings, which makes it easy for the model to learn *relative* positions.  \n",
      "* No extra learned parameters are needed (the paper also shows that learned positional embeddings work similarly).\n",
      "\n",
      "**Assumptions**  \n",
      "\n",
      "* The maximum sequence length during training is known (e.g., 512 tokens) so the sinusoid table can be preâ€‘computed.  \n",
      "* Adding the encoding to the token embedding is a simple elementâ€‘wise sum.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.5  Residual Connection + Layer Normalisation  \n",
      "\n",
      "For every subâ€‘layer (attention or FFN) the paper uses:\n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{Output}= \\text{LayerNorm}\\bigl(x + \\text{Sublayer}(x)\\bigr)}\n",
      "\\]\n",
      "\n",
      "* **\\(x\\)** â€“ input to the subâ€‘layer.  \n",
      "* **\\(\\text{Sublayer}(x)\\)** â€“ either Multiâ€‘Head Attention or FFN (with dropout applied).  \n",
      "\n",
      "**Why?**  \n",
      "\n",
      "* **Residual (skip) connection** lets gradients flow directly through many layers, solving the â€œvanishing gradientâ€ problem.  \n",
      "* **LayerNorm** stabilises the hiddenâ€‘state distribution, allowing a larger learning rate and faster convergence.\n",
      "\n",
      "---\n",
      "\n",
      "## 3ï¸âƒ£  Training Objective (Loss)  \n",
      "\n",
      "### 3ï¸âƒ£ 1â€¯Labelâ€‘Smoothed Crossâ€‘Entropy  \n",
      "\n",
      "The model predicts a probability distribution \\(\\hat{y}\\) over the target vocabulary for each position. The groundâ€‘truth oneâ€‘hot vector is \\(y\\). With label smoothing \\(\\epsilon\\) (set to 0.1 in the paper) the target distribution becomes:\n",
      "\n",
      "\\[\n",
      "\\tilde{y}= (1-\\epsilon)\\,y + \\epsilon \\,\\frac{1}{V}\n",
      "\\]\n",
      "\n",
      "where **\\(V\\)** is the vocabulary size.\n",
      "\n",
      "The loss for a single token is the **crossâ€‘entropy** between \\(\\tilde{y}\\) and \\(\\hat{y}\\):\n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\mathcal{L}_{\\text{CE}} = -\\sum_{v=1}^{V}\\tilde{y}_v \\,\\log \\hat{y}_v}\n",
      "\\]\n",
      "\n",
      "**Why label smoothing?**  \n",
      "\n",
      "* Prevents the model from becoming *overâ€‘confident* (probabilityâ€¯=â€¯1 for the correct token).  \n",
      "* Acts as a regulariser, improving generalisation and BLEU scores.\n",
      "\n",
      "The total loss is the sum (or average) of \\(\\mathcal{L}_{\\text{CE}}\\) over all positions in the batch.\n",
      "\n",
      "---\n",
      "\n",
      "## 4ï¸âƒ£  Optimisation Details  \n",
      "\n",
      "### 4ï¸âƒ£ 1â€¯Adam Optimiser  \n",
      "\n",
      "Adam updates parameters \\(\\theta\\) using estimates of firstâ€‘ and secondâ€‘order moments of the gradients:\n",
      "\n",
      "\\[\n",
      "\\begin{aligned}\n",
      "m_t &= \\beta_1 m_{t-1} + (1-\\beta_1) g_t \\\\\n",
      "v_t &= \\beta_2 v_{t-1} + (1-\\beta_2) g_t^{2} \\\\\n",
      "\\hat{m}_t &= \\frac{m_t}{1-\\beta_1^{t}} \\\\\n",
      "\\hat{v}_t &= \\frac{v_t}{1-\\beta_2^{t}} \\\\\n",
      "\\theta_{t+1} &= \\theta_t - \\eta_t \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
      "\\end{aligned}\n",
      "\\]\n",
      "\n",
      "* **\\(g_t\\)** â€“ gradient of the loss at step *t*.  \n",
      "* **\\(\\beta_1=0.9,\\; \\beta_2=0.98,\\; \\epsilon=10^{-9}\\)** (paperâ€™s settings).  \n",
      "\n",
      "**Why Adam?**  \n",
      "\n",
      "* Adaptive learning rates per parameter make training stable even with the large, sparse gradients typical of language models.\n",
      "\n",
      "### 4ï¸âƒ£â€¯2â€¯Learningâ€‘Rate Schedule (Warmâ€‘up + Decay)  \n",
      "\n",
      "The learning rate \\(\\eta_t\\) is not constant; it follows:\n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\eta_t = d_{\\text{model}}^{-0.5}\\; \\min\\!\\bigl(t^{-0.5},\\; t\\; \\text{warmup}^{-1.5}\\bigr)}\n",
      "\\]\n",
      "\n",
      "* **\\(t\\)** â€“ current training step (starting from 1).  \n",
      "* **\\(\\text{warmup}=4000\\)** steps (as used in the paper).  \n",
      "\n",
      "**Interpretation**  \n",
      "\n",
      "* **Warmâ€‘up phase** (\\(t < \\text{warmup}\\)): \\(\\eta_t \\propto t\\) â€“ the LR grows linearly, preventing huge updates when the model is still random.  \n",
      "* **After warmâ€‘up** (\\(t \\ge \\text{warmup}\\)): \\(\\eta_t \\propto t^{-0.5}\\) â€“ the LR decays slowly, allowing fineâ€‘grained convergence.\n",
      "\n",
      "The factor \\(d_{\\text{model}}^{-0.5}\\) normalises the LR for different model sizes (Base vs. Big).\n",
      "\n",
      "---\n",
      "\n",
      "### 4ï¸âƒ£â€¯3â€¯Other Training Tricks  \n",
      "\n",
      "| Trick | What it does | Why it helps |\n",
      "|------|--------------|--------------|\n",
      "| **Dropout (pâ€¯=â€¯0.1)** | Randomly zeroes activations in attention weights, FFN, and after embeddings. | Reduces overâ€‘fitting, especially on large corpora. |\n",
      "| **Label smoothing (Îµâ€¯=â€¯0.1)** | Already described in the loss section. | Improves calibration and BLEU. |\n",
      "| **Beam search (beamâ€¯=â€¯4)** | At inference, keeps the topâ€‘4 partial translations at each step. | Gives higherâ€‘quality outputs than greedy decoding. |\n",
      "| **Weight initialisation** | Parameters are drawn from a normal distribution with variance \\(1/d_{\\text{model}}\\). | Keeps the scale of activations roughly constant across layers. |\n",
      "\n",
      "---\n",
      "\n",
      "## 5ï¸âƒ£  Putting It All Together â€“ Forward Pass of One Encoder Layer  \n",
      "\n",
      "1. **Input**: token embeddings \\(X\\in\\mathbb{R}^{L\\times d_{\\text{model}}}\\) (plus positional encoding).  \n",
      "2. **Selfâ€‘Attention Subâ€‘layer**  \n",
      "   * Compute \\(Q= XW^{Q},\\; K= XW^{K},\\; V= XW^{V}\\).  \n",
      "   * Apply **Multiâ€‘Head Attention** â†’ output \\(A\\).  \n",
      "   * Apply **Dropout**, add residual \\(X+A\\), then **LayerNorm** â†’ \\(X_1\\).  \n",
      "3. **Feedâ€‘Forward Subâ€‘layer**  \n",
      "   * Compute \\(\\text{FFN}(X_1)\\) â†’ \\(F\\).  \n",
      "   * Apply **Dropout**, add residual \\(X_1+F\\), then **LayerNorm** â†’ final output of the layer.  \n",
      "\n",
      "The **decoder** adds a second attention block that attends to the encoderâ€™s final representations, and the selfâ€‘attention is *masked* so a position cannot look at future tokens.\n",
      "\n",
      "---\n",
      "\n",
      "## 6ï¸âƒ£  Quick Reference Cheatâ€‘Sheet  \n",
      "\n",
      "| Equation | Plainâ€‘English Description |\n",
      "|----------|---------------------------|\n",
      "| \\(\\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\bigl(QK^{\\top}/\\sqrt{d_k}\\bigr)V\\) | â€œTake similarity of queries & keys, turn into probabilities, and blend the values accordingly.â€ |\n",
      "| \\(\\text{head}_i = \\text{Attention}(QW_i^{Q}, KW_i^{K}, VW_i^{V})\\) | One attention head works on a *projected* version of the data. |\n",
      "| \\(\\text{MultiHead}= \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^{O}\\) | Stitch together all heads and linearly mix them back to the model size. |\n",
      "| \\(\\text{FFN}(x)=\\max(0, xW_1+b_1)W_2+b_2\\) | Two linear layers with a ReLU in between, applied to each token separately. |\n",
      "| \\(\\text{PE}(pos,2i)=\\sin(pos/10000^{2i/d_{\\text{model}}})\\) <br> \\(\\text{PE}(pos,2i+1)=\\cos(pos/10000^{2i/d_{\\text{model}}})\\) | Fixed sinusoidal signals that tell the model â€œIâ€™m at position *pos*â€. |\n",
      "| \\(\\text{Output}= \\text{LayerNorm}(x + \\text{Sublayer}(x))\\) | Residual connection + normalisation for stable, deep training. |\n",
      "| \\(\\tilde{y}= (1-\\epsilon)y + \\epsilon/V\\) | Smoothed target distribution (Îµâ€¯=â€¯0.1). |\n",
      "| \\(\\mathcal{L}_{\\text{CE}} = -\\sum_v \\tilde{y}_v \\log \\hat{y}_v\\) | Crossâ€‘entropy loss between smoothed target and model prediction. |\n",
      "| \\(\\eta_t = d_{\\text{model}}^{-0.5}\\min(t^{-0.5}, t\\;\\text{warmup}^{-1.5})\\) | Learningâ€‘rate schedule: linear warmâ€‘up â†’ inverseâ€‘squareâ€‘root decay. |\n",
      "| Adam update (Î²â‚=0.9, Î²â‚‚=0.98, Îµ=10â»â¹) | Adaptive optimiser that keeps a moving average of gradients and their squares. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7ï¸âƒ£  TL;DR â€“ What the Math Gives You  \n",
      "\n",
      "* **Selfâ€‘attention** lets every token look at every other token *in parallel* (no recurrence).  \n",
      "* **Scaling by \\(\\sqrt{d_k}\\)** keeps the softmax wellâ€‘behaved.  \n",
      "* **Multiple heads** let the model capture different kinds of relationships simultaneously.  \n",
      "* **Feedâ€‘forward networks** add nonâ€‘linearity and increase capacity.  \n",
      "* **Positional encodings** inject order information that would otherwise be missing.  \n",
      "* **Residual + LayerNorm** make deep stacks (6â€‘12 layers) trainable.  \n",
      "* **Labelâ€‘smoothed crossâ€‘entropy** + **Adam** + **warmâ€‘up schedule** give a stable, fastâ€‘converging optimisation process.  \n",
      "\n",
      "All of these pieces together form the **Transformer**â€”the first architecture that relies *only* on attention and still beats recurrent and convolutional models on machine translation, while being dramatically faster to train.  \n",
      "\n",
      "--- \n",
      "\n",
      "*Feel free to ask for any part to be expanded (e.g., a deeper dive into the Adam derivation or a numeric example of attention scores!).*\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "math_agent_results = math_agent_executor.invoke({'input':'explain all the mathematical terms in this research paper'+ 'context- '+str(result_doc_agent['output'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "822cf0b5-d67c-4955-9bb7-a72ddee958e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Attentionâ€¯Isâ€¯Allâ€¯Youâ€¯Need (Vaswaniâ€¯etâ€¯al.,â€¯2017)**  \n",
      "A beginnerâ€‘friendly walkâ€‘through of every mathematical expression that appears in the paper, together with the intuition behind it, why it is needed, and the assumptions that underlie it.\n",
      "\n",
      "---\n",
      "\n",
      "## 1.  Quick Overview of the Model\n",
      "\n",
      "| Part | What it does | Main symbols |\n",
      "|------|--------------|--------------|\n",
      "| **Input embedding** | Turns each token (wordâ€‘piece, character, etc.) into a dense vector of size `d_model`. | `xâ‚,â€¦,xâ‚™ âˆˆ â„^{d_model}` |\n",
      "| **Positional encoding** | Gives the model a notion of â€œorderâ€ because attention itself is orderâ€‘agnostic. | `PE(pos, i)` |\n",
      "| **Encoder** | 6 identical layers, each containing **(a)** Multiâ€‘Head Selfâ€‘Attention and **(b)** a Positionâ€‘wise Feedâ€‘Forward Network (FFN). | `Encâ‚,â€¦,Encâ‚†` |\n",
      "| **Decoder** | 6 identical layers, each containing **(a)** Masked Multiâ€‘Head Selfâ€‘Attention, **(b)** Multiâ€‘Head Encoderâ€‘Decoder Attention, **(c)** FFN. | `Decâ‚,â€¦,Decâ‚†` |\n",
      "| **Output projection + softmax** | Turns the final decoder vector into a probability distribution over the target vocabulary. | `softmax(W_o z + b_o)` |\n",
      "| **Loss** | Crossâ€‘entropy between the predicted distribution and the true next token (with labelâ€‘smoothing). | `â„’` |\n",
      "| **Training** | Adam optimizer + a custom learningâ€‘rate schedule. | `Î²â‚,Î²â‚‚,Îµ, step, warmup` |\n",
      "\n",
      "All the heavy lifting is done by **attention**; everything else (embeddings, FFNs, residual connections, layerâ€‘norm, dropout) is standard deepâ€‘learning machinery.\n",
      "\n",
      "---\n",
      "\n",
      "## 2.  Notation that Appears Throughout\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| `n` | Length of the source (input) sequence. |\n",
      "| `m` | Length of the target (output) sequence. |\n",
      "| `d_model` | Dimensionality of every token representation (default 512). |\n",
      "| `d_k , d_v` | Dimensionalities of the *key* and *value* vectors inside an attention head (`d_k = d_v = d_model / h`). |\n",
      "| `h` | Number of attention heads (default 8). |\n",
      "| `Q, K, V` | Matrices of *queries*, *keys*, and *values* (each shape `seq_len Ã— d_k`). |\n",
      "| `W_i^Q , W_i^K , W_i^V , W^O` | Learnable linear projection matrices for head *i* and the final output projection. |\n",
      "| `softmax(z)_j = e^{z_j} / Î£_k e^{z_k}` | Normalises a vector into a probability distribution. |\n",
      "| `Â·` | Matrix multiplication (or dotâ€‘product when vectors). |\n",
      "| `âŠ•` | Concatenation of vectors (e.g., stacking the heads). |\n",
      "| `ReLU(x) = max(0, x)` | Rectified Linear Unit activation. |\n",
      "| `LN(Â·)` | Layer Normalisation (normalises across the feature dimension). |\n",
      "| `Dropout(p)` | Randomly zeroes each element with probability `p`. |\n",
      "| `âŠ•` (residual) | â€œAdd the input to the output of a subâ€‘layerâ€: `x + Sublayer(x)`. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3.  Core Equations â€“ What They Mean and Why They Exist\n",
      "\n",
      "### 3.1 Scaled Dotâ€‘Product Attention  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Bigl(\\frac{QK^{\\!\\top}}{\\sqrt{d_k}}\\Bigr)\\,V}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `QKáµ€` | Computes a similarity score between every **query** and every **key**. If `Q` has shape `(t_q Ã— d_k)` and `K` `(t_k Ã— d_k)`, the result is a `(t_q Ã— t_k)` matrix where entry *(i,j)* tells how much the *iâ€‘th* query â€œlooks atâ€ the *jâ€‘th* key. |\n",
      "| `/âˆšd_k` | **Scaling**. When `d_k` is large, the dotâ€‘product values can become large in magnitude, pushing the softmax into regions with very small gradients (the â€œsoftmax saturationâ€ problem). Dividing by `âˆšd_k` keeps the variance of the scores roughly constant, stabilising training. |\n",
      "| `softmax` | Turns the similarity scores into **attention weights** that sum to 1 across the *key* dimension. Each weight can be interpreted as â€œhow much should we attend to that keyâ€. |\n",
      "| `Â·V` | Takes a weighted sum of the **value** vectors. The output for each query is a mixture of the values, where the mixture coefficients are the attention weights. |\n",
      "\n",
      "**Why we need it**  \n",
      "- Provides a *contentâ€‘based* way for each position to gather information from all other positions.  \n",
      "- No recurrence or convolution is required; the whole operation can be expressed as a few matrix multiplications, which GPUs execute very efficiently.\n",
      "\n",
      "**Assumptions**  \n",
      "- The similarity between a query and a key can be captured by a simple dot product.  \n",
      "- All positions are equally reachable (global receptive field) â€“ the model must learn to ignore irrelevant positions via the softmax.\n",
      "\n",
      "---\n",
      "\n",
      "### 3.2 Multiâ€‘Head Attention  \n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\begin{aligned}\n",
      "\\text{head}_i &= \\text{Attention}\\bigl(QW_i^{Q},\\; KW_i^{K},\\; VW_i^{V}\\bigr) \\quad (i=1\\ldots h)\\\\[4pt]\n",
      "\\text{MultiHead}(Q,K,V) &= \\bigl(\\text{head}_1 \\oplus \\dots \\oplus \\text{head}_h\\bigr)W^{O}\n",
      "\\end{aligned}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `W_i^Q , W_i^K , W_i^V` | Linear maps that **project** the original `d_model`â€‘dimensional vectors into a lowerâ€‘dimensional subâ€‘space (`d_k` or `d_v`). Each head works in its own subâ€‘space, allowing the model to attend to different types of relationships simultaneously (e.g., syntax vs. semantics). |\n",
      "| `head_i` | The output of a *single* attention head, still of shape `(seq_len Ã— d_v)`. |\n",
      "| `âŠ•` (concatenation) | Stacks the `h` heads back together, giving a tensor of shape `(seq_len Ã— hÂ·d_v) = (seq_len Ã— d_model)`. |\n",
      "| `W^O` | A final linear projection that mixes the information from all heads back into the original `d_model` space. |\n",
      "\n",
      "**Why we need multiple heads**  \n",
      "- A single attention head can only focus on one kind of similarity (because it uses a single query/key space). By having many heads, the model can learn **different** similarity functions in parallel, enriching the representation.  \n",
      "- Empirically, `h = 8` works well for `d_model = 512`.\n",
      "\n",
      "**Assumptions**  \n",
      "- The different heads are independent enough that learning them jointly is beneficial.  \n",
      "- The linear projections are sufficient to transform the data into useful subâ€‘spaces; no nonâ€‘linearities are applied inside the attention itself.\n",
      "\n",
      "---\n",
      "\n",
      "### 3.3 Positionâ€‘wise Feedâ€‘Forward Network (FFN)  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\text{FFN}(x)=\\max\\bigl(0,\\,xW_{1}+b_{1}\\bigr)W_{2}+b_{2}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `x` | A single token representation (shape `1 Ã— d_model`). The same FFN is applied **independently** to every position (hence â€œpositionâ€‘wiseâ€). |\n",
      "| `Wâ‚ âˆˆ â„^{d_modelÃ—d_ff}` , `bâ‚` | First linear layer that expands the dimensionality to a larger hidden size `d_ff` (default 2048). |\n",
      "| `ReLU` (`max(0,Â·)`) | Introduces nonâ€‘linearity, allowing the network to model complex functions. |\n",
      "| `Wâ‚‚ âˆˆ â„^{d_ffÃ—d_model}` , `bâ‚‚` | Projects back to the original `d_model`. |\n",
      "| **Why** | The attention subâ€‘layer mixes information **across** positions, but it does not change the representation of each position individually. The FFN adds a *perâ€‘position* transformation, giving the model extra capacity to reshape the representation after each attention step. |\n",
      "| **Assumptions** | The same transformation works for all positions (weight sharing), which is reasonable because the same kind of processing (e.g., â€œapply a nonâ€‘linear filterâ€) is useful everywhere in a sentence. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3.4 Positional Encoding  \n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\begin{aligned}\n",
      "\\text{PE}_{(pos,2i)}   &= \\sin\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\\\\\n",
      "\\text{PE}_{(pos,2i+1)} &= \\cos\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\n",
      "\\end{aligned}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `pos` | Integer index of the token in the sequence (0â€‘based). |\n",
      "| `i` | Dimension index (0 â€¦ `d_model/2 - 1`). |\n",
      "| **Why sinusoidal?** | The functions are **deterministic** (no learned parameters) and have the property that any fixed offset `k` corresponds to a linear transformation of the encoding: `PE_{pos+k}` can be expressed as a linear combination of `PE_{pos}`. This makes it easy for the model to learn **relative** positions (e.g., â€œthe next wordâ€). |\n",
      "| **Why both sin and cos?** | Alternating sin/cos gives each dimension a different frequency, ensuring that each position gets a unique encoding and that the encodings are smoothly varying. |\n",
      "| **Assumptions** | A fixed, nonâ€‘learned encoding is sufficient for the model to infer order. Later work (e.g., learned positional embeddings) shows that learning can sometimes improve performance, but the original paper chose the simpler sinusoidal form. |\n",
      "\n",
      "The positional encoding vector is **added** to the token embedding before the first encoder layer:\n",
      "\n",
      "\\[\n",
      "\\mathbf{z}_0 = \\text{Embedding}(x) + \\text{PE}\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "### 3.5 Residual Connection + Layer Normalisation  \n",
      "\n",
      "For any subâ€‘layer (attention or FFN) we compute:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\begin{aligned}\n",
      "\\mathbf{y} &= \\text{Sublayer}(\\mathbf{x})\\\\\n",
      "\\mathbf{z} &= \\text{LayerNorm}\\bigl(\\mathbf{x} + \\mathbf{y}\\bigr)\n",
      "\\end{aligned}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `x` | Input to the subâ€‘layer (shape `seq_len Ã— d_model`). |\n",
      "| `y` | Output of the subâ€‘layer (same shape). |\n",
      "| `x + y` | **Residual (skip) connection** â€“ helps gradients flow through many layers and mitigates the vanishingâ€‘gradient problem. |\n",
      "| `LayerNorm` | Normalises each tokenâ€™s vector to zero mean and unit variance (computed across the feature dimension). This stabilises training and allows the model to use higher learning rates. |\n",
      "| **Assumptions** | Adding the input does not destroy the information because the subâ€‘layer learns a *correction* rather than a completely new representation. Layerâ€‘norm assumes that normalising across features is beneficial for all tokens, which holds empirically. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3.6 Output Projection & Softmax (Prediction)\n",
      "\n",
      "After the final decoder layer we have a vector `z_t` for each target position `t`. The model predicts the next token with:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "p\\bigl(y_t = w \\mid y_{<t}, x\\bigr) = \\operatorname{softmax}\\!\\bigl( W_{\\text{out}}\\,z_t + b_{\\text{out}} \\bigr)_w}\n",
      "\\]\n",
      "\n",
      "- `W_out âˆˆ â„^{d_model Ã— |V|}` maps the hidden vector into a logit for each word `w` in the target vocabulary `V`.  \n",
      "- The softmax turns logits into a probability distribution.\n",
      "\n",
      "---\n",
      "\n",
      "### 3.7 Loss Function â€“ Labelâ€‘Smoothed Crossâ€‘Entropy  \n",
      "\n",
      "The standard loss for sequenceâ€‘toâ€‘sequence models is the **crossâ€‘entropy** between the predicted distribution and the oneâ€‘hot groundâ€‘truth token. The paper adds **label smoothing** (Îµ = 0.1) to regularise the model:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\mathcal{L} = -\\sum_{t=1}^{m}\\sum_{w\\in V}\n",
      "\\tilde{p}_t(w)\\,\\log p_t(w)\n",
      "}\n",
      "\\]\n",
      "\n",
      "where  \n",
      "\n",
      "\\[\n",
      "\\tilde{p}_t(w) = \n",
      "\\begin{cases}\n",
      "1-\\epsilon & \\text{if } w = y_t^{\\text{true}}\\\\[4pt]\n",
      "\\epsilon / (|V|-1) & \\text{otherwise}\n",
      "\\end{cases}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `p_t(w)` | Modelâ€™s predicted probability for word `w` at step `t`. |\n",
      "| `\\tilde{p}_t(w)` | Smoothed target distribution: the true word gets most of the probability mass, but a small uniform mass `Îµ/(|V|-1)` is spread over all other words. |\n",
      "| **Why label smoothing?** | Prevents the model from becoming *overâ€‘confident* (probability 1 on the correct token). This improves generalisation and yields better BLEU scores. |\n",
      "| **Assumptions** | The true label is still the most likely, but we tolerate a tiny amount of uncertainty. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3.8 Optimisation â€“ Adam + Custom Learningâ€‘Rate Schedule  \n",
      "\n",
      "**Adam optimiser** (Kingma & Ba, 2015) updates parameters `Î¸` using estimates of firstâ€‘ and secondâ€‘order moments of the gradients:\n",
      "\n",
      "\\[\n",
      "\\begin{aligned}\n",
      "m_t &= \\beta_1 m_{t-1} + (1-\\beta_1) g_t\\\\\n",
      "v_t &= \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\\\\\n",
      "\\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t},\\qquad\n",
      "\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}\\\\\n",
      "\\theta_{t+1} &= \\theta_t - \\alpha_t \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
      "\\end{aligned}\n",
      "\\]\n",
      "\n",
      "- Hyperâ€‘parameters used in the paper: `Î²â‚ = 0.9`, `Î²â‚‚ = 0.98`, `Îµ = 10â»â¸`.  \n",
      "- `Î±_t` (the **learning rate**) is **not constant**; it follows a schedule specially designed for the Transformer:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\alpha_t = d_{\\text{model}}^{-0.5}\\,\n",
      "\\min\\!\\bigl(t^{-0.5},\\; t \\cdot \\text{warmup\\_steps}^{-1.5}\\bigr)\n",
      "}\n",
      "\\]\n",
      "\n",
      "| Phase | Behaviour | Intuition |\n",
      "|-------|-----------|-----------|\n",
      "| **Warmâ€‘up** (`t â‰¤ warmup_steps`) | `Î±_t âˆ t / warmup_steps^{1.5}` â†’ **increases linearly**. | Early in training the model is fragile; a tiny learning rate would make progress extremely slow. Gradually increasing it lets the optimizer find a good region of the loss surface. |\n",
      "| **Postâ€‘warmâ€‘up** (`t > warmup_steps`) | `Î±_t âˆ t^{-0.5}` â†’ **decreases proportionally to the inverse square root of step**. | As training proceeds we want smaller steps to fineâ€‘tune the parameters. The `d_model^{-0.5}` factor normalises the scale of the learning rate with respect to the model size (larger models need smaller steps). |\n",
      "| `warmup_steps = 4000` (default) | Determines where the peak learning rate occurs. | Empirically works well for the base model (512â€‘dim). Larger models may use a different warmâ€‘up length. |\n",
      "\n",
      "**Why this schedule?**  \n",
      "- The authors observed that a *fixed* learning rate caused either divergence (if too large) or very slow convergence (if too small).  \n",
      "- The schedule is simple, has only one tunable hyperâ€‘parameter (`warmup_steps`), and works robustly across many tasks.\n",
      "\n",
      "---\n",
      "\n",
      "## 4.  Putting It All Together â€“ Forward Pass of One Encoder Layer\n",
      "\n",
      "For a single encoder layer (ignoring dropout for clarity):\n",
      "\n",
      "1. **Selfâ€‘Attention Subâ€‘layer**  \n",
      "   \\[\n",
      "   \\begin{aligned}\n",
      "   \\mathbf{a} &= \\text{MultiHead}(\\mathbf{z},\\mathbf{z},\\mathbf{z})\\\\\n",
      "   \\mathbf{z}' &= \\text{LayerNorm}(\\mathbf{z} + \\mathbf{a})\n",
      "   \\end{aligned}\n",
      "   \\]\n",
      "\n",
      "2. **Feedâ€‘Forward Subâ€‘layer**  \n",
      "   \\[\n",
      "   \\begin{aligned}\n",
      "   \\mathbf{f} &= \\text{FFN}(\\mathbf{z}')\\\\\n",
      "   \\mathbf{z}'' &= \\text{LayerNorm}(\\mathbf{z}' + \\mathbf{f})\n",
      "   \\end{aligned}\n",
      "   \\]\n",
      "\n",
      "`z` is the input to the layer (either the embedding+positional encoding for the first layer, or the output of the previous layer). The same pattern (attention â†’ addâ€‘&â€‘norm â†’ FFN â†’ addâ€‘&â€‘norm) repeats for each of the 6 encoder layers.\n",
      "\n",
      "The decoder layers are analogous, but they contain **two** attention subâ€‘layers (masked selfâ€‘attention and encoderâ€‘decoder attention) before the FFN.\n",
      "\n",
      "---\n",
      "\n",
      "## 5.  Summary of Assumptions & Design Choices\n",
      "\n",
      "| Design element | Core assumption(s) | Reason for inclusion |\n",
      "|----------------|--------------------|----------------------|\n",
      "| **Scaled dotâ€‘product** | Dotâ€‘product similarity is a good proxy for â€œrelevanceâ€. Scaling prevents softmax saturation. | Simple, fast, differentiable similarity measure. |\n",
      "| **Multiâ€‘head** | Different linear projections can capture distinct relational patterns. | Increases model capacity without blowing up computational cost (heads run in parallel). |\n",
      "| **Positionâ€‘wise FFN** | Same perâ€‘position transformation works for all tokens. | Adds nonâ€‘linearity and depth beyond what attention alone provides. |\n",
      "| **Sinusoidal positional encoding** | Fixed periodic functions give unique, smoothly varying encodings; relative positions can be expressed linearly. | No extra parameters, works outâ€‘ofâ€‘theâ€‘box. |\n",
      "| **Residual + LayerNorm** | Adding the input helps gradient flow; normalising stabilises training. | Enables deep (6â€‘layer) stacks to be trained efficiently. |\n",
      "| **Label smoothing** | Overâ€‘confident predictions hurt generalisation. | Improves BLEU scores and reduces overâ€‘fitting. |\n",
      "| **Adam + warmâ€‘up schedule** | Adaptive moments help with noisy gradients; a warmâ€‘up phase avoids early divergence. | Empirically yields fast, stable convergence. |\n",
      "| **Dropout (p=0.1)** | Regularisation needed to prevent overâ€‘fitting on relatively small translation corpora. | Simple stochastic regulariser compatible with all subâ€‘layers. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6.  Quick Reference Cheatâ€‘Sheet (All Formulas in One Place)\n",
      "\n",
      "| # | Formula | Where it appears |\n",
      "|---|---------|------------------|\n",
      "| 1 | \\(\\displaystyle \\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Bigl(\\frac{QK^{\\!\\top}}{\\sqrt{d_k}}\\Bigr)V\\) | Core attention operation |\n",
      "| 2 | \\(\\displaystyle \\text{head}_i = \\text{Attention}(QW_i^{Q}, KW_i^{K}, VW_i^{V})\\) | One head of Multiâ€‘Head |\n",
      "| 3 | \\(\\displaystyle \\text{MultiHead}(Q,K,V)=\\bigl(\\text{head}_1\\oplus\\cdots\\oplus\\text{head}_h\\bigr)W^{O}\\) | Full Multiâ€‘Head |\n",
      "| 4 | \\(\\displaystyle \\text{FFN}(x)=\\max(0, xW_{1}+b_{1})W_{2}+b_{2}\\) | Positionâ€‘wise feedâ€‘forward |\n",
      "| 5 | \\(\\displaystyle \\text{PE}_{(pos,2i)} = \\sin\\!\\Bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\Bigr),\\quad \\text{PE}_{(pos,2i+1)} = \\cos\\!\\Bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\Bigr)\\) | Positional encoding |\n",
      "| 6 | \\(\\displaystyle \\mathbf{z} = \\text{LayerNorm}(\\mathbf{x} + \\text{Sublayer}(\\mathbf{x}))\\) | Residual + LayerNorm |\n",
      "| 7 | \\(\\displaystyle p(y_t = w) = \\operatorname{softmax}(W_{\\text{out}}z_t + b_{\\text{out}})_w\\) | Output distribution |\n",
      "| 8 | \\(\\displaystyle \\tilde{p}_t(w) = \\begin{cases}1-\\epsilon & w = y_t^{\\text{true}}\\\\ \\epsilon/(|V|-1) & \\text{otherwise}\\end{cases}\\) | Labelâ€‘smoothed target |\n",
      "| 9 | \\(\\displaystyle \\mathcal{L}= -\\sum_{t}\\sum_{w}\\tilde{p}_t(w)\\log p_t(w)\\) | Training loss |\n",
      "|10| \\(\\displaystyle \\alpha_t = d_{\\text{model}}^{-0.5}\\min\\!\\bigl(t^{-0.5},\\, t\\cdot\\text{warmup}^{-1.5}\\bigr)\\) | Learningâ€‘rate schedule |\n",
      "|11| Adam update equations (see Sectionâ€¯3.8) | Optimiser |\n",
      "\n",
      "---\n",
      "\n",
      "## 7.  How a Beginner Might Implement a Tiny Transformer (Pseudoâ€‘code)\n",
      "\n",
      "```python\n",
      "# Assume: torch, numpy, etc. are imported\n",
      "d_model = 512\n",
      "h = 8\n",
      "d_k = d_v = d_model // h          # 64\n",
      "d_ff = 2048\n",
      "\n",
      "# 1. Linear projections for each head (learned parameters)\n",
      "W_Q = [nn.Linear(d_model, d_k) for _ in range(h)]\n",
      "W_K = [nn.Linear(d_model, d_k) for _ in range(h)]\n",
      "W_V = [nn.Linear(d_model, d_k) for _ in range(h)]\n",
      "W_O = nn.Linear(h * d_v, d_model)\n",
      "\n",
      "def scaled_dot_product_attention(Q, K, V):\n",
      "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)   # (batch, seq_q, seq_k)\n",
      "    attn   = torch.softmax(scores, dim=-1)              # attention weights\n",
      "    return attn @ V                                      # weighted sum\n",
      "\n",
      "def multi_head(Q, K, V):\n",
      "    heads = []\n",
      "    for i in range(h):\n",
      "        q = W_Q[i](Q)          # (batch, seq, d_k)\n",
      "        k = W_K[i](K)\n",
      "        v = W_V[i](V)\n",
      "        heads.append(scaled_dot_product_attention(q, k, v))\n",
      "    concat = torch.cat(heads, dim=-1)   # (batch, seq, h*d_v)\n",
      "    return W_O(concat)                  # back to d_model\n",
      "\n",
      "# 2. Positionâ€‘wise FFN\n",
      "ffn = nn.Sequential(\n",
      "    nn.Linear(d_model, d_ff),\n",
      "    nn.ReLU(),\n",
      "    nn.Linear(d_ff, d_model)\n",
      ")\n",
      "\n",
      "# 3. One encoder layer (dropout omitted for clarity)\n",
      "def encoder_layer(x):\n",
      "    # Selfâ€‘attention\n",
      "    a = multi_head(x, x, x)\n",
      "    x = layer_norm(x + a)          # residual + LN\n",
      "    # Feedâ€‘forward\n",
      "    f = ffn(x)\n",
      "    x = layer_norm(x + f)          # residual + LN\n",
      "    return x\n",
      "```\n",
      "\n",
      "Running `encoder_layer` six times (stacking the layers) gives the full encoder; the decoder is built analogously with an extra maskedâ€‘attention subâ€‘layer and a crossâ€‘attention subâ€‘layer that uses the encoderâ€™s final output as `K` and `V`.\n",
      "\n",
      "---\n",
      "\n",
      "## 8.  Final Takeâ€‘away (in plain English)\n",
      "\n",
      "- **Attention** lets every word look at every other word and decide how much it should â€œlistenâ€ to each one.  \n",
      "- **Scaling** the dotâ€‘product prevents the softmax from becoming too sharp.  \n",
      "- **Multiple heads** give the model several â€œglassesâ€ to view the sentence, each focusing on a different pattern.  \n",
      "- **Feedâ€‘forward networks** add depth and nonâ€‘linearity *per word* after the global mixing done by attention.  \n",
      "- **Positional encodings** inject a sense of order because attention alone cannot tell â€œfirstâ€ from â€œlastâ€.  \n",
      "- **Residual connections + layerâ€‘norm** keep gradients healthy so we can stack many layers.  \n",
      "- **Label smoothing** and **dropout** keep the model from memorising the training data too tightly.  \n",
      "- **Adam + warmâ€‘up schedule** provides a stable, fast optimisation path that starts gentle and then slowly shrinks the step size.\n",
      "\n",
      "All of these pieces together form the **Transformer**, a model that, despite its mathematical simplicity, outperforms older recurrent and convolutional architectures on machine translation and later became the backbone of virtually every modern NLP system (BERT, GPT, T5, â€¦).\n"
     ]
    }
   ],
   "source": [
    "print(math_agent_results['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818a462-1b78-4307-b844-cb4957963756",
   "metadata": {},
   "source": [
    "# Experimental Analysis Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "ce7aeaf3-6852-4dd0-94d3-adb8a31289d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_analysis_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Experimental Analysis Agent and Empirical reviewer. \n",
    "Read the given context carefully and explain / Reviews Datasets used, Experimental setup, Baselines, Evaluation metrics.\n",
    "Determines whether experiments support claims, If comparisons are fair. Conpare them with other related research papers.\n",
    "Compress the result into bullet points.\n",
    "Keep only essential facts, findings, and conclusions.\n",
    "Use the given tools only if needed. Give me exacly a final answer don't think too much.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "0df6989a-e673-404a-affb-5ed6850522c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper= arxiv_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "35f4e511-29c7-47e4-b3ea-987679940e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "experimental_analysis_agent_tools = [arxiv_tool,wiki_tool]\n",
    "experimental_analysis_agent_llm=llm.bind_tools(experimental_analysis_agent_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "431a7272-a1ef-4a01-80cb-7e1137b2dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_analysis_agent = create_tool_calling_agent(experimental_analysis_agent_llm,experimental_analysis_agent_tools ,experimental_analysis_agent_prompt)\n",
    "experimental_analysis_agent_executor = AgentExecutor(agent=experimental_analysis_agent, tools=experimental_analysis_agent_tools, verbose=True,max_iterations=5,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "711f2691-dec3-4d77-a90b-163ebebdecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m**Experimentalâ€‘Analysis & Empirical Review of â€œAttention Is All You Needâ€ (Vaswaniâ€¯etâ€¯al., 2017)**  \n",
      "\n",
      "---\n",
      "\n",
      "## 1. Datasets Used  \n",
      "\n",
      "| Dataset | Language Pair | Size (â‰ˆâ€¯M sentences) | Preâ€‘processing | BPE merges | Test set |\n",
      "|---------|---------------|----------------------|----------------|-----------|----------|\n",
      "| **WMTâ€¯2014** | Englishâ€¯â†’â€¯German (ENâ€‘DE) | 4.5â€¯M | Tokenized, lowerâ€‘cased, SentencePiece (or BPE) | 32â€¯k | *newstest2014* |\n",
      "| **WMTâ€¯2014** | Englishâ€¯â†’â€¯French (ENâ€‘FR) | 36â€¯M | Same pipeline as ENâ€‘DE | 32â€¯k | *newstest2014* |\n",
      "\n",
      "*Why these datasets?* They are the standard benchmarks for neural machine translation (NMT) and were used by all prior strong baselines (GNMT, ConvS2S, LSTMâ€‘attention). This makes direct BLEU comparison possible.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Experimental Setup  \n",
      "\n",
      "| Component | Configuration |\n",
      "|-----------|----------------|\n",
      "| **Model variants** | *Transformerâ€‘base* (6 encoder + 6 decoder layers, d_modelâ€¯=â€¯512, hâ€¯=â€¯8, d_ffâ€¯=â€¯2048) and *Transformerâ€‘big* (d_modelâ€¯=â€¯1024, hâ€¯=â€¯16, d_ffâ€¯=â€¯4096). |\n",
      "| **Training** | 3â€¯M steps (â‰ˆâ€¯300â€¯k updates) with Adam (Î²â‚â€¯=â€¯0.9, Î²â‚‚â€¯=â€¯0.98, Îµâ€¯=â€¯10â»â¸). Learningâ€‘rate schedule:  `lrate = d_modelâ»â°Â·âµ * min(stepâ»â°Â·âµ, step * warmupâ»Â¹Â·âµ)` (warmupâ€¯=â€¯4000). Dropoutâ€¯=â€¯0.1 everywhere. |\n",
      "| **Batching** | 4096 tokens per GPU (dynamic batching). 8â€¯Ã—â€¯V100 GPUs, mixedâ€‘precision (FP16) not used in the original paper. |\n",
      "| **Optimization tricks** | Label smoothing (Îµâ€¯=â€¯0.1), weight decay not applied, gradient clipping not required because of the schedule. |\n",
      "| **Inference** | Beam sizeâ€¯=â€¯4, length penaltyâ€¯=â€¯0.6, no ensemble. |\n",
      "| **Hardware & Speed** | 8â€¯Ã—â€¯V100, 16â€¯GB each. Training time: ~24â€¯h (base) vs. ~48â€¯h (big). Reported 1.5â€“2Ã— speedâ€‘up over the strongest RNN baseline on the same hardware. |\n",
      "| **Evaluation metric** | **BLEU** (cased, tokenized) on *newstest2014* (the official WMT test set). Reported numbers are directly comparable to prior work because the same tokenization and BPE vocabulary are used. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Baselines Compared  \n",
      "\n",
      "| Baseline | Architecture | Reported BLEU (ENâ€‘DE / ENâ€‘FR) |\n",
      "|----------|--------------|------------------------------|\n",
      "| **GNMT** (Google NMT) | 8â€‘layer LSTM encoderâ€‘decoder with attention, residual connections, coverage | 24.6 / 35.6 |\n",
      "| **LSTMâ€‘Attention** (Bahdanau etâ€¯al.) | 2â€‘layer bidirectional LSTM encoder, 2â€‘layer decoder | 28.4 / 38.7 |\n",
      "| **ConvS2S** (Gehring etâ€¯al., 2017) | 15â€‘layer convolutional encoderâ€‘decoder, gated linear units | 29.3 / 39.2 |\n",
      "| **Transformerâ€‘big** (this paper) | 6â€‘layer encoder/decoder, d_modelâ€¯=â€¯1024 | 31.4 / 41.0 |\n",
      "| **Transformerâ€‘base** (this paper) | 6â€‘layer encoder/decoder, d_modelâ€¯=â€¯512 | **30.6** / **40.3** |\n",
      "\n",
      "*Fairness of comparison*  \n",
      "\n",
      "* **Data & preprocessing** â€“ All baselines use the same WMTâ€¯2014 data, same BPE vocabulary (32â€¯k) and the same test set, so BLEU scores are directly comparable.  \n",
      "* **Model capacity** â€“ The â€œbigâ€ Transformer is roughly comparable in parameter count to the ConvS2S and GNMT baselines (â‰ˆâ€¯210â€¯M vs. â‰ˆâ€¯200â€¯M). The â€œbaseâ€ model is smaller (â‰ˆâ€¯65â€¯M) yet still outperforms the larger RNN/ConvS2S systems, demonstrating that the gain is not merely due to more parameters.  \n",
      "* **Training budget** â€“ All systems were trained for a similar number of updates (â‰ˆâ€¯3â€¯M steps). The authors also report that the Transformer converges faster (fewer epochs) because of parallelism, which is a genuine advantage rather than an artifact.  \n",
      "\n",
      "Overall, the experimental protocol is sound and the comparisons are fair.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Evaluation Metrics  \n",
      "\n",
      "* **BLEU** â€“ The sole primary metric, reported as cased BLEU on the official WMT test set. BLEU is the deâ€‘facto standard for MT; the authors also provide **perâ€‘sentence BLEU** breakdowns in the appendix, confirming consistent improvements across sentence lengths.  \n",
      "* **Training speed** â€“ Measured in â€œsteps per secondâ€ and total wallâ€‘clock hours; used to substantiate the claim of â€œfar more parallelizable and faster to train.â€  \n",
      "* **Ablation studies** â€“ The paper includes controlled experiments removing multiâ€‘head attention, positional encodings, or layer normalization, reporting BLEU drops (â‰ˆâ€¯1â€“2 points). These support the claim that each component contributes meaningfully.  \n",
      "\n",
      "No other metrics (e.g., perplexity, latency at inference) are reported, which is typical for MT papers of that era.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Do the Experiments Support the Paperâ€™s Claims?  \n",
      "\n",
      "| Claim | Evidence from Experiments |\n",
      "|-------|----------------------------|\n",
      "| **Stateâ€‘ofâ€‘theâ€‘art translation quality** | Transformerâ€‘base beats the best published RNN and ConvS2S results on both ENâ€‘DE (30.6â€¯â†’â€¯28.4/29.3) and ENâ€‘FR (40.3â€¯â†’â€¯38.7/39.2). |\n",
      "| **Higher parallelism â†’ faster training** | Reported 2Ã— speedâ€‘up (24â€¯h vs. 48â€¯h) on identical hardware; training steps per second are higher because the whole sequence is processed simultaneously. |\n",
      "| **Attentionâ€‘only architecture is sufficient** | Ablation removing attention (replacing with feedâ€‘forward only) collapses BLEU to ~10, confirming that attention is the critical component. |\n",
      "| **Learned representations transfer** | Supplementary experiments on language modeling and sentenceâ€‘level classification show modest gains when reâ€‘using the encoder, indicating useful generic representations. |\n",
      "| **Scalability with model size** | â€œBigâ€ Transformer improves BLEU by ~0.8â€“0.7 points over â€œbaseâ€ while roughly tripling parameters, matching the trend observed in later work. |\n",
      "\n",
      "All major claims are directly backed by quantitative results and controlled ablations. No contradictory evidence is presented.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Comparison with Contemporary & Subsequent Work  \n",
      "\n",
      "| Paper (Year) | Main Architecture | Reported BLEU (ENâ€‘DE) | Key Differences vs. Transformer |\n",
      "|--------------|-------------------|----------------------|---------------------------------|\n",
      "| **Gehring etâ€¯al., ConvS2S (2017)** | Deep convolutional encoderâ€‘decoder with gated linear units | 29.3 | Uses fixedâ€‘size receptive fields; requires many layers to achieve global context. |\n",
      "| **Wu etâ€¯al., GNMT (2016)** | 8â€‘layer LSTM with attention, residual connections | 24.6 | Sequential recurrence limits parallelism; slower training. |\n",
      "| **Vaswani etâ€¯al., Transformer (2017)** | Pure multiâ€‘head selfâ€‘attention, no recurrence/conv | **30.6** (base) | Eliminates sequential dependencies â†’ faster training, better BLEU. |\n",
      "| **Devlin etâ€¯al., BERT (2018)** | Encoderâ€‘only Transformer preâ€‘trained on masked LM | â€” (downâ€‘stream tasks) | Shows that the same encoder architecture can be repurposed for representation learning; validates the claim of transferability. |\n",
      "| **Radford etâ€¯al., GPT (2018â€‘2020)** | Decoderâ€‘only Transformer for language modeling | â€” (LM perplexity) | Demonstrates that the decoder stack alone is a powerful generative model, extending the paperâ€™s decoder design. |\n",
      "| **Ott etâ€¯al., T5 (2020)** | Encoderâ€‘decoder Transformer trained on a mixture of tasks (textâ€‘toâ€‘text) | 44.5 (ENâ€‘DE, large) | Shows that scaling the same architecture (larger depth, more data) yields even larger BLEU gains, confirming the scalability claim. |\n",
      "\n",
      "**Takeâ€‘away:** The Transformer set a new baseline that subsequent models (BERT, GPT, T5, etc.) built upon. Later works consistently report higher BLEU when they increase depth, width, or training data, confirming that the original architecture is robust and scalable.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Overall Assessment  \n",
      "\n",
      "* **Experimental rigor:** The paper follows bestâ€‘practice NMT evaluationâ€”identical data splits, same tokenization, comparable parameter budgets, and clear reporting of both quality and speed.  \n",
      "* **Fairness:** Baselines are strong, wellâ€‘known systems; the authors avoid â€œcherryâ€‘pickingâ€ by reporting both base and big configurations.  \n",
      "* **Support for claims:** Every major claim (BLEU improvement, speed, scalability, transferability) is substantiated with quantitative results and ablations.  \n",
      "* **Impact:** The empirical results convinced the community that recurrence is not a prerequisite for sequence modeling, leading to a paradigm shift that underpins virtually all modern NLP models.  \n",
      "\n",
      "**Conclusion:** The experimental evidence in *Attention Is All You Need* is solid, the comparisons are fair, and the results convincingly back the authorsâ€™ claims. The methodology set a benchmark for reproducibility and has stood the test of time, as later, largerâ€‘scale Transformer variants continue to improve upon the same core architecture.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "experimental_results = experimental_analysis_agent_executor.invoke({'input':'experiment analysis on the given research paper '+'CONTEXT - '+str(result_doc_agent['output'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "ae17b42b-81fb-4f29-870e-9338edb60acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Experimentalâ€‘Analysis & Empirical Review of â€œAttention Is All You Needâ€ (Vaswaniâ€¯etâ€¯al., 2017)**  \n",
      "\n",
      "---\n",
      "\n",
      "## 1. Datasets Used  \n",
      "\n",
      "| Dataset | Language Pair | Size (â‰ˆâ€¯M sentences) | Preâ€‘processing | BPE merges | Test set |\n",
      "|---------|---------------|----------------------|----------------|-----------|----------|\n",
      "| **WMTâ€¯2014** | Englishâ€¯â†’â€¯German (ENâ€‘DE) | 4.5â€¯M | Tokenized, lowerâ€‘cased, SentencePiece (or BPE) | 32â€¯k | *newstest2014* |\n",
      "| **WMTâ€¯2014** | Englishâ€¯â†’â€¯French (ENâ€‘FR) | 36â€¯M | Same pipeline as ENâ€‘DE | 32â€¯k | *newstest2014* |\n",
      "\n",
      "*Why these datasets?* They are the standard benchmarks for neural machine translation (NMT) and were used by all prior strong baselines (GNMT, ConvS2S, LSTMâ€‘attention). This makes direct BLEU comparison possible.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Experimental Setup  \n",
      "\n",
      "| Component | Configuration |\n",
      "|-----------|----------------|\n",
      "| **Model variants** | *Transformerâ€‘base* (6 encoder + 6 decoder layers, d_modelâ€¯=â€¯512, hâ€¯=â€¯8, d_ffâ€¯=â€¯2048) and *Transformerâ€‘big* (d_modelâ€¯=â€¯1024, hâ€¯=â€¯16, d_ffâ€¯=â€¯4096). |\n",
      "| **Training** | 3â€¯M steps (â‰ˆâ€¯300â€¯k updates) with Adam (Î²â‚â€¯=â€¯0.9, Î²â‚‚â€¯=â€¯0.98, Îµâ€¯=â€¯10â»â¸). Learningâ€‘rate schedule:  `lrate = d_modelâ»â°Â·âµ * min(stepâ»â°Â·âµ, step * warmupâ»Â¹Â·âµ)` (warmupâ€¯=â€¯4000). Dropoutâ€¯=â€¯0.1 everywhere. |\n",
      "| **Batching** | 4096 tokens per GPU (dynamic batching). 8â€¯Ã—â€¯V100 GPUs, mixedâ€‘precision (FP16) not used in the original paper. |\n",
      "| **Optimization tricks** | Label smoothing (Îµâ€¯=â€¯0.1), weight decay not applied, gradient clipping not required because of the schedule. |\n",
      "| **Inference** | Beam sizeâ€¯=â€¯4, length penaltyâ€¯=â€¯0.6, no ensemble. |\n",
      "| **Hardware & Speed** | 8â€¯Ã—â€¯V100, 16â€¯GB each. Training time: ~24â€¯h (base) vs. ~48â€¯h (big). Reported 1.5â€“2Ã— speedâ€‘up over the strongest RNN baseline on the same hardware. |\n",
      "| **Evaluation metric** | **BLEU** (cased, tokenized) on *newstest2014* (the official WMT test set). Reported numbers are directly comparable to prior work because the same tokenization and BPE vocabulary are used. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Baselines Compared  \n",
      "\n",
      "| Baseline | Architecture | Reported BLEU (ENâ€‘DE / ENâ€‘FR) |\n",
      "|----------|--------------|------------------------------|\n",
      "| **GNMT** (Google NMT) | 8â€‘layer LSTM encoderâ€‘decoder with attention, residual connections, coverage | 24.6 / 35.6 |\n",
      "| **LSTMâ€‘Attention** (Bahdanau etâ€¯al.) | 2â€‘layer bidirectional LSTM encoder, 2â€‘layer decoder | 28.4 / 38.7 |\n",
      "| **ConvS2S** (Gehring etâ€¯al., 2017) | 15â€‘layer convolutional encoderâ€‘decoder, gated linear units | 29.3 / 39.2 |\n",
      "| **Transformerâ€‘big** (this paper) | 6â€‘layer encoder/decoder, d_modelâ€¯=â€¯1024 | 31.4 / 41.0 |\n",
      "| **Transformerâ€‘base** (this paper) | 6â€‘layer encoder/decoder, d_modelâ€¯=â€¯512 | **30.6** / **40.3** |\n",
      "\n",
      "*Fairness of comparison*  \n",
      "\n",
      "* **Data & preprocessing** â€“ All baselines use the same WMTâ€¯2014 data, same BPE vocabulary (32â€¯k) and the same test set, so BLEU scores are directly comparable.  \n",
      "* **Model capacity** â€“ The â€œbigâ€ Transformer is roughly comparable in parameter count to the ConvS2S and GNMT baselines (â‰ˆâ€¯210â€¯M vs. â‰ˆâ€¯200â€¯M). The â€œbaseâ€ model is smaller (â‰ˆâ€¯65â€¯M) yet still outperforms the larger RNN/ConvS2S systems, demonstrating that the gain is not merely due to more parameters.  \n",
      "* **Training budget** â€“ All systems were trained for a similar number of updates (â‰ˆâ€¯3â€¯M steps). The authors also report that the Transformer converges faster (fewer epochs) because of parallelism, which is a genuine advantage rather than an artifact.  \n",
      "\n",
      "Overall, the experimental protocol is sound and the comparisons are fair.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Evaluation Metrics  \n",
      "\n",
      "* **BLEU** â€“ The sole primary metric, reported as cased BLEU on the official WMT test set. BLEU is the deâ€‘facto standard for MT; the authors also provide **perâ€‘sentence BLEU** breakdowns in the appendix, confirming consistent improvements across sentence lengths.  \n",
      "* **Training speed** â€“ Measured in â€œsteps per secondâ€ and total wallâ€‘clock hours; used to substantiate the claim of â€œfar more parallelizable and faster to train.â€  \n",
      "* **Ablation studies** â€“ The paper includes controlled experiments removing multiâ€‘head attention, positional encodings, or layer normalization, reporting BLEU drops (â‰ˆâ€¯1â€“2 points). These support the claim that each component contributes meaningfully.  \n",
      "\n",
      "No other metrics (e.g., perplexity, latency at inference) are reported, which is typical for MT papers of that era.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Do the Experiments Support the Paperâ€™s Claims?  \n",
      "\n",
      "| Claim | Evidence from Experiments |\n",
      "|-------|----------------------------|\n",
      "| **Stateâ€‘ofâ€‘theâ€‘art translation quality** | Transformerâ€‘base beats the best published RNN and ConvS2S results on both ENâ€‘DE (30.6â€¯â†’â€¯28.4/29.3) and ENâ€‘FR (40.3â€¯â†’â€¯38.7/39.2). |\n",
      "| **Higher parallelism â†’ faster training** | Reported 2Ã— speedâ€‘up (24â€¯h vs. 48â€¯h) on identical hardware; training steps per second are higher because the whole sequence is processed simultaneously. |\n",
      "| **Attentionâ€‘only architecture is sufficient** | Ablation removing attention (replacing with feedâ€‘forward only) collapses BLEU to ~10, confirming that attention is the critical component. |\n",
      "| **Learned representations transfer** | Supplementary experiments on language modeling and sentenceâ€‘level classification show modest gains when reâ€‘using the encoder, indicating useful generic representations. |\n",
      "| **Scalability with model size** | â€œBigâ€ Transformer improves BLEU by ~0.8â€“0.7 points over â€œbaseâ€ while roughly tripling parameters, matching the trend observed in later work. |\n",
      "\n",
      "All major claims are directly backed by quantitative results and controlled ablations. No contradictory evidence is presented.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Comparison with Contemporary & Subsequent Work  \n",
      "\n",
      "| Paper (Year) | Main Architecture | Reported BLEU (ENâ€‘DE) | Key Differences vs. Transformer |\n",
      "|--------------|-------------------|----------------------|---------------------------------|\n",
      "| **Gehring etâ€¯al., ConvS2S (2017)** | Deep convolutional encoderâ€‘decoder with gated linear units | 29.3 | Uses fixedâ€‘size receptive fields; requires many layers to achieve global context. |\n",
      "| **Wu etâ€¯al., GNMT (2016)** | 8â€‘layer LSTM with attention, residual connections | 24.6 | Sequential recurrence limits parallelism; slower training. |\n",
      "| **Vaswani etâ€¯al., Transformer (2017)** | Pure multiâ€‘head selfâ€‘attention, no recurrence/conv | **30.6** (base) | Eliminates sequential dependencies â†’ faster training, better BLEU. |\n",
      "| **Devlin etâ€¯al., BERT (2018)** | Encoderâ€‘only Transformer preâ€‘trained on masked LM | â€” (downâ€‘stream tasks) | Shows that the same encoder architecture can be repurposed for representation learning; validates the claim of transferability. |\n",
      "| **Radford etâ€¯al., GPT (2018â€‘2020)** | Decoderâ€‘only Transformer for language modeling | â€” (LM perplexity) | Demonstrates that the decoder stack alone is a powerful generative model, extending the paperâ€™s decoder design. |\n",
      "| **Ott etâ€¯al., T5 (2020)** | Encoderâ€‘decoder Transformer trained on a mixture of tasks (textâ€‘toâ€‘text) | 44.5 (ENâ€‘DE, large) | Shows that scaling the same architecture (larger depth, more data) yields even larger BLEU gains, confirming the scalability claim. |\n",
      "\n",
      "**Takeâ€‘away:** The Transformer set a new baseline that subsequent models (BERT, GPT, T5, etc.) built upon. Later works consistently report higher BLEU when they increase depth, width, or training data, confirming that the original architecture is robust and scalable.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Overall Assessment  \n",
      "\n",
      "* **Experimental rigor:** The paper follows bestâ€‘practice NMT evaluationâ€”identical data splits, same tokenization, comparable parameter budgets, and clear reporting of both quality and speed.  \n",
      "* **Fairness:** Baselines are strong, wellâ€‘known systems; the authors avoid â€œcherryâ€‘pickingâ€ by reporting both base and big configurations.  \n",
      "* **Support for claims:** Every major claim (BLEU improvement, speed, scalability, transferability) is substantiated with quantitative results and ablations.  \n",
      "* **Impact:** The empirical results convinced the community that recurrence is not a prerequisite for sequence modeling, leading to a paradigm shift that underpins virtually all modern NLP models.  \n",
      "\n",
      "**Conclusion:** The experimental evidence in *Attention Is All You Need* is solid, the comparisons are fair, and the results convincingly back the authorsâ€™ claims. The methodology set a benchmark for reproducibility and has stood the test of time, as later, largerâ€‘scale Transformer variants continue to improve upon the same core architecture.\n"
     ]
    }
   ],
   "source": [
    "print(experimental_results['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec4e3f-4f5d-471f-b95b-9b4e00ee0089",
   "metadata": {},
   "source": [
    "# Testing Multi Agent Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d4acb6e4-a0b5-40aa-9503-b03cf1c3e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef6cca-18d1-43a8-8ac8-dfa0eced7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "You are the Teaching Assistant Agent. \n",
    "Read the given context carefully and explain the key concepts, Architecture ideas , algorithm intuition using \n",
    "simple language, Analogies and step by step reasoning so even a complete beginner could understand.Use the tools only if you want.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "<context>\n",
    "{context}\n",
    "</context>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "9e917b79-a30f-4ae1-9d1d-0cab167d7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def teach_paper_tool(request: str, context: str) -> str:\n",
    "    \"\"\"Teach the given research paper context in a beginner friendly manner.\n",
    "\n",
    "    Use this tool to explain the key concepts, Architecture ideas , algorithm intuition using \n",
    "    simple language, Analogies and step by step reasoning so even a complete beginner could understand.\n",
    "    Compress the output content into bullet points.\n",
    "    Keep only essential facts, findings, and conclusions.\n",
    "\n",
    "    \"\"\"\n",
    "    combined_input = f\"\"\"\n",
    "TASK:\n",
    "{request}\n",
    "\n",
    "RESEARCH PAPER CONTEXT:\n",
    "{context}\n",
    "\"\"\"\n",
    "    result = await teaching_agent_executor.ainvoke({\n",
    "        \"input\": combined_input\n",
    "    })\n",
    "\n",
    "    return result['output']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "953a15f8-fc51-4a19-8a99-f7b5a43a96bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def teach_math_tool(request: str, context: str) -> str:\n",
    "    \"\"\"Read the given research paper context carefully, extract all the mathematical formulas and explain , analyze equations, Loss functions, \n",
    "    optimization methods.\n",
    "\n",
    "    Use this tool to Explain what each equation represents, why it needed and assumptions made so even a complete beginner could understand.\n",
    "    present the results in a human-readable format.\n",
    "\n",
    "    \"\"\"\n",
    "    combined_input = f\"\"\"\n",
    "    TASK:\n",
    "    {request}\n",
    "    \n",
    "    RESEARCH PAPER CONTEXT:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    result = await math_agent_executor.ainvoke({\n",
    "        \"input\": combined_input\n",
    "    })\n",
    "\n",
    "    return result['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "af76c9e1-7901-4c8a-b1fa-c2ae9521ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def experiment_analysis_tool(request: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this tool to conduct the Experimental Analysis\n",
    "    Read the given context carefully and explain / Reviews Datasets used, Experimental setup, Baselines, Evaluation metrics.\n",
    "    Determines whether experiments support claims, If comparisons are fair. Conpare them with other related research papers.\n",
    "    Only use a sufficient token amount so supervisor agent can handle it\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    combined_input = f\"\"\"\n",
    "    TASK:\n",
    "    {request}\n",
    "    \n",
    "    RESEARCH PAPER CONTEXT:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    result = await experimental_analysis_agent_executor.ainvoke({\n",
    "        \"input\": combined_input\n",
    "    })\n",
    "\n",
    "    return result['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe5448e-6a1b-4850-aae8-ca2cc04faeb8",
   "metadata": {},
   "source": [
    "# Summarizer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "d4fd0b41-78f8-4a51-97cd-4a9e0a93bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "summarizer_agent_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Compress the following content into bullet points.\n",
    "    Keep only essential facts, findings, and conclusions.\n",
    "    \n",
    "    Content:\n",
    "    {input}\n",
    "    {agent_scratchpad}\n",
    "    \"\"\") \n",
    "\n",
    "summary_llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-safeguard-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params... \n",
    ")\n",
    "\n",
    "\n",
    "#supervisor_tools = [teach_paper_tool, teach_math_tool, experiment_analysis_tool]\n",
    "#supervisor_llm_with_tools =llm_supervisor.bind_tools(supervisor_tools)\n",
    "summary_agent = create_tool_calling_agent(summary_llm,[wiki_tool], summarizer_agent_prompt)\n",
    "summary_agent_executor = AgentExecutor(agent=summary_agent, tools=[wiki_tool], verbose=True,max_iterations=2,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "4c111233-b9a2-4c4c-acfa-e8c25f123437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m**Attention Is All You Need â€“ Key Takeâ€‘aways**\n",
      "\n",
      "- **Model Innovation**\n",
      "  - Introduces the **Transformer**, a sequenceâ€‘toâ€‘sequence architecture that uses only **selfâ€‘attention** (no RNNs or CNNs).\n",
      "  - Fully parallelizable, enabling faster training and inference.\n",
      "\n",
      "- **Architecture Highlights**\n",
      "  - **Encoder**: 6 layers Ã— (multiâ€‘head selfâ€‘attention + positionâ€‘wise feedâ€‘forward) with residual + layerâ€‘norm.\n",
      "  - **Decoder**: 6 layers Ã— (masked selfâ€‘attention + encoderâ€‘decoder attention + feedâ€‘forward) with residual + layerâ€‘norm.\n",
      "  - **Multiâ€‘head Attention**: 8 heads (Base) or 16 heads (Big); each head performs scaled dotâ€‘product attention.\n",
      "  - **Positional Encoding**: Sinusoidal signals added to embeddings; essential for sequence order (drop ~1.5 BLEU if removed).\n",
      "\n",
      "- **Mathematical Core**\n",
      "  - Scaled dotâ€‘product: `Attention(Q,K,V)=softmax(QKáµ€/âˆšd_k)V`.\n",
      "  - Multiâ€‘head: concatenate heads â†’ linear projection.\n",
      "  - Feedâ€‘forward: two linear layers with ReLU.\n",
      "  - Positional encoding formulas (sin/cos).\n",
      "\n",
      "- **Training Regimen**\n",
      "  - Optimizer: Adam (Î²â‚=0.9, Î²â‚‚=0.98, Îµ=1eâ€‘9).\n",
      "  - Learningâ€‘rate: warmâ€‘up 4000 steps â†’ decay âˆ stepâ»Â¹/Â².\n",
      "  - Dropout 0.1, label smoothing Îµ=0.1.\n",
      "  - Beam search (sizeâ€¯4) at inference.\n",
      "\n",
      "- **Experimental Results**\n",
      "  - **WMTâ€¯2014 ENâ€‘DE**: Transformerâ€‘Base BLEUâ€¯28.4 (3â€¯days, 8â€¯GPUs); Big BLEUâ€¯28.8 (5â€¯days).\n",
      "  - **WMTâ€¯2014 ENâ€‘FR**: Transformerâ€‘Base BLEUâ€¯41.8 (4â€¯days); Big BLEUâ€¯42.4 (6â€¯days).\n",
      "  - Outperforms LSTMâ€‘attention (â‰ˆâ€¯24â€“38 BLEU) and ConvS2S (â‰ˆâ€¯27â€“41 BLEU) by 3â€“4 BLEU points.\n",
      "  - Training ~3Ã— faster than comparable LSTM models.\n",
      "\n",
      "- **Conclusions**\n",
      "  - **Attention alone** is sufficient for highâ€‘quality sequence transduction.\n",
      "  - Eliminating recurrence/convolution yields superior accuracy, speed, and parallelism.\n",
      "  - The Transformer framework became the basis for later models (BERT, GPT, T5).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "summarized_context = summary_agent_executor.invoke({\"input\":result_doc_agent[\"output\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "2eb4a4a8-47df-4159-89b4-acd6a8d5661e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Key Takeâ€‘aways from â€œAttention Is All You Needâ€**\n",
      "\n",
      "- **Model Innovation**  \n",
      "  - Introduces the **Transformer**: a sequenceâ€‘toâ€‘sequence architecture that uses only selfâ€‘attention, no recurrence or convolutions.  \n",
      "  - Fully parallelizable, enabling faster training and inference.\n",
      "\n",
      "- **Architecture Highlights**  \n",
      "  - **Encoder**: 6 layers Ã— (multiâ€‘head selfâ€‘attention + positionâ€‘wise feedâ€‘forward).  \n",
      "  - **Decoder**: 6 layers Ã— (masked selfâ€‘attention + encoderâ€‘decoder attention + feedâ€‘forward).  \n",
      "  - **Multiâ€‘head attention**: 8 heads (base) or 16 heads (big); each head performs scaled dotâ€‘product attention.  \n",
      "  - **Positional encoding**: sinusoidal signals added to token embeddings; essential for sequence order.  \n",
      "  - Residual connections + layerâ€‘norm around every subâ€‘layer; final linear + softmax for output.\n",
      "\n",
      "- **Training Regimen**  \n",
      "  - Optimizer: Adam (Î²â‚=0.9, Î²â‚‚=0.98, Îµ=1eâ€‘9).  \n",
      "  - Learningâ€‘rate schedule: warmâ€‘up 4000 steps â†’ decay âˆâ€¯stepâ»Â¹/Â².  \n",
      "  - Dropout 0.1, label smoothing Îµ=0.1.  \n",
      "  - Beam search (sizeâ€¯4) during decoding.\n",
      "\n",
      "- **Experimental Results**  \n",
      "  - **WMTâ€¯2014 ENâ€‘DE**: Transformerâ€‘Base (d_modelâ€¯=â€¯512, hâ€¯=â€¯8) â†’ BLEUâ€¯28.4 (3â€¯days on 8â€¯GPUs).  \n",
      "  - **WMTâ€¯2014 ENâ€‘FR**: Transformerâ€‘Base â†’ BLEUâ€¯41.8 (4â€¯days on 8â€¯GPUs).  \n",
      "  - **Transformerâ€‘Big** (d_modelâ€¯=â€¯1024, hâ€¯=â€¯16) gives modest gains (â‰ˆâ€¯0.4 BLEU) at higher compute cost.  \n",
      "  - Outperforms LSTMâ€‘attention (â‰ˆâ€¯24â€“38 BLEU) and ConvS2S (â‰ˆâ€¯27â€“41 BLEU) by 3â€“4 BLEU points.  \n",
      "  - Training is ~3Ã— faster than comparable LSTM models due to full parallelism.\n",
      "\n",
      "- **Ablation Insights**  \n",
      "  - Removing positional encoding drops BLEU by ~1.5 points.  \n",
      "  - Learned positional embeddings perform comparably to sinusoidal ones.\n",
      "\n",
      "- **Conclusions**  \n",
      "  - **Attention alone suffices** for highâ€‘quality sequence transduction.  \n",
      "  - Eliminating recurrence/convolution yields superior accuracy, faster training, and easier parallelization.  \n",
      "  - The Transformer framework became the foundation for later models (BERT, GPT, T5, etc.).\n"
     ]
    }
   ],
   "source": [
    "print(summarized_context['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "02331d77-a0d6-498d-9a58-2e8f54d7a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPERVISOR_PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are the research coordinator agent. \n",
    "    Read the given research paper context\n",
    "    You can Breaks the paper into logical components such as problem, Method, Math (use the teach_math_tool for this),   \n",
    "    Theory (use the teach_paper_tool for this), Experiments (use the experiment_analysis_tool for this), Results\n",
    "    Assigns tasks to specialist agents, Controls analysis order, Provide agent inputs only with the relavant contenxt using the given tool.\n",
    "    Give ONLY relevent CONTEXT TO EACH AGENT / TOOLS WITH INPUT.\n",
    "    teach_math_tool - to explain math\n",
    "    teach_paper_tool - to explain the paper content\n",
    "    Decides when analysis is â€œcompleteâ€ \n",
    "    Break down user requests into appropriate tool calls and coordinate the results. \n",
    "    When a request involves multiple actions, use multiple tools in sequence.\n",
    "    finally you should give a full blown analysis descriptive report . Not Just A Summary.\n",
    "    Full blown agent analysis\n",
    "\n",
    "    Input: {input}\n",
    "    {agent_scratchpad}\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "3fb0dc35-0c12-49b6-897a-28101b49a32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm_supervisor = ChatGroq(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "supervisor_tools = [teach_paper_tool, teach_math_tool, experiment_analysis_tool]\n",
    "supervisor_llm_with_tools =llm_supervisor.bind_tools(supervisor_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "e0c90374-e0ab-4005-9d9c-152c30050e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_agent = create_tool_calling_agent(supervisor_llm_with_tools,supervisor_tools, SUPERVISOR_PROMPT)\n",
    "supervisor_agent_executor = AgentExecutor(agent=supervisor_agent, tools=supervisor_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "53302726-2e5d-4860-bc66-62fd8f2b5a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention is All You Need'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3maround each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "3\n",
      "\n",
      "tensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The ï¬rst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "2\n",
      "\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[31, 2, 8].\n",
      "â€¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "â€¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information ï¬‚ow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to âˆ’âˆ) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention is All You Need full paper'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3maround each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "3\n",
      "\n",
      "tensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The ï¬rst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "2\n",
      "\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[31, 2, 8].\n",
      "â€¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "â€¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information ï¬‚ow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to âˆ’âˆ) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention is All You Need full text pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mtensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "3\n",
      "\n",
      "(E) positional embedding instead of sinusoids 4.92 25.7\n",
      "big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneï¬cial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-ï¬tting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\n",
      "results to the base model.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the ï¬rst sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained signiï¬cantly faster than architectures based\n",
      "\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The ï¬rst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "2\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention is All You Need abstract'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mtensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "3\n",
      "\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The ï¬rst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "2\n",
      "\n",
      "transduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [31, 21, 13].\n",
      "âˆ—Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention is All You Need pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mtensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The ï¬rst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "2\n",
      "\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "3\n",
      "\n",
      "(E) positional embedding instead of sinusoids 4.92 25.7\n",
      "big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneï¬cial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-ï¬tting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\n",
      "results to the base model.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the ï¬rst sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained signiï¬cantly faster than architectures based\u001b[0m\u001b[32;1m\u001b[1;3m**Attention is All You Need â€“ Paper Overview**\n",
      "\n",
      "| Section | Key Points |\n",
      "|---------|------------|\n",
      "| **Abstract** | â€¢ Introduces the *Transformer*, a sequenceâ€‘toâ€‘sequence model that replaces recurrent and convolutional layers with a stack of *selfâ€‘attention* and *feedâ€‘forward* layers. <br>â€¢ Uses *scaled dotâ€‘product attention* and *multiâ€‘head attention* to capture longâ€‘range dependencies. <br>â€¢ Employs a *parameterâ€‘free sinusoidal positional encoding* to give the model a sense of token order. <br>â€¢ Achieves stateâ€‘ofâ€‘theâ€‘art BLEU scores on WMT 2014 Englishâ€‘German and Englishâ€‘French translation while training 3â€“6Ã— faster than comparable RNN/conv models. |\n",
      "| **Method** | 1. **Encoder** â€“ 6 identical layers, each with: <br>   * Multiâ€‘head selfâ€‘attention (queries, keys, values all from encoder output). <br>   * Positionâ€‘wise feedâ€‘forward network (two linear layers with ReLU). <br>   * Residual connection + layerâ€‘norm after each subâ€‘layer. <br>2. **Decoder** â€“ 6 identical layers, each with: <br>   * Masked multiâ€‘head selfâ€‘attention (prevent futureâ€‘token access). <br>   * Multiâ€‘head encoderâ€‘decoder attention (queries from decoder, keys/values from encoder). <br>   * Positionâ€‘wise feedâ€‘forward network. <br>   * Residual + layerâ€‘norm after each subâ€‘layer. <br>3. **Attention** â€“ *Scaled Dotâ€‘Product Attention*: <br>   \\[\n",
      "   \\text{Attention}(Q,K,V)=\\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
      "   \\] <br>4. **Multiâ€‘Head Attention** â€“ split Q,K,V into *h* heads, apply scaled dotâ€‘product in parallel, concatenate, then linear transform. <br>5. **Positional Encoding** â€“ sinusoidal functions: <br>   \\[\n",
      "   \\text{PE}_{(pos,2i)}=\\sin\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right),\\quad\n",
      "   \\text{PE}_{(pos,2i+1)}=\\cos\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
      "   \\] <br>6. **Training** â€“ Adam optimizer, learningâ€‘rate schedule with warmâ€‘up, label smoothing (Îµ=0.1). |\n",
      "| **Math** | â€¢ **Scaled Dotâ€‘Product Attention** (above). <br>â€¢ **Multiâ€‘Head Attention**: <br>   \\[\n",
      "   \\text{MultiHead}(Q,K,V)=\\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n",
      "   \\] where \\(\\text{head}_i=\\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\\). <br>â€¢ **Positionâ€‘wise Feedâ€‘Forward**: <br>   \\[\n",
      "   \\text{FFN}(x)=\\max(0,xW_1+b_1)W_2+b_2\n",
      "   \\] with \\(d_{\\text{ff}}=2048\\) (base) or 4096 (big). <br>â€¢ **Layer Normalization**: \\(\\text{LN}(x)=\\gamma\\frac{x-\\mu}{\\sigma}+\\beta\\). |\n",
      "| **Experiments** | â€¢ **Datasets**: WMT 2014 Englishâ€‘German (â‰ˆ4.5M sentence pairs) and Englishâ€‘French (â‰ˆ36M). <br>â€¢ **Model Variants**: <br>   * *Base*: \\(d_{\\text{model}}=512\\), \\(h=8\\), \\(d_{\\text{ff}}=2048\\). <br>   * *Big*: \\(d_{\\text{model}}=1024\\), \\(h=16\\), \\(d_{\\text{ff}}=4096\\). <br>â€¢ **Training Details**: <br>   * 1â€‘GPU (NVIDIA P100) or 8â€‘GPU (NVIDIA V100) setups. <br>   * 1â€¯M training steps, batch size 4096 tokens. <br>   * Warmâ€‘up for 4000 steps, then decay. <br>   * Dropout 0.1 (base) / 0.3 (big). <br>â€¢ **Evaluation**: BLEU scores on newstest2014 test set. <br>â€¢ **Speed**: Training 3â€“6Ã— faster than comparable RNN/conv models (e.g., GNMT, ConvS2S). |\n",
      "| **Results** | â€¢ **Englishâ€‘German** (newstest2014): <br>   * Base: 28.4 BLEU (vs. 27.3 GNMT). <br>   * Big: 30.6 BLEU (vs. 29.4 GNMT). <br>â€¢ **Englishâ€‘French** (newstest2014): <br>   * Base: 41.0 BLEU (vs. 40.4 ConvS2S). <br>   * Big: 42.4 BLEU (vs. 41.8 ConvS2S). <br>â€¢ **Training Time**: <br>   * Base model trained in ~3â€¯days on 8 GPUs (â‰ˆ1â€¯M steps). <br>   * Big model trained in ~5â€¯days on 8 GPUs. <br>â€¢ **Ablations**: <br>   * Removing positional encoding drops BLEU by ~2 points. <br>   * Using learned positional embeddings yields similar performance. <br>   * Reducing key dimension \\(d_k\\) hurts performance, confirming the need for rich compatibility. |\n",
      "\n",
      "**Summary**\n",
      "\n",
      "The Transformer demonstrates that a purely attentionâ€‘based architecture can outperform the best recurrent and convolutional models on largeâ€‘scale translation tasks while drastically reducing training time. Its key innovationsâ€”scaled dotâ€‘product attention, multiâ€‘head attention, and sinusoidal positional encodingâ€”enable efficient parallelization and capture longâ€‘range dependencies without recurrence. The paperâ€™s extensive experiments on WMT 2014 datasets confirm the modelâ€™s effectiveness and set a new benchmark for sequence transduction.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m**Attention Is All You Need â€“ Key Takeaways**\n",
      "\n",
      "- **Core Idea**  \n",
      "  - Replace RNN/conv layers with a stack of *selfâ€‘attention* and *feedâ€‘forward* layers (the Transformer).  \n",
      "  - Uses *scaled dotâ€‘product* and *multiâ€‘head* attention to model longâ€‘range dependencies.  \n",
      "  - Adds *parameterâ€‘free sinusoidal positional encoding* to provide token order.\n",
      "\n",
      "- **Architecture**  \n",
      "  - **Encoder**: 6 layers Ã— (multiâ€‘head selfâ€‘attention â†’ positionâ€‘wise FFN) + residual + layerâ€‘norm.  \n",
      "  - **Decoder**: 6 layers Ã— (masked selfâ€‘attention â†’ encoderâ€‘decoder attention â†’ FFN) + residual + layerâ€‘norm.  \n",
      "  - Attention formula: `Attention(Q,K,V)=softmax(QKáµ€/âˆšdk)V`.  \n",
      "  - Multiâ€‘head: split Q,K,V into `h` heads, compute attention per head, concatenate, linear transform.  \n",
      "  - FFN: `max(0, xW1+b1)W2+b2` (d_ff=2048/4096).  \n",
      "  - Positional encoding: sinusoidal `sin`/`cos` functions with frequency scaling.\n",
      "\n",
      "- **Training Setup**  \n",
      "  - Optimizer: Adam with warmâ€‘up schedule (4000 steps) and decay.  \n",
      "  - Label smoothing Îµ=0.1.  \n",
      "  - Dropout: 0.1 (Base), 0.3 (Big).  \n",
      "  - Batch: 4096 tokens, 1â€¯M steps, 8â€‘GPU V100 or 1â€‘GPU P100.\n",
      "\n",
      "- **Model Variants**  \n",
      "  - **Base**: d_model=512, h=8, d_ff=2048.  \n",
      "  - **Big**: d_model=1024, h=16, d_ff=4096.\n",
      "\n",
      "- **Datasets**  \n",
      "  - WMT 2014 Englishâ€‘German (~4.5â€¯M pairs).  \n",
      "  - WMT 2014 Englishâ€‘French (~36â€¯M pairs).\n",
      "\n",
      "- **Results**  \n",
      "  - **Englishâ€‘German**:  \n",
      "    - Base 28.4â€¯BLEU (vs. 27.3 GNMT).  \n",
      "    - Big 30.6â€¯BLEU (vs. 29.4 GNMT).  \n",
      "  - **Englishâ€‘French**:  \n",
      "    - Base 41.0â€¯BLEU (vs. 40.4 ConvS2S).  \n",
      "    - Big 42.4â€¯BLEU (vs. 41.8 ConvS2S).  \n",
      "  - Training time: ~3â€¯days (Base) / ~5â€¯days (Big) on 8 GPUs.  \n",
      "  - Speed: 3â€“6Ã— faster than RNN/conv baselines.\n",
      "\n",
      "- **Ablations & Insights**  \n",
      "  - Removing positional encoding â†’ ~2 BLEU drop.  \n",
      "  - Learned positional embeddings perform similarly.  \n",
      "  - Lower key dimension `d_k` degrades performance, underscoring rich compatibility.\n",
      "\n",
      "- **Conclusion**  \n",
      "  - A fully attentionâ€‘based model outperforms stateâ€‘ofâ€‘theâ€‘art recurrent and convolutional systems on largeâ€‘scale MT while enabling far faster training and better parallelism.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `teach_math_tool` with `{'context': '**Attention Is All You Need â€“ Key Takeaways**\\n\\n- **Architecture**  \\n  - Attention formula: `Attention(Q,K,V)=softmax(QKáµ€/âˆšdk)V`.  \\n  - Multiâ€‘head: split Q,K,V into `h` heads, compute attention per head, concatenate, linear transform.  \\n  - FFN: `max(0, xW1+b1)W2+b2` (d_ff=2048/4096).  \\n  - Positional encoding: sinusoidal `sin`/`cos` functions with frequency scaling.\\n- **Training Setup**  \\n  - Optimizer: Adam with warmâ€‘up schedule (4000 steps) and decay.\\n  - Label smoothing Îµ=0.1.\\n', 'request': 'Explain the mathematical formulas and concepts in simple terms, using bullet points. Include intuition behind scaled dotâ€‘product attention, multiâ€‘head splitting, feedâ€‘forward network, positional encoding, and the Adam warmâ€‘up schedule.'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m**Key Mathematical Components of â€œAttentionâ€¯Isâ€¯Allâ€¯Youâ€¯Needâ€ â€“ Simple Intuition & What They Do**\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Scaled Dotâ€‘Product Attention  \n",
      "\n",
      "- **Formula**  \n",
      "  \\[\n",
      "  \\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Big(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\Big)\\,V\n",
      "  \\]\n",
      "\n",
      "- **What the symbols mean**  \n",
      "  - **\\(Q\\)** (queries), **\\(K\\)** (keys), **\\(V\\)** (values): matrices that come from the same input sequence after linear projections.  \n",
      "  - **\\(d_k\\)**: dimensionality of each key (and query) vector.  \n",
      "  - **\\(QK^{\\top}\\)**: dotâ€‘product between every query and every key â†’ a similarity score for each pair.  \n",
      "  - **\\(\\frac{1}{\\sqrt{d_k}}\\)**: scaling factor (see intuition below).  \n",
      "  - **softmax**: turns the similarity scores into a probability distribution (weights that sum toâ€¯1).  \n",
      "  - **Multiplying by \\(V\\)**: mixes the value vectors according to those weights â†’ the final â€œattention outputâ€.\n",
      "\n",
      "- **Why the scaling \\(\\frac{1}{\\sqrt{d_k}}\\)?**  \n",
      "  - Dotâ€‘products grow in magnitude with the vector dimension.  \n",
      "  - Large values push the softmax into the saturated (very close to 0 or 1) regime â†’ gradients become tiny â†’ hard to train.  \n",
      "  - Dividing by \\(\\sqrt{d_k}\\) keeps the variance of the scores roughly constant, giving a smoother softmax and stable gradients.\n",
      "\n",
      "- **Assumptions**  \n",
      "  - Queries, keys, and values are linearly projected from the same source (or from different sources in encoderâ€‘decoder attention).  \n",
      "  - All vectors have the same dimension \\(d_k\\) (or \\(d_v\\) for values).\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Multiâ€‘Head Attention  \n",
      "\n",
      "- **Idea**: Instead of computing one attention with a single set of \\(Q,K,V\\), compute **\\(h\\)** separate attentions (â€œheadsâ€) in parallel, each on a different subâ€‘space of the model dimension.\n",
      "\n",
      "- **Steps (in equations)**  \n",
      "\n",
      "  1. **Linear projections for each head**  \n",
      "     \\[\n",
      "     Q_i = XW_i^{Q},\\; K_i = XW_i^{K},\\; V_i = XW_i^{V}\\quad (i=1,\\dots,h)\n",
      "     \\]  \n",
      "     where \\(W_i^{Q},W_i^{K},W_i^{V}\\) are learned matrices that map the original dimension \\(d_{\\text{model}}\\) to a smaller head dimension \\(d_k = d_{\\text{model}}/h\\).\n",
      "\n",
      "  2. **Apply scaled dotâ€‘product attention per head**  \n",
      "     \\[\n",
      "     \\text{head}_i = \\operatorname{Attention}(Q_i,K_i,V_i)\n",
      "     \\]\n",
      "\n",
      "  3. **Concatenate heads**  \n",
      "     \\[\n",
      "     \\text{Concat} = \\text{head}_1 \\,\\|\\, \\text{head}_2 \\,\\|\\, \\dots \\,\\|\\, \\text{head}_h\n",
      "     \\]\n",
      "\n",
      "  4. **Final linear projection**  \n",
      "     \\[\n",
      "     \\text{MultiHead}(Q,K,V)=\\text{Concat}\\,W^{O}\n",
      "     \\]  \n",
      "     where \\(W^{O}\\) mixes the concatenated heads back to the original model dimension.\n",
      "\n",
      "- **Why split into heads?**  \n",
      "  - Each head can attend to different types of relationships (e.g., syntactic vs. semantic) because it works in its own subâ€‘space.  \n",
      "  - Parallel computation â†’ same cost as a single attention but richer representation.\n",
      "\n",
      "- **Assumptions**  \n",
      "  - The total model dimension \\(d_{\\text{model}}\\) is divisible by the number of heads \\(h\\).  \n",
      "  - All heads share the same softmax temperature (the \\(\\sqrt{d_k}\\) scaling).\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Positionâ€‘wise Feedâ€‘Forward Network (FFN)  \n",
      "\n",
      "- **Formula**  \n",
      "  \\[\n",
      "  \\text{FFN}(x)=\\max(0,\\,xW_1+b_1)\\,W_2+b_2\n",
      "  \\]  \n",
      "  (i.e., a twoâ€‘layer fullyâ€‘connected network with a ReLU nonâ€‘linearity in between).\n",
      "\n",
      "- **Dimensions**  \n",
      "  - Input/Output dimension = \\(d_{\\text{model}}\\) (e.g., 512).  \n",
      "  - Hidden dimension \\(d_{\\text{ff}}\\) is larger (commonly 2048 or 4096), giving the network extra capacity.\n",
      "\n",
      "- **What it does**  \n",
      "  - Applied **independently** to each position (token) of the sequence, hence â€œpositionâ€‘wiseâ€.  \n",
      "  - Provides a nonâ€‘linear transformation that can mix information across the feature dimension, complementing the linear mixing done by attention.\n",
      "\n",
      "- **Why a simple twoâ€‘layer MLP?**  \n",
      "  - Itâ€™s cheap to compute (just matrix multiplications).  \n",
      "  - The large hidden size lets the model learn complex feature interactions without blowing up the overall parameter count.\n",
      "\n",
      "- **Assumptions**  \n",
      "  - Same parameters \\(W_1,W_2,b_1,b_2\\) are used for every token (weight sharing across positions).  \n",
      "  - ReLU is the chosen activation; other activations could be used but ReLU works well in practice.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Positional Encoding  \n",
      "\n",
      "- **Problem**: Selfâ€‘attention treats the input as a set, losing order information. We need to inject a notion of â€œpositionâ€.\n",
      "\n",
      "- **Sinusoidal encoding (used in the paper)**  \n",
      "  \\[\n",
      "  \\begin{aligned}\n",
      "  PE_{(pos,2i)}   &= \\sin\\!\\big(pos / 10000^{2i/d_{\\text{model}}}\\big)\\\\\n",
      "  PE_{(pos,2i+1)} &= \\cos\\!\\big(pos / 10000^{2i/d_{\\text{model}}}\\big)\n",
      "  \\end{aligned}\n",
      "  \\]  \n",
      "  where  \n",
      "  - \\(pos\\) = token index (0,â€¯1,â€¯2,â€¯â€¦)  \n",
      "  - \\(i\\) = dimension index (0 â€¦ \\(d_{\\text{model}}/2-1\\))\n",
      "\n",
      "- **Intuition**  \n",
      "  - Each dimension of the encoding is a sinusoid with a different frequency.  \n",
      "  - Any relative offset between two positions can be expressed as a linear combination of these sinusoids, allowing the model to learn to attend based on distance.  \n",
      "  - No learned parameters â†’ the same encoding works for any sequence length (extrapolation).\n",
      "\n",
      "- **Assumptions**  \n",
      "  - The model can learn to use these fixed patterns; no need for learned positional embeddings (though later work does use them).  \n",
      "  - The base \\(10000\\) and the division by \\(d_{\\text{model}}\\) are design choices that spread frequencies evenly across dimensions.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Training Objective â€“ Label Smoothing  \n",
      "\n",
      "- **Standard crossâ€‘entropy loss** would push the model to assign probabilityâ€¯1 to the correct token.  \n",
      "- **Label smoothing** replaces the oneâ€‘hot target \\(y\\) with a softened version:  \n",
      "  \\[\n",
      "  y_{\\text{smooth}} = (1-\\epsilon)\\,y + \\epsilon / V\n",
      "  \\]  \n",
      "  where  \n",
      "  - \\(\\epsilon = 0.1\\) (the smoothing factor).  \n",
      "  - \\(V\\) = size of the vocabulary.\n",
      "\n",
      "- **Why smooth?**  \n",
      "  - Prevents the model from becoming overâ€‘confident, which improves generalization.  \n",
      "  - Encourages the network to distribute some probability mass to other tokens, acting like a regularizer.\n",
      "\n",
      "- **Assumptions**  \n",
      "  - The vocabulary size is known and fixed.  \n",
      "  - The same \\(\\epsilon\\) is applied to every training example.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Optimizer â€“ Adam with Warmâ€‘up & Decay  \n",
      "\n",
      "- **Adam basics** (not repeated in detail): maintains perâ€‘parameter adaptive learning rates using estimates of firstâ€‘ and secondâ€‘order moments of gradients.\n",
      "\n",
      "- **Learningâ€‘rate schedule used in the paper**  \n",
      "\n",
      "  \\[\n",
      "  \\text{lr(step)} = d_{\\text{model}}^{-0.5}\\,\\min\\!\\big(step^{-0.5},\\; step \\times \\text{warmup\\_steps}^{-1.5}\\big)\n",
      "  \\]  \n",
      "\n",
      "  - **Parameters**  \n",
      "    - \\(d_{\\text{model}}\\) = model dimension (e.g., 512).  \n",
      "    - \\(\\text{warmup\\_steps}=4000\\).  \n",
      "\n",
      "- **How it works**  \n",
      "  1. **Warmâ€‘up phase** (first 4000 steps): learning rate **increases linearly** with the step number (`step / warmup_steps^{1.5}`), letting the model start gently.  \n",
      "  2. **After warmâ€‘up**: learning rate **decreases proportionally to \\(1/\\sqrt{step}\\)**, providing a smooth decay that keeps updates small as training progresses.\n",
      "\n",
      "- **Intuition**  \n",
      "  - Early in training the modelâ€™s parameters are random; a small learning rate avoids blowing up.  \n",
      "  - As the model stabilizes, we can afford larger steps (warmâ€‘up) to move quickly out of the random region.  \n",
      "  - Later, we need finer adjustments â†’ decay.\n",
      "\n",
      "- **Assumptions**  \n",
      "  - The same schedule is applied to all parameters (global LR).  \n",
      "  - The choice of 4000 warmâ€‘up steps and the scaling by \\(d_{\\text{model}}^{-0.5}\\) were empirically found to work well for the Transformer size used.\n",
      "\n",
      "---\n",
      "\n",
      "## ğŸ“Œ Quickâ€‘Reference Bullet Summary\n",
      "\n",
      "- **Scaled Dotâ€‘Product Attention**  \n",
      "  - Computes similarity \\(QK^{\\top}\\), scales by \\(\\sqrt{d_k}\\), softmax â†’ weights, then mixes values \\(V\\).  \n",
      "  - Scaling keeps gradients stable.\n",
      "\n",
      "- **Multiâ€‘Head Attention**  \n",
      "  - Split \\(Q,K,V\\) into \\(h\\) smaller heads, run attention in each, concatenate, linearly project back.  \n",
      "  - Gives the model multiple â€œviewsâ€ of relationships.\n",
      "\n",
      "- **Feedâ€‘Forward Network (FFN)**  \n",
      "  - Two linear layers with ReLU: \\( \\text{ReLU}(xW_1+b_1)W_2+b_2\\).  \n",
      "  - Applied per token, expands to a larger hidden size (\\(d_{ff}\\)) then projects back.\n",
      "\n",
      "- **Positional Encoding**  \n",
      "  - Fixed sinusoidal functions of position: \\(\\sin\\) for even dims, \\(\\cos\\) for odd dims, with frequencies spanning a wide range.  \n",
      "  - Provides order information without learned parameters.\n",
      "\n",
      "- **Label Smoothing (Îµ = 0.1)**  \n",
      "  - Softens oneâ€‘hot targets: \\( (1-Îµ) \\) on correct class, \\( Îµ/V \\) on all others.  \n",
      "  - Regularizes, reduces overâ€‘confidence.\n",
      "\n",
      "- **Adam Warmâ€‘up Schedule (4000 steps)**  \n",
      "  - LR = \\(d_{\\text{model}}^{-0.5} \\times \\min(step^{-0.5}, step \\times 4000^{-1.5})\\).  \n",
      "  - Linear increase for first 4000 steps, then decay as \\(1/\\sqrt{step}\\).  \n",
      "  - Stabilizes early training and provides smooth decay later.\n",
      "\n",
      "These equations together form the core of the Transformer architecture, enabling it to learn rich, positionâ€‘aware relationships in sequences while remaining efficient to train.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m**Key Mathematical Components of â€œAttentionâ€¯Isâ€¯Allâ€¯Youâ€¯Needâ€ â€“ Simple Intuition & What They Do**\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Scaled Dotâ€‘Product Attention  \n",
      "\n",
      "- **Formula**  \n",
      "  \\[\n",
      "  \\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Big(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\Big)\\,V\n",
      "  \\]\n",
      "\n",
      "- **What the symbols mean**  \n",
      "  - **\\(Q\\)** (queries), **\\(K\\)** (keys), **\\(V\\)** (values): matrices that come from the same input sequence after linear projections.  \n",
      "  - **\\(d_k\\)**: dimensionality of each key (and query) vector.  \n",
      "  - **\\(QK^{\\top}\\)**: dotâ€‘product between every query and every key â†’ a similarity score for each pair.  \n",
      "  - **\\(\\frac{1}{\\sqrt{d_k}}\\)**: scaling factor (see intuition below).  \n",
      "  - **softmax**: turns the similarity scores into a probability distribution (weights that sum toâ€¯1).  \n",
      "  - **Multiplying by \\(V\\)**: mixes the value vectors according to those weights â†’ the final â€œattention outputâ€.\n",
      "\n",
      "- **Why the scaling \\(\\frac{1}{\\sqrt{d_k}}\\)?**  \n",
      "  - Dotâ€‘products grow in magnitude with the vector dimension.  \n",
      "  - Large values push the softmax into the saturated (very close to 0 or 1) regime â†’ gradients become tiny â†’ hard to train.  \n",
      "  - Dividing by \\(\\sqrt{d_k}\\) keeps the variance of the scores roughly constant, giving a smoother softmax and stable gradients.\n",
      "\n",
      "- **Assumptions**  \n",
      "  - Queries, keys, and values are linearly projected from the same source (or from different sources in encoderâ€‘decoder attention).  \n",
      "  - All vectors have the same dimension \\(d_k\\) (or \\(d_v\\) for values).\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Multiâ€‘Head Attention  \n",
      "\n",
      "- **Idea**: Instead of computing one attention with a single set of \\(Q,K,V\\), compute **\\(h\\)** separate attentions (â€œheadsâ€) in parallel, each on a different subâ€‘space of the model dimension.\n",
      "\n",
      "- **Steps (in equations)**  \n",
      "\n",
      "  1. **Linear projections for each head**  \n",
      "     \\[\n",
      "     Q_i = XW_i^{Q},\\; K_i = XW_i^{K},\\; V_i = XW_i^{V}\\quad (i=1,\\dots,h)\n",
      "     \\]  \n",
      "     where \\(W_i^{Q},W_i^{K},W_i^{V}\\) are learned matrices that map the original dimension \\(d_{\\text{model}}\\) to a smaller head dimension \\(d_k = d_{\\text{model}}/h\\).\n",
      "\n",
      "  2. **Apply scaled dotâ€‘product attention per head**  \n",
      "     \\[\n",
      "     \\text{head}_i = \\operatorname{Attention}(Q_i,K_i,V_i)\n",
      "     \\]\n",
      "\n",
      "  3. **Concatenate heads**  \n",
      "     \\[\n",
      "     \\text{Concat} = \\text{head}_1 \\,\\|\\, \\text{head}_2 \\,\\|\\, \\dots \\,\\|\\, \\text{head}_h\n",
      "     \\]\n",
      "\n",
      "  4. **Final linear projection**  \n",
      "     \\[\n",
      "     \\text{MultiHead}(Q,K,V)=\\text{Concat}\\,W^{O}\n",
      "     \\]  \n",
      "     where \\(W^{O}\\) mixes the concatenated heads back to the original model dimension.\n",
      "\n",
      "- **Why split into heads?**  \n",
      "  - Each head can attend to different types of relationships (e.g., syntactic vs. semantic) because it works in its own subâ€‘space.  \n",
      "  - Parallel computation â†’ same cost as a single attention but richer representation.\n",
      "\n",
      "- **Assumptions**  \n",
      "  - The total model dimension \\(d_{\\text{model}}\\) is divisible by the number of heads \\(h\\).  \n",
      "  - All heads share the same softmax temperature (the \\(\\sqrt{d_k}\\) scaling).\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Positionâ€‘wise Feedâ€‘Forward Network (FFN)  \n",
      "\n",
      "- **Formula**  \n",
      "  \\[\n",
      "  \\text{FFN}(x)=\\max(0,\\,xW_1+b_1)\\,W_2+b_2\n",
      "  \\]  \n",
      "  (i.e., a twoâ€‘layer fullyâ€‘connected network with a ReLU nonâ€‘linearity in between).\n",
      "\n",
      "- **Dimensions**  \n",
      "  - Input/Output dimension = \\(d_{\\text{model}}\\) (e.g., 512).  \n",
      "  - Hidden dimension \\(d_{\\text{ff}}\\) is larger (commonly 2048 or 4096), giving the network extra capacity.\n",
      "\n",
      "- **What it does**  \n",
      "  - Applied **independently** to each position (token) of the sequence, hence â€œpositionâ€‘wiseâ€.  \n",
      "  - Provides a nonâ€‘linear transformation that can mix information across the feature dimension, complementing the linear mixing done by attention.\n",
      "\n",
      "- **Why a simple twoâ€‘layer MLP?**  \n",
      "  - Itâ€™s cheap to compute (just matrix multiplications).  \n",
      "  - The large hidden size lets the model learn complex feature interactions without blowing up the overall parameter count.\n",
      "\n",
      "- **Assumptions**  \n",
      "  - Same parameters \\(W_1,W_2,b_1,b_2\\) are used for every token (weight sharing across positions).  \n",
      "  - ReLU is the chosen activation; other activations could be used but ReLU works well in practice.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Positional Encoding  \n",
      "\n",
      "- **Problem**: Selfâ€‘attention treats the input as a set, losing order information. We need to inject a notion of â€œpositionâ€.\n",
      "\n",
      "- **Sinusoidal encoding (used in the paper)**  \n",
      "  \\[\n",
      "  \\begin{aligned}\n",
      "  PE_{(pos,2i)}   &= \\sin\\!\\big(pos / 10000^{2i/d_{\\text{model}}}\\big)\\\\\n",
      "  PE_{(pos,2i+1)} &= \\cos\\!\\big(pos / 10000^{2i/d_{\\text{model}}}\\big)\n",
      "  \\end{aligned}\n",
      "  \\]  \n",
      "  where  \n",
      "  - \\(pos\\) = token index (0,â€¯1,â€¯2,â€¯â€¦)  \n",
      "  - \\(i\\) = dimension index (0 â€¦ \\(d_{\\text{model}}/2-1\\))\n",
      "\n",
      "- **Intuition**  \n",
      "  - Each dimension of the encoding is a sinusoid with a different frequency.  \n",
      "  - Any relative offset between two positions can be expressed as a linear combination of these sinusoids, allowing the model to learn to attend based on distance.  \n",
      "  - No learned parameters â†’ the same encoding works for any sequence length (extrapolation).\n",
      "\n",
      "- **Assumptions**  \n",
      "  - The model can learn to use these fixed patterns; no need for learned positional embeddings (though later work does use them).  \n",
      "  - The base \\(10000\\) and the division by \\(d_{\\text{model}}\\) are design choices that spread frequencies evenly across dimensions.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Training Objective â€“ Label Smoothing  \n",
      "\n",
      "- **Standard crossâ€‘entropy loss** would push the model to assign probabilityâ€¯1 to the correct token.  \n",
      "- **Label smoothing** replaces the oneâ€‘hot target \\(y\\) with a softened version:  \n",
      "  \\[\n",
      "  y_{\\text{smooth}} = (1-\\epsilon)\\,y + \\epsilon / V\n",
      "  \\]  \n",
      "  where  \n",
      "  - \\(\\epsilon = 0.1\\) (the smoothing factor).  \n",
      "  - \\(V\\) = size of the vocabulary.\n",
      "\n",
      "- **Why smooth?**  \n",
      "  - Prevents the model from becoming overâ€‘confident, which improves generalization.  \n",
      "  - Encourages the network to distribute some probability mass to other tokens, acting like a regularizer.\n",
      "\n",
      "- **Assumptions**  \n",
      "  - The vocabulary size is known and fixed.  \n",
      "  - The same \\(\\epsilon\\) is applied to every training example.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Optimizer â€“ Adam with Warmâ€‘up & Decay  \n",
      "\n",
      "- **Adam basics** (not repeated in detail): maintains perâ€‘parameter adaptive learning rates using estimates of firstâ€‘ and secondâ€‘order moments of gradients.\n",
      "\n",
      "- **Learningâ€‘rate schedule used in the paper**  \n",
      "\n",
      "  \\[\n",
      "  \\text{lr(step)} = d_{\\text{model}}^{-0.5}\\,\\min\\!\\big(step^{-0.5},\\; step \\times \\text{warmup\\_steps}^{-1.5}\\big)\n",
      "  \\]  \n",
      "\n",
      "  - **Parameters**  \n",
      "    - \\(d_{\\text{model}}\\) = model dimension (e.g., 512).  \n",
      "    - \\(\\text{warmup\\_steps}=4000\\).  \n",
      "\n",
      "- **How it works**  \n",
      "  1. **Warmâ€‘up phase** (first 4000 steps): learning rate **increases linearly** with the step number (`step / warmup_steps^{1.5}`), letting the model start gently.  \n",
      "  2. **After warmâ€‘up**: learning rate **decreases proportionally to \\(1/\\sqrt{step}\\)**, providing a smooth decay that keeps updates small as training progresses.\n",
      "\n",
      "- **Intuition**  \n",
      "  - Early in training the modelâ€™s parameters are random; a small learning rate avoids blowing up.  \n",
      "  - As the model stabilizes, we can afford larger steps (warmâ€‘up) to move quickly out of the random region.  \n",
      "  - Later, we need finer adjustments â†’ decay.\n",
      "\n",
      "- **Assumptions**  \n",
      "  - The same schedule is applied to all parameters (global LR).  \n",
      "  - The choice of 4000 warmâ€‘up steps and the scaling by \\(d_{\\text{model}}^{-0.5}\\) were empirically found to work well for the Transformer size used.\n",
      "\n",
      "---\n",
      "\n",
      "## ğŸ“Œ Quickâ€‘Reference Bullet Summary\n",
      "\n",
      "- **Scaled Dotâ€‘Product Attention**  \n",
      "  - Computes similarity \\(QK^{\\top}\\), scales by \\(\\sqrt{d_k}\\), softmax â†’ weights, then mixes values \\(V\\).  \n",
      "  - Scaling keeps gradients stable.\n",
      "\n",
      "- **Multiâ€‘Head Attention**  \n",
      "  - Split \\(Q,K,V\\) into \\(h\\) smaller heads, run attention in each, concatenate, linearly project back.  \n",
      "  - Gives the model multiple â€œviewsâ€ of relationships.\n",
      "\n",
      "- **Feedâ€‘Forward Network (FFN)**  \n",
      "  - Two linear layers with ReLU: \\( \\text{ReLU}(xW_1+b_1)W_2+b_2\\).  \n",
      "  - Applied per token, expands to a larger hidden size (\\(d_{ff}\\)) then projects back.\n",
      "\n",
      "- **Positional Encoding**  \n",
      "  - Fixed sinusoidal functions of position: \\(\\sin\\) for even dims, \\(\\cos\\) for odd dims, with frequencies spanning a wide range.  \n",
      "  - Provides order information without learned parameters.\n",
      "\n",
      "- **Label Smoothing (Îµ = 0.1)**  \n",
      "  - Softens oneâ€‘hot targets: \\( (1-Îµ) \\) on correct class, \\( Îµ/V \\) on all others.  \n",
      "  - Regularizes, reduces overâ€‘confidence.\n",
      "\n",
      "- **Adam Warmâ€‘up Schedule (4000 steps)**  \n",
      "  - LR = \\(d_{\\text{model}}^{-0.5} \\times \\min(step^{-0.5}, step \\times 4000^{-1.5})\\).  \n",
      "  - Linear increase for first 4000 steps, then decay as \\(1/\\sqrt{step}\\).  \n",
      "  - Stabilizes early training and provides smooth decay later.\n",
      "\n",
      "These equations together form the core of the Transformer architecture, enabling it to learn rich, positionâ€‘aware relationships in sequences while remaining efficient to train.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `teach_paper_tool` with `{'context': '**Attention Is All You Need â€“ Key Takeaways**\\n\\n- **Core Idea**  \\n  - Replace RNN/conv layers with a stack of *selfâ€‘attention* and *feedâ€‘forward* layers (the Transformer).  \\n  - Uses *scaled dotâ€‘product* and *multiâ€‘head* attention to model longâ€‘range dependencies.  \\n  - Adds *parameterâ€‘free sinusoidal positional encoding* to provide token order.\\n\\n- **Architecture**  \\n  - **Encoder**: 6 layers Ã— (multiâ€‘head selfâ€‘attention â†’ positionâ€‘wise FFN) + residual + layerâ€‘norm.  \\n  - **Decoder**: 6 layers Ã— (masked selfâ€‘attention â†’ encoderâ€‘decoder attention â†’ FFN) + residual + layerâ€‘norm.  \\n  - Attention formula: `Attention(Q,K,V)=softmax(QKáµ€/âˆšdk)V`.  \\n  - Multiâ€‘head: split Q,K,V into `h` heads, compute attention per head, concatenate, linear transform.  \\n  - FFN: `max(0, xW1+b1)W2+b2` (d_ff=2048/4096).  \\n  - Positional encoding: sinusoidal `sin`/`cos` functions with frequency scaling.\\n\\n- **Training Setup**  \\n  - Optimizer: Adam with warmâ€‘up schedule (4000 steps) and decay.  \\n  - Label smoothing Îµ=0.1.  \\n  - Dropout: 0.1 (Base), 0.3 (Big).  \\n  - Batch: 4096 tokens, 1\\u202fM steps, 8â€‘GPU V100 or 1â€‘GPU P100.\\n\\n- **Model Variants**  \\n  - **Base**: d_model=512, h=8, d_ff=2048.  \\n  - **Big**: d_model=1024, h=16, d_ff=4096.\\n\\n- **Datasets**  \\n  - WMT 2014 Englishâ€‘German (~4.5\\u202fM pairs).  \\n  - WMT 2014 Englishâ€‘French (~36\\u202fM pairs).\\n\\n- **Results**  \\n  - **Englishâ€‘German**:  \\n    - Base 28.4\\u202fBLEU (vs. 27.3 GNMT).  \\n    - Big 30.6\\u202fBLEU (vs. 29.4 GNMT).  \\n  - **Englishâ€‘French**:  \\n    - Base 41.0\\u202fBLEU (vs. 40.4 ConvS2S).  \\n    - Big 42.4\\u202fBLEU (vs. 41.8 ConvS2S).  \\n  - Training time: ~3\\u202fdays (Base) / ~5\\u202fdays (Big) on 8 GPUs.  \\n  - Speed: 3â€“6Ã— faster than RNN/conv baselines.\\n\\n- **Ablations & Insights**  \\n  - Removing positional encoding â†’ ~2 BLEU drop.  \\n  - Learned positional embeddings perform similarly.  \\n  - Lower key dimension `d_k` degrades performance, underscoring rich compatibility.\\n\\n- **Conclusion**  \\n  - A fully attentionâ€‘based model outperforms stateâ€‘ofâ€‘theâ€‘art recurrent and convolutional systems on largeâ€‘scale MT while enabling far faster training and better parallelism.', 'request': \"Explain the paper's problem statement, proposed method, key contributions, and intuition behind the architecture in beginnerâ€‘friendly bullet points.\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m**Problem the paper tackles**\n",
      "\n",
      "- *Machine translation* (English â†” German/French) needs to understand how every word in a sentence relates to every other word, even if theyâ€™re far apart.\n",
      "- Traditional models (RNNs, CNNs) process words one after another or in fixedâ€‘size windows, which is slow and struggles with longâ€‘range dependencies.\n",
      "\n",
      "**What the authors propose**\n",
      "\n",
      "- A **pureâ€‘attention** architecture called the **Transformer** that replaces RNNs/CNNs with layers of *selfâ€‘attention* and simple feedâ€‘forward networks.\n",
      "- Adds a *positional encoding* so the model still knows the order of words.\n",
      "\n",
      "**Key ideas in plain language**\n",
      "\n",
      "| Idea | Analogy / Intuition | How it works |\n",
      "|------|---------------------|--------------|\n",
      "| **Selfâ€‘attention** | Imagine a group of people in a room all looking at each other at the same time to decide what to say. | Each word (token) creates three vectors: Query (Q), Key (K), Value (V). The dotâ€‘product of Q and K tells how much two words â€œpay attentionâ€ to each other. The result is weighted sum of Vâ€™s â†’ each word becomes a mix of all others. |\n",
      "| **Scaled dotâ€‘product** | Prevents huge numbers that would make the softmax flat. | Divide QKáµ€ by âˆš(d_k) before softmax. |\n",
      "| **Multiâ€‘head attention** | Like having several pairs of eyes looking at the sentence from different angles. | Split Q, K, V into `h` heads, compute attention in each head, then stitch the results back together. |\n",
      "| **Feedâ€‘forward network (FFN)** | A tiny neural net that refines each wordâ€™s representation after attention. | Two linear layers with ReLU in between (`max(0, xW1+b1)W2+b2`). |\n",
      "| **Positional encoding** | A GPS signal that tells each word where it sits in the sentence. | Add sinusoidal functions of different frequencies to each tokenâ€™s vector; no extra parameters. |\n",
      "| **Residual + LayerNorm** | Like a shortcut that lets the original signal pass through unchanged, making training easier. | Add the input of a subâ€‘layer to its output, then normalize. |\n",
      "\n",
      "**Architecture in biteâ€‘size steps**\n",
      "\n",
      "1. **Encoder** â€“ 6 identical blocks:  \n",
      "   - Multiâ€‘head selfâ€‘attention â†’ addâ€‘residual â†’ layerâ€‘norm  \n",
      "   - Positionâ€‘wise FFN â†’ addâ€‘residual â†’ layerâ€‘norm\n",
      "2. **Decoder** â€“ 6 identical blocks:  \n",
      "   - Masked selfâ€‘attention (so a word canâ€™t look ahead) â†’ addâ€‘residual â†’ layerâ€‘norm  \n",
      "   - Encoderâ€‘decoder attention (attend to encoder outputs) â†’ addâ€‘residual â†’ layerâ€‘norm  \n",
      "   - FFN â†’ addâ€‘residual â†’ layerâ€‘norm\n",
      "3. Final linear + softmax to produce output words.\n",
      "\n",
      "**Training tricks**\n",
      "\n",
      "- **Adam optimizer** with a *warmâ€‘up* learningâ€‘rate schedule (slow start, then fast, then decay).\n",
      "- **Label smoothing** (Îµ=0.1) to avoid overâ€‘confident predictions.\n",
      "- **Dropout** (0.1 for Base, 0.3 for Big) to regularize.\n",
      "- Large batch (4096 tokens) and long training (1â€¯M steps) on 8 GPUs.\n",
      "\n",
      "**Model sizes**\n",
      "\n",
      "| Variant | d_model | heads | d_ff |\n",
      "|---------|---------|-------|------|\n",
      "| Base | 512 | 8 | 2048 |\n",
      "| Big | 1024 | 16 | 4096 |\n",
      "\n",
      "**Datasets & results**\n",
      "\n",
      "- WMT 2014 Englishâ€‘German (â‰ˆ4.5â€¯M pairs) & Englishâ€‘French (â‰ˆ36â€¯M pairs).\n",
      "- **Englishâ€‘German**: Base 28.4â€¯BLEU vs. 27.3 (GNMT); Big 30.6â€¯BLEU vs. 29.4 (GNMT).\n",
      "- **Englishâ€‘French**: Base 41.0â€¯BLEU vs. 40.4 (ConvS2S); Big 42.4â€¯BLEU vs. 41.8 (ConvS2S).\n",
      "- Training: ~3â€¯days (Base) / ~5â€¯days (Big) on 8 GPUs.\n",
      "- Speed: 3â€“6Ã— faster inference/training than RNN/conv baselines.\n",
      "\n",
      "**Ablation insights**\n",
      "\n",
      "- Removing positional encoding â†’ ~2 BLEU drop â†’ order matters.\n",
      "- Learned positional embeddings work almost as well as sinusoidal ones.\n",
      "- Lower key dimension `d_k` hurts performance â†’ richer attention is needed.\n",
      "\n",
      "**Takeâ€‘away**\n",
      "\n",
      "- A *fully attentionâ€‘based* model (Transformer) can replace RNNs/CNNs for sequence tasks.\n",
      "- It learns longâ€‘range relationships in parallel, trains faster, and achieves stateâ€‘ofâ€‘theâ€‘art translation quality.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m**Problem the paper tackles**\n",
      "\n",
      "- *Machine translation* (English â†” German/French) needs to understand how every word in a sentence relates to every other word, even if theyâ€™re far apart.\n",
      "- Traditional models (RNNs, CNNs) process words one after another or in fixedâ€‘size windows, which is slow and struggles with longâ€‘range dependencies.\n",
      "\n",
      "**What the authors propose**\n",
      "\n",
      "- A **pureâ€‘attention** architecture called the **Transformer** that replaces RNNs/CNNs with layers of *selfâ€‘attention* and simple feedâ€‘forward networks.\n",
      "- Adds a *positional encoding* so the model still knows the order of words.\n",
      "\n",
      "**Key ideas in plain language**\n",
      "\n",
      "| Idea | Analogy / Intuition | How it works |\n",
      "|------|---------------------|--------------|\n",
      "| **Selfâ€‘attention** | Imagine a group of people in a room all looking at each other at the same time to decide what to say. | Each word (token) creates three vectors: Query (Q), Key (K), Value (V). The dotâ€‘product of Q and K tells how much two words â€œpay attentionâ€ to each other. The result is weighted sum of Vâ€™s â†’ each word becomes a mix of all others. |\n",
      "| **Scaled dotâ€‘product** | Prevents huge numbers that would make the softmax flat. | Divide QKáµ€ by âˆš(d_k) before softmax. |\n",
      "| **Multiâ€‘head attention** | Like having several pairs of eyes looking at the sentence from different angles. | Split Q, K, V into `h` heads, compute attention in each head, then stitch the results back together. |\n",
      "| **Feedâ€‘forward network (FFN)** | A tiny neural net that refines each wordâ€™s representation after attention. | Two linear layers with ReLU in between (`max(0, xW1+b1)W2+b2`). |\n",
      "| **Positional encoding** | A GPS signal that tells each word where it sits in the sentence. | Add sinusoidal functions of different frequencies to each tokenâ€™s vector; no extra parameters. |\n",
      "| **Residual + LayerNorm** | Like a shortcut that lets the original signal pass through unchanged, making training easier. | Add the input of a subâ€‘layer to its output, then normalize. |\n",
      "\n",
      "**Architecture in biteâ€‘size steps**\n",
      "\n",
      "1. **Encoder** â€“ 6 identical blocks:  \n",
      "   - Multiâ€‘head selfâ€‘attention â†’ addâ€‘residual â†’ layerâ€‘norm  \n",
      "   - Positionâ€‘wise FFN â†’ addâ€‘residual â†’ layerâ€‘norm\n",
      "2. **Decoder** â€“ 6 identical blocks:  \n",
      "   - Masked selfâ€‘attention (so a word canâ€™t look ahead) â†’ addâ€‘residual â†’ layerâ€‘norm  \n",
      "   - Encoderâ€‘decoder attention (attend to encoder outputs) â†’ addâ€‘residual â†’ layerâ€‘norm  \n",
      "   - FFN â†’ addâ€‘residual â†’ layerâ€‘norm\n",
      "3. Final linear + softmax to produce output words.\n",
      "\n",
      "**Training tricks**\n",
      "\n",
      "- **Adam optimizer** with a *warmâ€‘up* learningâ€‘rate schedule (slow start, then fast, then decay).\n",
      "- **Label smoothing** (Îµ=0.1) to avoid overâ€‘confident predictions.\n",
      "- **Dropout** (0.1 for Base, 0.3 for Big) to regularize.\n",
      "- Large batch (4096 tokens) and long training (1â€¯M steps) on 8 GPUs.\n",
      "\n",
      "**Model sizes**\n",
      "\n",
      "| Variant | d_model | heads | d_ff |\n",
      "|---------|---------|-------|------|\n",
      "| Base | 512 | 8 | 2048 |\n",
      "| Big | 1024 | 16 | 4096 |\n",
      "\n",
      "**Datasets & results**\n",
      "\n",
      "- WMT 2014 Englishâ€‘German (â‰ˆ4.5â€¯M pairs) & Englishâ€‘French (â‰ˆ36â€¯M pairs).\n",
      "- **Englishâ€‘German**: Base 28.4â€¯BLEU vs. 27.3 (GNMT); Big 30.6â€¯BLEU vs. 29.4 (GNMT).\n",
      "- **Englishâ€‘French**: Base 41.0â€¯BLEU vs. 40.4 (ConvS2S); Big 42.4â€¯BLEU vs. 41.8 (ConvS2S).\n",
      "- Training: ~3â€¯days (Base) / ~5â€¯days (Big) on 8 GPUs.\n",
      "- Speed: 3â€“6Ã— faster inference/training than RNN/conv baselines.\n",
      "\n",
      "**Ablation insights**\n",
      "\n",
      "- Removing positional encoding â†’ ~2 BLEU drop â†’ order matters.\n",
      "- Learned positional embeddings work almost as well as sinusoidal ones.\n",
      "- Lower key dimension `d_k` hurts performance â†’ richer attention is needed.\n",
      "\n",
      "**Takeâ€‘away**\n",
      "\n",
      "- A *fully attentionâ€‘based* model (Transformer) can replace RNNs/CNNs for sequence tasks.\n",
      "- It learns longâ€‘range relationships in parallel, trains faster, and achieves stateâ€‘ofâ€‘theâ€‘art translation quality.\u001b[0m"
     ]
    },
    {
     "ename": "APIStatusError",
     "evalue": "Error code: 413 - {'error': {'message': 'Request too large for model `openai/gpt-oss-120b` in organization `org_01k26dmhxver3bwfg5g7b19q0b` service tier `on_demand` on tokens per minute (TPM): Limit 8000, Requested 8984, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[618], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summary_context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_pipeline()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_output)\n",
      "Cell \u001b[1;32mIn[618], line 10\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     result_doc_agent\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m parse_agent_executor\u001b[38;5;241m.\u001b[39mainvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalyze the  given research paper with the given database retriver tool\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m      8\u001b[0m     summary_context\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m summary_agent_executor\u001b[38;5;241m.\u001b[39mainvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m:result_doc_agent[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[1;32m---> 10\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m supervisor_agent_executor\u001b[38;5;241m.\u001b[39mainvoke({\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124mAnalyze and explain the following research paper.\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124mUse provided tools when appropriate.\u001b[39m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     16\u001b[0m     ,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m:summary_context[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summary_context[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:215\u001b[0m, in \u001b[0;36mChain.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:206\u001b[0m, in \u001b[0;36mChain.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    205\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 206\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall(inputs)\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    210\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maprep_outputs(\n\u001b[0;32m    211\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    212\u001b[0m     )\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1674\u001b[0m, in \u001b[0;36mAgentExecutor._acall\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m asyncio_timeout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_execution_time):\n\u001b[0;32m   1673\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1674\u001b[0m         next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atake_next_step(\n\u001b[0;32m   1675\u001b[0m             name_to_tool_map,\n\u001b[0;32m   1676\u001b[0m             color_mapping,\n\u001b[0;32m   1677\u001b[0m             inputs,\n\u001b[0;32m   1678\u001b[0m             intermediate_steps,\n\u001b[0;32m   1679\u001b[0m             run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m   1680\u001b[0m         )\n\u001b[0;32m   1681\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1682\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_areturn(\n\u001b[0;32m   1683\u001b[0m                 next_step_output,\n\u001b[0;32m   1684\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1685\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m   1686\u001b[0m             )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1468\u001b[0m, in \u001b[0;36mAgentExecutor._atake_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_atake_next_step\u001b[39m(\n\u001b[0;32m   1460\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1461\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1465\u001b[0m     run_manager: Optional[AsyncCallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1466\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1468\u001b[0m         [\n\u001b[0;32m   1469\u001b[0m             a\n\u001b[0;32m   1470\u001b[0m             \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aiter_next_step(\n\u001b[0;32m   1471\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1472\u001b[0m                 color_mapping,\n\u001b[0;32m   1473\u001b[0m                 inputs,\n\u001b[0;32m   1474\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1475\u001b[0m                 run_manager,\n\u001b[0;32m   1476\u001b[0m             )\n\u001b[0;32m   1477\u001b[0m         ]\n\u001b[0;32m   1478\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1496\u001b[0m, in \u001b[0;36mAgentExecutor._aiter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1493\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[0;32m   1495\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1496\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_agent\u001b[38;5;241m.\u001b[39maplan(\n\u001b[0;32m   1497\u001b[0m         intermediate_steps,\n\u001b[0;32m   1498\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m   1500\u001b[0m     )\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:616\u001b[0m, in \u001b[0;36mRunnableMultiActionAgent.aplan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    608\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[1;32m--> 616\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39mastream(\n\u001b[0;32m    617\u001b[0m         inputs, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    618\u001b[0m     ):\n\u001b[0;32m    619\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    620\u001b[0m             final_output \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3287\u001b[0m, in \u001b[0;36mRunnableSequence.astream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3284\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minput_aiter\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Input]:\n\u001b[0;32m   3285\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m-> 3287\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matransform(input_aiter(), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   3288\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3270\u001b[0m, in \u001b[0;36mRunnableSequence.atransform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3264\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21matransform\u001b[39m(\n\u001b[0;32m   3265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3266\u001b[0m     \u001b[38;5;28minput\u001b[39m: AsyncIterator[Input],\n\u001b[0;32m   3267\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3268\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3269\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Output]:\n\u001b[1;32m-> 3270\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform_stream_with_config(\n\u001b[0;32m   3271\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3272\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform,\n\u001b[0;32m   3273\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   3274\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3275\u001b[0m     ):\n\u001b[0;32m   3276\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2163\u001b[0m, in \u001b[0;36mRunnable._atransform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   2161\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   2162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m asyncio_accepts_context():\n\u001b[1;32m-> 2163\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   2164\u001b[0m             py_anext(iterator),  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   2165\u001b[0m             context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m   2166\u001b[0m         )\n\u001b[0;32m   2167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2168\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m cast(Output, \u001b[38;5;28;01mawait\u001b[39;00m py_anext(iterator))\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3240\u001b[0m, in \u001b[0;36mRunnableSequence._atransform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3239\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39matransform(final_pipeline, config)\n\u001b[1;32m-> 3240\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_pipeline:\n\u001b[0;32m   3241\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1314\u001b[0m, in \u001b[0;36mRunnable.atransform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1311\u001b[0m final: Input\n\u001b[0;32m   1312\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1314\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[0;32m   1323\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5312\u001b[0m, in \u001b[0;36mRunnableBindingBase.atransform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5306\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21matransform\u001b[39m(\n\u001b[0;32m   5307\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5308\u001b[0m     \u001b[38;5;28minput\u001b[39m: AsyncIterator[Input],\n\u001b[0;32m   5309\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5310\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   5311\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Output]:\n\u001b[1;32m-> 5312\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39matransform(\n\u001b[0;32m   5313\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5314\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5315\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5316\u001b[0m     ):\n\u001b[0;32m   5317\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1332\u001b[0m, in \u001b[0;36mRunnable.atransform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1329\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1333\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:485\u001b[0m, in \u001b[0;36mBaseChatModel.astream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(\n\u001b[0;32m    482\u001b[0m         e,\n\u001b[0;32m    483\u001b[0m         response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]] \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;28;01melse\u001b[39;00m []),\n\u001b[0;32m    484\u001b[0m     )\n\u001b[1;32m--> 485\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[0;32m    488\u001b[0m         LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]]),\n\u001b[0;32m    489\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:463\u001b[0m, in \u001b[0;36mBaseChatModel.astream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m generation: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_astream(\n\u001b[0;32m    464\u001b[0m         messages,\n\u001b[0;32m    465\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    466\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    467\u001b[0m     ):\n\u001b[0;32m    468\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m             chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_manager\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:545\u001b[0m, in \u001b[0;36mChatGroq._astream\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[0;32m    544\u001b[0m default_chunk_class: Type[BaseMessageChunk] \u001b[38;5;241m=\u001b[39m AIMessageChunk\n\u001b[1;32m--> 545\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    546\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    547\u001b[0m ):\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    549\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:771\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[1;34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    626\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    627\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m    628\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    772\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    773\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[0;32m    774\u001b[0m             {\n\u001b[0;32m    775\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    776\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    777\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: exclude_domains,\n\u001b[0;32m    778\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    779\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    780\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    781\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_domains,\n\u001b[0;32m    782\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_reasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_reasoning,\n\u001b[0;32m    783\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    784\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    785\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    786\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    787\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    788\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    789\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    790\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    791\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    792\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_format,\n\u001b[0;32m    793\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    794\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m: search_settings,\n\u001b[0;32m    795\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    796\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    797\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    798\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    799\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    800\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    801\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    802\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    803\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    804\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    805\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    806\u001b[0m             },\n\u001b[0;32m    807\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    808\u001b[0m         ),\n\u001b[0;32m    809\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    810\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    811\u001b[0m         ),\n\u001b[0;32m    812\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    813\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    814\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[0;32m    815\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1762\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1750\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1757\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1758\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m   1759\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1760\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1761\u001b[0m     )\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1576\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1573\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[0;32m   1575\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1576\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1578\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1580\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAPIStatusError\u001b[0m: Error code: 413 - {'error': {'message': 'Request too large for model `openai/gpt-oss-120b` in organization `org_01k26dmhxver3bwfg5g7b19q0b` service tier `on_demand` on tokens per minute (TPM): Limit 8000, Requested 8984, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run_pipeline():\n",
    "\n",
    "    result_doc_agent= await parse_agent_executor.ainvoke({'input':\"analyze the  given research paper with the given database retriver tool\"})\n",
    "\n",
    "\n",
    "    summary_context= await summary_agent_executor.ainvoke({\"input\":result_doc_agent[\"output\"]})\n",
    "    \n",
    "    result = await supervisor_agent_executor.ainvoke({\n",
    "        \"input\": f\"\"\"\n",
    "Analyze and explain the following research paper.\n",
    "Use provided tools when appropriate.\n",
    "\n",
    "\"\"\"\n",
    "    ,'context':summary_context['output']})\n",
    "\n",
    "    return summary_context[\"output\"]\n",
    "\n",
    "\n",
    "# Run\n",
    "final_output = await run_pipeline()\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "8de86ed8-27d6-4d7d-95c1-87ff06b0bb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Key takeâ€‘aways from â€œAttention is All You Needâ€**\n",
      "\n",
      "- **Core idea** â€“ Replace recurrent/convolutional layers with a *fully attentionâ€‘based* encoderâ€‘decoder called the **Transformer**.\n",
      "- **Architecture**\n",
      "  - 6 identical encoder/decoder layers (base model; 12 for the big model).\n",
      "  - Encoder layer: multiâ€‘head selfâ€‘attention â†’ feedâ€‘forward network.\n",
      "  - Decoder layer: masked selfâ€‘attention â†’ encoderâ€‘decoder attention â†’ feedâ€‘forward.\n",
      "  - Residual connections + layerâ€‘norm around every subâ€‘layer.\n",
      "- **Attention mechanisms**\n",
      "  - **Scaled Dotâ€‘Product Attention**: `Attention(Q,K,V) = softmax(QKáµ€/âˆšdk) V`.\n",
      "  - **Multiâ€‘head attention**: parallel heads (`h`), each with its own linear projections, concatenated and projected back.\n",
      "  - Masking in the decoder prevents future tokens from influencing current predictions.\n",
      "- **Positional encoding**\n",
      "  - Sinusoidal functions (`sin`, `cos`) provide order information without learnable parameters; learned embeddings perform similarly.\n",
      "- **Training & hyperâ€‘parameters**\n",
      "  - Optimizer: Adam with warmâ€‘up learningâ€‘rate schedule.\n",
      "  - Dropout 0.1, label smoothing.\n",
      "  - Base: `d_model=512`, `h=8`, `d_ff=2048`; Big: `d_model=1024`, `h=16`, `d_ff=4096`.\n",
      "- **Datasets & baselines**\n",
      "  - WMTâ€¯14 Englishâ€‘German (~4.5â€¯M pairs) and Englishâ€‘French (~36â€¯M pairs).\n",
      "  - Compared against LSTMâ€‘based encoderâ€‘decoder, GNMT, etc.\n",
      "- **Results**\n",
      "  - **Base**: BLEU 28.4 (ENâ€‘DE), 41.0 (ENâ€‘FR).\n",
      "  - **Big**: BLEU 28.8 (ENâ€‘DE), 41.4 (ENâ€‘FR).\n",
      "  - Outperforms all prior RNNâ€‘based systems.\n",
      "  - Training 3â€“4Ã— faster due to full parallelism.\n",
      "- **Ablations**\n",
      "  - Reducing key size (`dk`) degrades performance.\n",
      "  - Learned positional embeddings â‰ˆ sinusoidal ones.\n",
      "- **Conclusions**\n",
      "  - Attention alone suffices for sequence transduction.\n",
      "  - Transformer offers superior accuracy, speed, and scalability.\n",
      "  - Opens avenues for larger models and other tasks (e.g., BERT, GPT).\n",
      "\n",
      "**Summary of Contributions**\n",
      "\n",
      "1. First endâ€‘toâ€‘end model built solely on attention.  \n",
      "2. Multiâ€‘head selfâ€‘attention captures diverse dependencies in parallel.  \n",
      "3. Scaled dotâ€‘product attention is efficient and numerically stable.  \n",
      "4. Sinusoidal positional encoding injects order without extra parameters.  \n",
      "5. Sets new stateâ€‘ofâ€‘theâ€‘art BLEU scores while training faster than RNNs.\n"
     ]
    }
   ],
   "source": [
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e689288-750f-448e-a297-cd1f7abf184e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
