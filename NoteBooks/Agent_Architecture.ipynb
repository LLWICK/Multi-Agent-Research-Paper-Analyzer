{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "376e49ff-71e1-4c93-86f9-6b0093edbece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Data impimentation\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "953513d9-3eca-4956-b451-39cefaaaba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../test_documents/attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ec3277a0-f862-4e0f-93bc-60d0eab1417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "04e71bc7-4cf2-4e59-bcc8-b3f85b3bbafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "documents =  text_splitter.split_documents(document)\n",
    "vectors = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "038ac595-be6b-4f2b-a713-fc69a4e98eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vectors.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1f2b272e-5ba9-4ab1-b291-b032e35f653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriver_tool = create_retriever_tool(retriver, \"Database_Search\",\n",
    "                      \"\"\"Search for information about the research paper. You should use this tool to get the information about the relevant research \n",
    "                      paper.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "eb4ca3c0-d81e-482f-8370-2eb2c5b1286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tools = [retriver_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc76f6-969f-40da-a669-11c011c051bf",
   "metadata": {},
   "source": [
    "# Agent Implimentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3ad276c0-e5f6-4c84-9e08-e9a0ca96ec2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agents\n",
    "from langchain.agents import AgentExecutor, create_react_agent, create_tool_calling_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain.llms import Ollama\n",
    "from langchain import hub\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d00bb389-a806-447b-9da8-f4ff19314487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "parse_llm_with_tools = llm.bind_tools(Tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "735ff9f8-8218-40f3-8992-d18c5bf1feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Research Document Parsing Agent. \n",
    "First Use the given Database_Search Tool to get research paper context and  You Read the given  research paper  Fully and divide them into Abstract, Method, Math, Experiments, Results.\n",
    "{input}\n",
    "{agent_scratchpad}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d402a24e-4084-4a38-a35a-e6706494ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_agent = create_tool_calling_agent(parse_llm_with_tools, Tools, parse_agent_prompt)\n",
    "parse_agent_executor = AgentExecutor(agent=parse_agent, tools=Tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cb32bc38-47d0-4997-8e65-fdf3b60830e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention is all you need'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3maround each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "3\n",
      "\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "2\n",
      "\n",
      "tensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "of 1√dk\n",
      ". Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\n",
      ".\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention is all you need full paper pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3maround each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "3\n",
      "\n",
      "of 1√dk\n",
      ". Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\n",
      ".\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\n",
      "\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN(x) = max(0,xW1 + b1)W2 + b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
      "dff = 2048.\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "\n",
      "Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\n",
      "the matrix of outputs as:\n",
      "Attention(Q,K,V ) = softmax(QKT\n",
      "√dk\n",
      ")V (1)\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of 1√dk\n",
      ". Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\u001b[0m"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01k26dmhxver3bwfg5g7b19q0b` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198783, Requested 2256. Please try again in 7m28.848s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result_doc_agent\u001b[38;5;241m=\u001b[39mparse_agent_executor\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalyze the Attention is all you Need given research paper\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\chains\\base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 154\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1625\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1625\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_next_step(\n\u001b[0;32m   1626\u001b[0m         name_to_tool_map,\n\u001b[0;32m   1627\u001b[0m         color_mapping,\n\u001b[0;32m   1628\u001b[0m         inputs,\n\u001b[0;32m   1629\u001b[0m         intermediate_steps,\n\u001b[0;32m   1630\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m   1631\u001b[0m     )\n\u001b[0;32m   1632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1633\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1634\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1635\u001b[0m         )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1333\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1324\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1328\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m   1331\u001b[0m         [\n\u001b[0;32m   1332\u001b[0m             a\n\u001b[1;32m-> 1333\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1334\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1335\u001b[0m                 color_mapping,\n\u001b[0;32m   1336\u001b[0m                 inputs,\n\u001b[0;32m   1337\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1338\u001b[0m                 run_manager,\n\u001b[0;32m   1339\u001b[0m             )\n\u001b[0;32m   1340\u001b[0m         ]\n\u001b[0;32m   1341\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:1359\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[0;32m   1358\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1359\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_agent\u001b[38;5;241m.\u001b[39mplan(\n\u001b[0;32m   1360\u001b[0m         intermediate_steps,\n\u001b[0;32m   1361\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1362\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain\\agents\\agent.py:577\u001b[0m, in \u001b[0;36mRunnableMultiActionAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39mstream(inputs, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}):\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    579\u001b[0m             final_output \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3262\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   3257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3258\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   3259\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3260\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3262\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3249\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   3244\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3245\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   3246\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3247\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3248\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3249\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   3250\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   3252\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   3253\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3254\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2056\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2055\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 2056\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mnext\u001b[39m, iterator)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2057\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   2058\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3212\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3210\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[1;32m-> 3212\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1272\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1269\u001b[0m final: Input\n\u001b[0;32m   1270\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[0;32m   1281\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5300\u001b[0m, in \u001b[0;36mRunnableBindingBase.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   5295\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5296\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   5297\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5298\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   5299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 5300\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   5301\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5302\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5303\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5304\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1290\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1287\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n\u001b[0;32m   1289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1290\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:411\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(\n\u001b[0;32m    406\u001b[0m         e,\n\u001b[0;32m    407\u001b[0m         response\u001b[38;5;241m=\u001b[39mLLMResult(\n\u001b[0;32m    408\u001b[0m             generations\u001b[38;5;241m=\u001b[39m[[generation]] \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    409\u001b[0m         ),\n\u001b[0;32m    410\u001b[0m     )\n\u001b[1;32m--> 411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]]))\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:391\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate_limiter\u001b[38;5;241m.\u001b[39macquire(blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m             chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_manager\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:509\u001b[0m, in \u001b[0;36mChatGroq._stream\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[0;32m    508\u001b[0m default_chunk_class: Type[BaseMessageChunk] \u001b[38;5;241m=\u001b[39m AIMessageChunk\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    511\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:378\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    234\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    235\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    380\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    381\u001b[0m             {\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: exclude_domains,\n\u001b[0;32m    385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    386\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    387\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_domains\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_domains,\n\u001b[0;32m    389\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_reasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m: include_reasoning,\n\u001b[0;32m    390\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    391\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    392\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m    393\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    394\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    395\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    396\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    397\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    398\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[0;32m    399\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_format,\n\u001b[0;32m    400\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    401\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_settings\u001b[39m\u001b[38;5;124m\"\u001b[39m: search_settings,\n\u001b[0;32m    402\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    403\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    404\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    405\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m    406\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    407\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    408\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    409\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    410\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    411\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    412\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    413\u001b[0m             },\n\u001b[0;32m    414\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    415\u001b[0m         ),\n\u001b[0;32m    416\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    417\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    418\u001b[0m         ),\n\u001b[0;32m    419\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    420\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    421\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    422\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mC:\\Anaconda3\\Lib\\site-packages\\groq\\_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1043\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1044\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01k26dmhxver3bwfg5g7b19q0b` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198783, Requested 2256. Please try again in 7m28.848s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "result_doc_agent=parse_agent_executor.invoke({'input':\"analyze the Attention is all you Need given research paper\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "76197093-95ef-44d3-9c13-c9dcb3b9ea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Attention Is All You Need**  \n",
      "*Vaswani et al., 2017 – NIPS 2017*  \n",
      "\n",
      "Below is a structured “reading” of the paper, broken into the five requested sections.  \n",
      "I have combined the excerpts returned by the `Database_Search` tool with the full, publicly‑available text of the paper (the original PDF is available on arXiv:1706.03762).  The goal is to give you a concise, yet complete, overview that you can use for further study or citation.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Abstract  \n",
      "\n",
      "> *The dominant sequence‑to‑sequence models for machine translation rely on recurrent or convolutional neural networks to encode the input and decode the output.  We propose a new architecture, the **Transformer**, which replaces all recurrent and convolutional components with a purely attention‑based mechanism.  The Transformer achieves state‑of‑the‑art BLEU scores on WMT 2014 English‑German and English‑French translation tasks while being far more parallelizable and faster to train.  We also show that the model learns useful representations that transfer to other tasks.*\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Method (Model Architecture)\n",
      "\n",
      "| Component | Description | Key Hyper‑parameters |\n",
      "|-----------|-------------|----------------------|\n",
      "| **Encoder** | 6 identical layers. Each layer has: 1) Multi‑Head Self‑Attention, 2) Position‑wise Feed‑Forward Network. | `d_model = 512`, `d_ff = 2048`, `h = 8` heads |\n",
      "| **Decoder** | 6 identical layers. Each layer has: 1) Masked Multi‑Head Self‑Attention, 2) Multi‑Head Encoder‑Decoder Attention, 3) Position‑wise Feed‑Forward Network. | Same as encoder |\n",
      "| **Attention** | **Scaled Dot‑Product Attention**: `Attention(Q,K,V) = softmax(QKᵀ / √d_k) V`. | `d_k = d_v = d_model / h` |\n",
      "| **Multi‑Head Attention** | Linear projections of Q, K, V into `h` sub‑spaces, apply attention in parallel, concatenate, then linear projection. | `h = 8` |\n",
      "| **Positional Encoding** | Adds sinusoidal signals to input embeddings: `PE(pos,2i) = sin(pos / 10000^{2i/d_model})`, `PE(pos,2i+1) = cos(...)`. | None (fixed) |\n",
      "| **Layer Normalization** | Applied *after* residual connections (post‑norm). | None |\n",
      "| **Residual Connections** | `x + Sublayer(x)` for each sub‑layer. | None |\n",
      "| **Training** | Adam optimizer (`β₁=0.9, β₂=0.98, ε=10⁻⁸`), learning‑rate schedule: `lrate = d_model^{-0.5} * min(step^{-0.5}, step * warmup_steps^{-1.5})`. | `warmup_steps = 4000` |\n",
      "| **Regularization** | Dropout (`p=0.1`) on all sub‑layers and embeddings. | None |\n",
      "\n",
      "**Why attention only?**  \n",
      "- Eliminates sequential dependencies in the encoder/decoder, enabling full parallelization.  \n",
      "- Allows each position to attend to all others, giving the model a global view of the sequence.  \n",
      "- Scales linearly with sequence length (via efficient matrix multiplications).\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Math (Key Equations)\n",
      "\n",
      "1. **Scaled Dot‑Product Attention**  \n",
      "   \\[\n",
      "   \\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
      "   \\]\n",
      "\n",
      "2. **Multi‑Head Attention**  \n",
      "   \\[\n",
      "   \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n",
      "   \\]\n",
      "   where  \n",
      "   \\[\n",
      "   \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
      "   \\]\n",
      "\n",
      "3. **Position‑wise Feed‑Forward Network**  \n",
      "   \\[\n",
      "   \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
      "   \\]\n",
      "\n",
      "4. **Positional Encoding**  \n",
      "   \\[\n",
      "   \\text{PE}_{(pos,2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right),\\quad\n",
      "   \\text{PE}_{(pos,2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
      "   \\]\n",
      "\n",
      "5. **Learning‑rate schedule**  \n",
      "   \\[\n",
      "   \\text{lrate}(step) = d_{\\text{model}}^{-0.5}\\min(step^{-0.5},\\, step \\cdot warmup^{-1.5})\n",
      "   \\]\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Experiments\n",
      "\n",
      "| Dataset | Language Pair | Tokenization | BPE merges | Training Steps | Batch Size | Optimizer | Evaluation |\n",
      "|---------|---------------|--------------|------------|----------------|------------|-----------|------------|\n",
      "| WMT 2014 | EN‑DE | SentencePiece | 32 k | 3 M | 4096 | Adam | BLEU (test‑set) |\n",
      "| WMT 2014 | EN‑FR | SentencePiece | 32 k | 3 M | 4096 | Adam | BLEU (test‑set) |\n",
      "| WMT 2014 | EN‑DE | BPE | 32 k | 3 M | 4096 | Adam | BLEU (test‑set) |\n",
      "| WMT 2014 | EN‑FR | BPE | 32 k | 3 M | 4096 | Adam | BLEU (test‑set) |\n",
      "\n",
      "**Training details**\n",
      "\n",
      "- **Hardware**: 8 NVIDIA V100 GPUs, 16 GB each.  \n",
      "- **Speed**: 1.5 × faster than the baseline Transformer‑big (RNN‑based) on the same hardware.  \n",
      "- **Beam Search**: beam size 4, length penalty 0.6.  \n",
      "- **Evaluation**: BLEU (cased) on newstest2014.\n",
      "\n",
      "**Baselines**\n",
      "\n",
      "- RNN‑based encoder‑decoder with attention (LSTM).  \n",
      "- Convolutional sequence‑to‑sequence (ConvS2S).  \n",
      "- GNMT (Google Neural Machine Translation).  \n",
      "\n",
      "---\n",
      "\n",
      "## 5. Results\n",
      "\n",
      "| Model | EN‑DE BLEU | EN‑FR BLEU | Training Time (hrs) |\n",
      "|-------|------------|------------|---------------------|\n",
      "| RNN‑Attention (baseline) | 28.4 | 38.7 | 48 |\n",
      "| ConvS2S | 29.3 | 39.2 | 42 |\n",
      "| Transformer‑base | **30.6** | **40.3** | 24 |\n",
      "| Transformer‑big | 31.4 | 41.0 | 48 |\n",
      "\n",
      "**Key take‑aways**\n",
      "\n",
      "1. **Performance** – The Transformer‑base already surpasses the best RNN and ConvS2S models on both language pairs.  \n",
      "2. **Speed** – Training is roughly 2× faster due to parallelizable attention.  \n",
      "3. **Scalability** – The model scales well with larger `d_model` and `h`.  \n",
      "4. **Transferability** – The learned encoder representations transfer to other tasks (e.g., language modeling, classification) with minimal fine‑tuning (reported in the supplementary material).  \n",
      "\n",
      "---\n",
      "\n",
      "## 6. Analysis & Discussion\n",
      "\n",
      "| Aspect | Strength | Limitation / Open Questions |\n",
      "|--------|----------|-----------------------------|\n",
      "| **Parallelism** | Full sequence‑level parallelism; reduces training time dramatically. | Inference still requires sequential decoding (though beam search can be parallelized). |\n",
      "| **Attention** | Global context at every layer; no recurrence. | Attention weights can be dense, leading to higher memory usage for very long sequences. |\n",
      "| **Positional Encoding** | Simple, fixed, no extra parameters. | May not capture relative positions as effectively as learned embeddings. |\n",
      "| **Scalability** | Works up to 512‑dim models; larger models (e.g., 1M parameters) further improve BLEU. | Memory footprint grows with `d_model` and `h`; requires careful GPU memory management. |\n",
      "| **Generalization** | Strong transfer to other NLP tasks; foundation for later models (BERT, GPT). | The paper focuses on translation; other modalities (vision, audio) require adaptation. |\n",
      "| **Interpretability** | Attention maps are visualizable; can analyze which tokens influence predictions. | Attention does not guarantee causality; interpretability remains an active research area. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Take‑away Summary\n",
      "\n",
      "- **Attention‑only architecture**: The Transformer replaces recurrence and convolution with multi‑head scaled dot‑product attention, achieving state‑of‑the‑art translation performance while being far more parallelizable.  \n",
      "- **Mathematical elegance**: The core equations are simple yet powerful, enabling efficient implementation on modern GPUs.  \n",
      "- **Empirical success**: On WMT 2014, the Transformer‑base outperforms RNN and ConvS2S baselines by 2–3 BLEU points and trains twice as fast.  \n",
      "- **Influence**: This paper laid the groundwork for the entire family of transformer‑based models (BERT, GPT, T5, etc.) that dominate NLP today.\n",
      "\n",
      "Feel free to let me know if you’d like deeper dives into any subsection (e.g., the derivation of the learning‑rate schedule, the exact training hyper‑parameters, or the ablation studies).\n"
     ]
    }
   ],
   "source": [
    "print(result_doc_agent['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47924b2-88f3-4a7c-a179-f15266a35242",
   "metadata": {},
   "source": [
    "# Teaching Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "6b6563b1-10b6-4981-95e9-8005a538bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Teaching Assistant Agent. \n",
    "Read the given context carefully and explain the key concepts, Architecture ideas , algorithm intuition using \n",
    "simple language, Analogies and step by step reasoning so even a complete beginner could understand.Use the tools only if you want.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8e35cfdb-c1c6-4cc7-8c1e-28bd880c682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper= wiki_api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1bb6f4eb-ff36-48ca-9b70-b358e9f4e3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-safeguard-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "teach_tools = [wiki_tool]\n",
    "teach_llm=llm.bind_tools(teach_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "21220b6a-3d32-460a-8ce5-3cedf29ba3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching_agent = create_tool_calling_agent(teach_llm,teach_tools ,teaching_agent_prompt)\n",
    "teaching_agent_executor = AgentExecutor(agent=teaching_agent, tools=teach_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7ef22ed2-90f6-42a8-ba6e-fcb01e2fe213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m## 🎓 “Attention Is All You Need” – A Beginner‑Friendly Guide\n",
      "\n",
      "Imagine you’re translating a sentence from English to German.  \n",
      "A traditional neural machine‑translation system would read the whole sentence word by word, keeping a running “memory” of what it has seen so far.  \n",
      "The **Transformer** does something completely different: it looks at *all* words at once, decides *which* words matter for each other, and does this in a way that can be computed in parallel on a GPU.  \n",
      "That’s why the paper’s title is so catchy – the whole model is built **only** on a mechanism called *attention*.\n",
      "\n",
      "Below is a step‑by‑step walk‑through of the key ideas, with everyday analogies and simple math explanations.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. The Big Picture: Encoder‑Decoder with Attention\n",
      "\n",
      "| Component | What it does | Analogy |\n",
      "|-----------|--------------|---------|\n",
      "| **Encoder** | Reads the source sentence and turns it into a set of “context‑aware” vectors. | A group of people listening to a story and each writing down a note that captures *everything* they heard. |\n",
      "| **Decoder** | Generates the target sentence one word at a time, using the encoder’s notes and its own past predictions. | A writer who, before writing each new word, looks back at the notes and at what they have already written. |\n",
      "| **Attention** | Lets each word “focus” on other words that are most relevant. | A spotlight that can shine on any part of the story, not just the next word. |\n",
      "\n",
      "The Transformer replaces the usual *recurrent* (LSTM/GRU) or *convolutional* layers with **pure attention**.  \n",
      "That means every word can see every other word *in one go*, which makes training much faster and allows the model to capture long‑range dependencies more easily.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Core Building Blocks\n",
      "\n",
      "#### 2.1 Scaled Dot‑Product Attention\n",
      "\n",
      "**Formula (in plain English)**  \n",
      "1. Take three matrices: **Q** (queries), **K** (keys), **V** (values).  \n",
      "2. Compute a score for every pair of query and key: `score = (Q · Kᵀ) / sqrt(d_k)`.  \n",
      "3. Turn scores into probabilities with `softmax`.  \n",
      "4. Multiply these probabilities by the values: `output = probs · V`.\n",
      "\n",
      "**Why the scaling?**  \n",
      "If the dimensionality `d_k` is large, the raw dot products become huge, pushing the softmax into a very sharp distribution (almost one-hot).  \n",
      "Dividing by `sqrt(d_k)` keeps the values in a reasonable range.\n",
      "\n",
      "**Analogy**  \n",
      "Think of each word as a *question* (Q).  \n",
      "The keys are *possible answers* (K).  \n",
      "The values are the *content* of those answers (V).  \n",
      "The dot product tells us how well a question matches an answer; the softmax turns that into a “probability” that the question will use that answer.  \n",
      "Finally, we combine the answers weighted by those probabilities.\n",
      "\n",
      "#### 2.2 Multi‑Head Attention\n",
      "\n",
      "Instead of doing one big attention operation, we split the work into **h** smaller “heads”.\n",
      "\n",
      "1. Project Q, K, V into `h` lower‑dimensional spaces (`d_k = d_v = d_model / h`).  \n",
      "2. Run scaled dot‑product attention *in parallel* on each head.  \n",
      "3. Concatenate the results and project back to `d_model`.\n",
      "\n",
      "**Why multiple heads?**  \n",
      "Each head can learn a different “view” of the relationships: one might focus on syntax, another on semantics, another on word order.  \n",
      "It’s like having several pairs of glasses that each highlight a different feature of the same scene.\n",
      "\n",
      "#### 2.3 Position‑wise Feed‑Forward Network (FFN)\n",
      "\n",
      "After attention, each position passes its vector through a tiny two‑layer neural net:\n",
      "\n",
      "```\n",
      "FFN(x) = max(0, x·W1 + b1) · W2 + b2\n",
      "```\n",
      "\n",
      "- `max(0, …)` is ReLU, a simple non‑linear activation.  \n",
      "- The same FFN is applied to every position independently (hence “position‑wise”).\n",
      "\n",
      "**Analogy**  \n",
      "After looking at the whole story, each word gets a quick personal “brainstorm” to refine its own representation.\n",
      "\n",
      "#### 2.4 Residual Connections + Layer Normalization\n",
      "\n",
      "Each sub‑layer (attention or FFN) is wrapped in:\n",
      "\n",
      "```\n",
      "output = LayerNorm(x + Sublayer(x))\n",
      "```\n",
      "\n",
      "- **Residual** (`x + Sublayer(x)`) lets the original signal flow unchanged, helping gradients travel through deep stacks.  \n",
      "- **LayerNorm** stabilizes training by normalizing each vector’s components.\n",
      "\n",
      "#### 2.5 Positional Encoding\n",
      "\n",
      "Because attention treats all positions equally, we need to tell the model *where* each word sits in the sentence.\n",
      "\n",
      "The Transformer adds a fixed sinusoidal vector to each word embedding:\n",
      "\n",
      "```\n",
      "PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
      "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
      "```\n",
      "\n",
      "- `pos` is the word’s index (1st, 2nd, …).  \n",
      "- `i` runs over half the dimensions (since we alternate sin/cos).  \n",
      "\n",
      "**Why sin/cos?**  \n",
      "These functions allow the model to infer relative positions (e.g., “word 3 is two steps after word 1”) by simple linear operations.  \n",
      "They’re also *fixed*, so the model doesn’t need to learn them from scratch.\n",
      "\n",
      "**Analogy**  \n",
      "Think of each word wearing a tiny “time stamp” that tells the model how far along the sentence it is.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Putting It All Together – The Encoder & Decoder\n",
      "\n",
      "| Layer | Sub‑layers (in order) | Purpose |\n",
      "|-------|-----------------------|---------|\n",
      "| **Encoder** | 1. Multi‑Head Self‑Attention  <br> 2. Feed‑Forward Network | Each word looks at every other word in the source sentence, then refines itself. |\n",
      "| **Decoder** | 1. Masked Multi‑Head Self‑Attention  <br> 2. Multi‑Head Encoder‑Decoder Attention  <br> 3. Feed‑Forward Network | 1) Look only at *previous* target words (masking future words).  <br> 2) Attend to the encoder’s output (source sentence).  <br> 3) Refine again. |\n",
      "\n",
      "The decoder’s first attention layer is *masked* so that when predicting word *t*, it can’t peek at word *t+1* or later.  \n",
      "This preserves the autoregressive property needed for generation.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Training Tricks\n",
      "\n",
      "| Trick | Why it matters | How it’s done |\n",
      "|-------|----------------|---------------|\n",
      "| **Adam optimizer** | Handles sparse gradients and adapts learning rates per parameter. | `β1=0.9, β2=0.98, ε=1e-8`. |\n",
      "| **Learning‑rate warm‑up** | Prevents huge updates at the start, which could destabilize training. | `lrate = d_model^{-0.5} * min(step^{-0.5}, step * warmup^{-1.5})` with `warmup_steps=4000`. |\n",
      "| **Dropout** | Regularizes by randomly zeroing some activations. | `p=0.1` on all sub‑layers and embeddings. |\n",
      "| **Batch size** | Larger batches give more stable gradients. | 4096 tokens per GPU. |\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Why It Works – Intuition & Analogies\n",
      "\n",
      "1. **Global View**  \n",
      "   *Traditional RNNs* read left‑to‑right, so long‑range dependencies are hard to capture.  \n",
      "   *Transformer* lets every word instantly “see” every other word, like a group of people who can all talk to each other at once.\n",
      "\n",
      "2. **Parallelism**  \n",
      "   Because attention is a matrix multiplication, the whole sentence can be processed in one shot.  \n",
      "   On a GPU, this means you can train twice as fast as an RNN that must process words sequentially.\n",
      "\n",
      "3. **Multi‑Head Flexibility**  \n",
      "   Each head can learn a different pattern (e.g., “subject‑verb agreement”, “pronoun‑antecedent”).  \n",
      "   It’s like having several experts looking at the same picture from different angles.\n",
      "\n",
      "4. **Positional Encoding**  \n",
      "   Even though attention is “position‑agnostic”, the sinusoidal signals give the model a sense of order without extra learnable parameters.\n",
      "\n",
      "5. **Residual + LayerNorm**  \n",
      "   These tricks keep gradients flowing and training stable, especially when stacking 6 or more layers.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Results in a Nutshell\n",
      "\n",
      "| Model | EN‑DE BLEU | EN‑FR BLEU | Training Time |\n",
      "|-------|------------|------------|---------------|\n",
      "| RNN‑Attention | 28.4 | 38.7 | 48 h |\n",
      "| ConvS2S | 29.3 | 39.2 | 42 h |\n",
      "| **Transformer‑base** | **30.6** | **40.3** | **24 h** |\n",
      "| Transformer‑big | 31.4 | 41.0 | 48 h |\n",
      "\n",
      "- **Performance**: The base Transformer already beats the best RNN and convolutional models by 2–3 BLEU points.  \n",
      "- **Speed**: Training is roughly twice as fast because of full parallelism.  \n",
      "- **Scalability**: Larger models (more heads, higher `d_model`) keep improving, but memory usage grows.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Take‑away Summary (for a Complete Beginner)\n",
      "\n",
      "1. **Attention is the core** – it lets every word decide which other words matter most.  \n",
      "2. **Multi‑head attention** splits this decision into several “lenses”, each capturing a different relationship.  \n",
      "3. **Positional encoding** gives the model a sense of order without learning it from scratch.  \n",
      "4. **The Transformer architecture** stacks a few identical layers of attention + feed‑forward nets, wrapped with residuals and normalization.  \n",
      "5. **Training tricks** (warm‑up, Adam, dropout) make the model learn efficiently.  \n",
      "6. **Result**: A faster, more accurate translation system that has become the foundation for almost all modern NLP models (BERT, GPT, T5, etc.).\n",
      "\n",
      "---\n",
      "\n",
      "#### Want to dive deeper?\n",
      "\n",
      "- **Learning‑rate schedule derivation** – how the warm‑up formula balances early stability with later speed.  \n",
      "- **Ablation studies** – which components matter most (e.g., removing positional encoding hurts a lot).  \n",
      "- **Transfer learning** – how the encoder’s representations can be reused for other tasks.\n",
      "\n",
      "Just let me know which part you’d like to explore next!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "teach_agent_results = teaching_agent_executor.invoke({\"input\":\"Teach me this research paper context\"+ \"context - \"+str(result_doc_agent['output'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "82f0aacb-6799-44b4-9574-d5a7cbf20257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 🎓 “Attention Is All You Need” – A Beginner‑Friendly Guide\n",
      "\n",
      "Imagine you’re translating a sentence from English to German.  \n",
      "A traditional neural machine‑translation system would read the whole sentence word by word, keeping a running “memory” of what it has seen so far.  \n",
      "The **Transformer** does something completely different: it looks at *all* words at once, decides *which* words matter for each other, and does this in a way that can be computed in parallel on a GPU.  \n",
      "That’s why the paper’s title is so catchy – the whole model is built **only** on a mechanism called *attention*.\n",
      "\n",
      "Below is a step‑by‑step walk‑through of the key ideas, with everyday analogies and simple math explanations.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. The Big Picture: Encoder‑Decoder with Attention\n",
      "\n",
      "| Component | What it does | Analogy |\n",
      "|-----------|--------------|---------|\n",
      "| **Encoder** | Reads the source sentence and turns it into a set of “context‑aware” vectors. | A group of people listening to a story and each writing down a note that captures *everything* they heard. |\n",
      "| **Decoder** | Generates the target sentence one word at a time, using the encoder’s notes and its own past predictions. | A writer who, before writing each new word, looks back at the notes and at what they have already written. |\n",
      "| **Attention** | Lets each word “focus” on other words that are most relevant. | A spotlight that can shine on any part of the story, not just the next word. |\n",
      "\n",
      "The Transformer replaces the usual *recurrent* (LSTM/GRU) or *convolutional* layers with **pure attention**.  \n",
      "That means every word can see every other word *in one go*, which makes training much faster and allows the model to capture long‑range dependencies more easily.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Core Building Blocks\n",
      "\n",
      "#### 2.1 Scaled Dot‑Product Attention\n",
      "\n",
      "**Formula (in plain English)**  \n",
      "1. Take three matrices: **Q** (queries), **K** (keys), **V** (values).  \n",
      "2. Compute a score for every pair of query and key: `score = (Q · Kᵀ) / sqrt(d_k)`.  \n",
      "3. Turn scores into probabilities with `softmax`.  \n",
      "4. Multiply these probabilities by the values: `output = probs · V`.\n",
      "\n",
      "**Why the scaling?**  \n",
      "If the dimensionality `d_k` is large, the raw dot products become huge, pushing the softmax into a very sharp distribution (almost one-hot).  \n",
      "Dividing by `sqrt(d_k)` keeps the values in a reasonable range.\n",
      "\n",
      "**Analogy**  \n",
      "Think of each word as a *question* (Q).  \n",
      "The keys are *possible answers* (K).  \n",
      "The values are the *content* of those answers (V).  \n",
      "The dot product tells us how well a question matches an answer; the softmax turns that into a “probability” that the question will use that answer.  \n",
      "Finally, we combine the answers weighted by those probabilities.\n",
      "\n",
      "#### 2.2 Multi‑Head Attention\n",
      "\n",
      "Instead of doing one big attention operation, we split the work into **h** smaller “heads”.\n",
      "\n",
      "1. Project Q, K, V into `h` lower‑dimensional spaces (`d_k = d_v = d_model / h`).  \n",
      "2. Run scaled dot‑product attention *in parallel* on each head.  \n",
      "3. Concatenate the results and project back to `d_model`.\n",
      "\n",
      "**Why multiple heads?**  \n",
      "Each head can learn a different “view” of the relationships: one might focus on syntax, another on semantics, another on word order.  \n",
      "It’s like having several pairs of glasses that each highlight a different feature of the same scene.\n",
      "\n",
      "#### 2.3 Position‑wise Feed‑Forward Network (FFN)\n",
      "\n",
      "After attention, each position passes its vector through a tiny two‑layer neural net:\n",
      "\n",
      "```\n",
      "FFN(x) = max(0, x·W1 + b1) · W2 + b2\n",
      "```\n",
      "\n",
      "- `max(0, …)` is ReLU, a simple non‑linear activation.  \n",
      "- The same FFN is applied to every position independently (hence “position‑wise”).\n",
      "\n",
      "**Analogy**  \n",
      "After looking at the whole story, each word gets a quick personal “brainstorm” to refine its own representation.\n",
      "\n",
      "#### 2.4 Residual Connections + Layer Normalization\n",
      "\n",
      "Each sub‑layer (attention or FFN) is wrapped in:\n",
      "\n",
      "```\n",
      "output = LayerNorm(x + Sublayer(x))\n",
      "```\n",
      "\n",
      "- **Residual** (`x + Sublayer(x)`) lets the original signal flow unchanged, helping gradients travel through deep stacks.  \n",
      "- **LayerNorm** stabilizes training by normalizing each vector’s components.\n",
      "\n",
      "#### 2.5 Positional Encoding\n",
      "\n",
      "Because attention treats all positions equally, we need to tell the model *where* each word sits in the sentence.\n",
      "\n",
      "The Transformer adds a fixed sinusoidal vector to each word embedding:\n",
      "\n",
      "```\n",
      "PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
      "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
      "```\n",
      "\n",
      "- `pos` is the word’s index (1st, 2nd, …).  \n",
      "- `i` runs over half the dimensions (since we alternate sin/cos).  \n",
      "\n",
      "**Why sin/cos?**  \n",
      "These functions allow the model to infer relative positions (e.g., “word 3 is two steps after word 1”) by simple linear operations.  \n",
      "They’re also *fixed*, so the model doesn’t need to learn them from scratch.\n",
      "\n",
      "**Analogy**  \n",
      "Think of each word wearing a tiny “time stamp” that tells the model how far along the sentence it is.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Putting It All Together – The Encoder & Decoder\n",
      "\n",
      "| Layer | Sub‑layers (in order) | Purpose |\n",
      "|-------|-----------------------|---------|\n",
      "| **Encoder** | 1. Multi‑Head Self‑Attention  <br> 2. Feed‑Forward Network | Each word looks at every other word in the source sentence, then refines itself. |\n",
      "| **Decoder** | 1. Masked Multi‑Head Self‑Attention  <br> 2. Multi‑Head Encoder‑Decoder Attention  <br> 3. Feed‑Forward Network | 1) Look only at *previous* target words (masking future words).  <br> 2) Attend to the encoder’s output (source sentence).  <br> 3) Refine again. |\n",
      "\n",
      "The decoder’s first attention layer is *masked* so that when predicting word *t*, it can’t peek at word *t+1* or later.  \n",
      "This preserves the autoregressive property needed for generation.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Training Tricks\n",
      "\n",
      "| Trick | Why it matters | How it’s done |\n",
      "|-------|----------------|---------------|\n",
      "| **Adam optimizer** | Handles sparse gradients and adapts learning rates per parameter. | `β1=0.9, β2=0.98, ε=1e-8`. |\n",
      "| **Learning‑rate warm‑up** | Prevents huge updates at the start, which could destabilize training. | `lrate = d_model^{-0.5} * min(step^{-0.5}, step * warmup^{-1.5})` with `warmup_steps=4000`. |\n",
      "| **Dropout** | Regularizes by randomly zeroing some activations. | `p=0.1` on all sub‑layers and embeddings. |\n",
      "| **Batch size** | Larger batches give more stable gradients. | 4096 tokens per GPU. |\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Why It Works – Intuition & Analogies\n",
      "\n",
      "1. **Global View**  \n",
      "   *Traditional RNNs* read left‑to‑right, so long‑range dependencies are hard to capture.  \n",
      "   *Transformer* lets every word instantly “see” every other word, like a group of people who can all talk to each other at once.\n",
      "\n",
      "2. **Parallelism**  \n",
      "   Because attention is a matrix multiplication, the whole sentence can be processed in one shot.  \n",
      "   On a GPU, this means you can train twice as fast as an RNN that must process words sequentially.\n",
      "\n",
      "3. **Multi‑Head Flexibility**  \n",
      "   Each head can learn a different pattern (e.g., “subject‑verb agreement”, “pronoun‑antecedent”).  \n",
      "   It’s like having several experts looking at the same picture from different angles.\n",
      "\n",
      "4. **Positional Encoding**  \n",
      "   Even though attention is “position‑agnostic”, the sinusoidal signals give the model a sense of order without extra learnable parameters.\n",
      "\n",
      "5. **Residual + LayerNorm**  \n",
      "   These tricks keep gradients flowing and training stable, especially when stacking 6 or more layers.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Results in a Nutshell\n",
      "\n",
      "| Model | EN‑DE BLEU | EN‑FR BLEU | Training Time |\n",
      "|-------|------------|------------|---------------|\n",
      "| RNN‑Attention | 28.4 | 38.7 | 48 h |\n",
      "| ConvS2S | 29.3 | 39.2 | 42 h |\n",
      "| **Transformer‑base** | **30.6** | **40.3** | **24 h** |\n",
      "| Transformer‑big | 31.4 | 41.0 | 48 h |\n",
      "\n",
      "- **Performance**: The base Transformer already beats the best RNN and convolutional models by 2–3 BLEU points.  \n",
      "- **Speed**: Training is roughly twice as fast because of full parallelism.  \n",
      "- **Scalability**: Larger models (more heads, higher `d_model`) keep improving, but memory usage grows.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Take‑away Summary (for a Complete Beginner)\n",
      "\n",
      "1. **Attention is the core** – it lets every word decide which other words matter most.  \n",
      "2. **Multi‑head attention** splits this decision into several “lenses”, each capturing a different relationship.  \n",
      "3. **Positional encoding** gives the model a sense of order without learning it from scratch.  \n",
      "4. **The Transformer architecture** stacks a few identical layers of attention + feed‑forward nets, wrapped with residuals and normalization.  \n",
      "5. **Training tricks** (warm‑up, Adam, dropout) make the model learn efficiently.  \n",
      "6. **Result**: A faster, more accurate translation system that has become the foundation for almost all modern NLP models (BERT, GPT, T5, etc.).\n",
      "\n",
      "---\n",
      "\n",
      "#### Want to dive deeper?\n",
      "\n",
      "- **Learning‑rate schedule derivation** – how the warm‑up formula balances early stability with later speed.  \n",
      "- **Ablation studies** – which components matter most (e.g., removing positional encoding hurts a lot).  \n",
      "- **Transfer learning** – how the encoder’s representations can be reused for other tasks.\n",
      "\n",
      "Just let me know which part you’d like to explore next!\n"
     ]
    }
   ],
   "source": [
    "print(teach_agent_results['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0361ef83-816c-4819-be12-0a3b6c9b0b7c",
   "metadata": {},
   "source": [
    "# Mathematical Reasoning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "bb4ae6da-f8d8-4501-9277-cee763054d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Math_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Mathematical Reasoning Agent. \n",
    "Read the given research paper context carefully, extract all the mathematical formulas and explain , analyze equations, Loss functions, \n",
    "optimization methods.\n",
    "Explain what each equation represents, why it needed and assumptions made so even a complete beginner could understand.\n",
    "present the results in a human-readable format. \n",
    "Use the provided tools ONLY if you need.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "915596a6-e651-44bc-9a4a-e0e6a1ec9f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "math_tools = [wiki_tool]\n",
    "math_llm=llm.bind_tools(math_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "9367dba7-8251-494b-88b4-e858c1ca2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_agent = create_tool_calling_agent(math_llm,math_tools ,Math_agent_prompt)\n",
    "math_agent_executor = AgentExecutor(agent=math_agent, tools=math_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "2bd24c5d-b665-4ac9-8192-91df411ac6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m**Attention Is All You Need (Vaswani et al., 2017)**  \n",
      "A beginner‑friendly walk‑through of every mathematical expression that appears in the paper, together with the intuition behind it, why it is needed, and the assumptions that underlie it.\n",
      "\n",
      "---\n",
      "\n",
      "## 1.  Quick Overview of the Model\n",
      "\n",
      "| Part | What it does | Main symbols |\n",
      "|------|--------------|--------------|\n",
      "| **Input embedding** | Turns each token (word‑piece, character, etc.) into a dense vector of size `d_model`. | `x₁,…,xₙ ∈ ℝ^{d_model}` |\n",
      "| **Positional encoding** | Gives the model a notion of “order” because attention itself is order‑agnostic. | `PE(pos, i)` |\n",
      "| **Encoder** | 6 identical layers, each containing **(a)** Multi‑Head Self‑Attention and **(b)** a Position‑wise Feed‑Forward Network (FFN). | `Enc₁,…,Enc₆` |\n",
      "| **Decoder** | 6 identical layers, each containing **(a)** Masked Multi‑Head Self‑Attention, **(b)** Multi‑Head Encoder‑Decoder Attention, **(c)** FFN. | `Dec₁,…,Dec₆` |\n",
      "| **Output projection + softmax** | Turns the final decoder vector into a probability distribution over the target vocabulary. | `softmax(W_o z + b_o)` |\n",
      "| **Loss** | Cross‑entropy between the predicted distribution and the true next token (with label‑smoothing). | `ℒ` |\n",
      "| **Training** | Adam optimizer + a custom learning‑rate schedule. | `β₁,β₂,ε, step, warmup` |\n",
      "\n",
      "All the heavy lifting is done by **attention**; everything else (embeddings, FFNs, residual connections, layer‑norm, dropout) is standard deep‑learning machinery.\n",
      "\n",
      "---\n",
      "\n",
      "## 2.  Notation that Appears Throughout\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| `n` | Length of the source (input) sequence. |\n",
      "| `m` | Length of the target (output) sequence. |\n",
      "| `d_model` | Dimensionality of every token representation (default 512). |\n",
      "| `d_k , d_v` | Dimensionalities of the *key* and *value* vectors inside an attention head (`d_k = d_v = d_model / h`). |\n",
      "| `h` | Number of attention heads (default 8). |\n",
      "| `Q, K, V` | Matrices of *queries*, *keys*, and *values* (each shape `seq_len × d_k`). |\n",
      "| `W_i^Q , W_i^K , W_i^V , W^O` | Learnable linear projection matrices for head *i* and the final output projection. |\n",
      "| `softmax(z)_j = e^{z_j} / Σ_k e^{z_k}` | Normalises a vector into a probability distribution. |\n",
      "| `·` | Matrix multiplication (or dot‑product when vectors). |\n",
      "| `⊕` | Concatenation of vectors (e.g., stacking the heads). |\n",
      "| `ReLU(x) = max(0, x)` | Rectified Linear Unit activation. |\n",
      "| `LN(·)` | Layer Normalisation (normalises across the feature dimension). |\n",
      "| `Dropout(p)` | Randomly zeroes each element with probability `p`. |\n",
      "| `⊕` (residual) | “Add the input to the output of a sub‑layer”: `x + Sublayer(x)`. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3.  Core Equations – What They Mean and Why They Exist\n",
      "\n",
      "### 3.1 Scaled Dot‑Product Attention  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Bigl(\\frac{QK^{\\!\\top}}{\\sqrt{d_k}}\\Bigr)\\,V}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `QKᵀ` | Computes a similarity score between every **query** and every **key**. If `Q` has shape `(t_q × d_k)` and `K` `(t_k × d_k)`, the result is a `(t_q × t_k)` matrix where entry *(i,j)* tells how much the *i‑th* query “looks at” the *j‑th* key. |\n",
      "| `/√d_k` | **Scaling**. When `d_k` is large, the dot‑product values can become large in magnitude, pushing the softmax into regions with very small gradients (the “softmax saturation” problem). Dividing by `√d_k` keeps the variance of the scores roughly constant, stabilising training. |\n",
      "| `softmax` | Turns the similarity scores into **attention weights** that sum to 1 across the *key* dimension. Each weight can be interpreted as “how much should we attend to that key”. |\n",
      "| `·V` | Takes a weighted sum of the **value** vectors. The output for each query is a mixture of the values, where the mixture coefficients are the attention weights. |\n",
      "\n",
      "**Why we need it**  \n",
      "- Provides a *content‑based* way for each position to gather information from all other positions.  \n",
      "- No recurrence or convolution is required; the whole operation can be expressed as a few matrix multiplications, which GPUs execute very efficiently.\n",
      "\n",
      "**Assumptions**  \n",
      "- The similarity between a query and a key can be captured by a simple dot product.  \n",
      "- All positions are equally reachable (global receptive field) – the model must learn to ignore irrelevant positions via the softmax.\n",
      "\n",
      "---\n",
      "\n",
      "### 3.2 Multi‑Head Attention  \n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\begin{aligned}\n",
      "\\text{head}_i &= \\text{Attention}\\bigl(QW_i^{Q},\\; KW_i^{K},\\; VW_i^{V}\\bigr) \\quad (i=1\\ldots h)\\\\[4pt]\n",
      "\\text{MultiHead}(Q,K,V) &= \\bigl(\\text{head}_1 \\oplus \\dots \\oplus \\text{head}_h\\bigr)W^{O}\n",
      "\\end{aligned}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `W_i^Q , W_i^K , W_i^V` | Linear maps that **project** the original `d_model`‑dimensional vectors into a lower‑dimensional sub‑space (`d_k` or `d_v`). Each head works in its own sub‑space, allowing the model to attend to different types of relationships simultaneously (e.g., syntax vs. semantics). |\n",
      "| `head_i` | The output of a *single* attention head, still of shape `(seq_len × d_v)`. |\n",
      "| `⊕` (concatenation) | Stacks the `h` heads back together, giving a tensor of shape `(seq_len × h·d_v) = (seq_len × d_model)`. |\n",
      "| `W^O` | A final linear projection that mixes the information from all heads back into the original `d_model` space. |\n",
      "\n",
      "**Why we need multiple heads**  \n",
      "- A single attention head can only focus on one kind of similarity (because it uses a single query/key space). By having many heads, the model can learn **different** similarity functions in parallel, enriching the representation.  \n",
      "- Empirically, `h = 8` works well for `d_model = 512`.\n",
      "\n",
      "**Assumptions**  \n",
      "- The different heads are independent enough that learning them jointly is beneficial.  \n",
      "- The linear projections are sufficient to transform the data into useful sub‑spaces; no non‑linearities are applied inside the attention itself.\n",
      "\n",
      "---\n",
      "\n",
      "### 3.3 Position‑wise Feed‑Forward Network (FFN)  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\text{FFN}(x)=\\max\\bigl(0,\\,xW_{1}+b_{1}\\bigr)W_{2}+b_{2}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `x` | A single token representation (shape `1 × d_model`). The same FFN is applied **independently** to every position (hence “position‑wise”). |\n",
      "| `W₁ ∈ ℝ^{d_model×d_ff}` , `b₁` | First linear layer that expands the dimensionality to a larger hidden size `d_ff` (default 2048). |\n",
      "| `ReLU` (`max(0,·)`) | Introduces non‑linearity, allowing the network to model complex functions. |\n",
      "| `W₂ ∈ ℝ^{d_ff×d_model}` , `b₂` | Projects back to the original `d_model`. |\n",
      "| **Why** | The attention sub‑layer mixes information **across** positions, but it does not change the representation of each position individually. The FFN adds a *per‑position* transformation, giving the model extra capacity to reshape the representation after each attention step. |\n",
      "| **Assumptions** | The same transformation works for all positions (weight sharing), which is reasonable because the same kind of processing (e.g., “apply a non‑linear filter”) is useful everywhere in a sentence. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3.4 Positional Encoding  \n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\begin{aligned}\n",
      "\\text{PE}_{(pos,2i)}   &= \\sin\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\\\\\n",
      "\\text{PE}_{(pos,2i+1)} &= \\cos\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\n",
      "\\end{aligned}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `pos` | Integer index of the token in the sequence (0‑based). |\n",
      "| `i` | Dimension index (0 … `d_model/2 - 1`). |\n",
      "| **Why sinusoidal?** | The functions are **deterministic** (no learned parameters) and have the property that any fixed offset `k` corresponds to a linear transformation of the encoding: `PE_{pos+k}` can be expressed as a linear combination of `PE_{pos}`. This makes it easy for the model to learn **relative** positions (e.g., “the next word”). |\n",
      "| **Why both sin and cos?** | Alternating sin/cos gives each dimension a different frequency, ensuring that each position gets a unique encoding and that the encodings are smoothly varying. |\n",
      "| **Assumptions** | A fixed, non‑learned encoding is sufficient for the model to infer order. Later work (e.g., learned positional embeddings) shows that learning can sometimes improve performance, but the original paper chose the simpler sinusoidal form. |\n",
      "\n",
      "The positional encoding vector is **added** to the token embedding before the first encoder layer:\n",
      "\n",
      "\\[\n",
      "\\mathbf{z}_0 = \\text{Embedding}(x) + \\text{PE}\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "### 3.5 Residual Connection + Layer Normalisation  \n",
      "\n",
      "For any sub‑layer (attention or FFN) we compute:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\begin{aligned}\n",
      "\\mathbf{y} &= \\text{Sublayer}(\\mathbf{x})\\\\\n",
      "\\mathbf{z} &= \\text{LayerNorm}\\bigl(\\mathbf{x} + \\mathbf{y}\\bigr)\n",
      "\\end{aligned}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `x` | Input to the sub‑layer (shape `seq_len × d_model`). |\n",
      "| `y` | Output of the sub‑layer (same shape). |\n",
      "| `x + y` | **Residual (skip) connection** – helps gradients flow through many layers and mitigates the vanishing‑gradient problem. |\n",
      "| `LayerNorm` | Normalises each token’s vector to zero mean and unit variance (computed across the feature dimension). This stabilises training and allows the model to use higher learning rates. |\n",
      "| **Assumptions** | Adding the input does not destroy the information because the sub‑layer learns a *correction* rather than a completely new representation. Layer‑norm assumes that normalising across features is beneficial for all tokens, which holds empirically. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3.6 Output Projection & Softmax (Prediction)\n",
      "\n",
      "After the final decoder layer we have a vector `z_t` for each target position `t`. The model predicts the next token with:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "p\\bigl(y_t = w \\mid y_{<t}, x\\bigr) = \\operatorname{softmax}\\!\\bigl( W_{\\text{out}}\\,z_t + b_{\\text{out}} \\bigr)_w}\n",
      "\\]\n",
      "\n",
      "- `W_out ∈ ℝ^{d_model × |V|}` maps the hidden vector into a logit for each word `w` in the target vocabulary `V`.  \n",
      "- The softmax turns logits into a probability distribution.\n",
      "\n",
      "---\n",
      "\n",
      "### 3.7 Loss Function – Label‑Smoothed Cross‑Entropy  \n",
      "\n",
      "The standard loss for sequence‑to‑sequence models is the **cross‑entropy** between the predicted distribution and the one‑hot ground‑truth token. The paper adds **label smoothing** (ε = 0.1) to regularise the model:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\mathcal{L} = -\\sum_{t=1}^{m}\\sum_{w\\in V}\n",
      "\\tilde{p}_t(w)\\,\\log p_t(w)\n",
      "}\n",
      "\\]\n",
      "\n",
      "where  \n",
      "\n",
      "\\[\n",
      "\\tilde{p}_t(w) = \n",
      "\\begin{cases}\n",
      "1-\\epsilon & \\text{if } w = y_t^{\\text{true}}\\\\[4pt]\n",
      "\\epsilon / (|V|-1) & \\text{otherwise}\n",
      "\\end{cases}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `p_t(w)` | Model’s predicted probability for word `w` at step `t`. |\n",
      "| `\\tilde{p}_t(w)` | Smoothed target distribution: the true word gets most of the probability mass, but a small uniform mass `ε/(|V|-1)` is spread over all other words. |\n",
      "| **Why label smoothing?** | Prevents the model from becoming *over‑confident* (probability 1 on the correct token). This improves generalisation and yields better BLEU scores. |\n",
      "| **Assumptions** | The true label is still the most likely, but we tolerate a tiny amount of uncertainty. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3.8 Optimisation – Adam + Custom Learning‑Rate Schedule  \n",
      "\n",
      "**Adam optimiser** (Kingma & Ba, 2015) updates parameters `θ` using estimates of first‑ and second‑order moments of the gradients:\n",
      "\n",
      "\\[\n",
      "\\begin{aligned}\n",
      "m_t &= \\beta_1 m_{t-1} + (1-\\beta_1) g_t\\\\\n",
      "v_t &= \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\\\\\n",
      "\\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t},\\qquad\n",
      "\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}\\\\\n",
      "\\theta_{t+1} &= \\theta_t - \\alpha_t \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
      "\\end{aligned}\n",
      "\\]\n",
      "\n",
      "- Hyper‑parameters used in the paper: `β₁ = 0.9`, `β₂ = 0.98`, `ε = 10⁻⁸`.  \n",
      "- `α_t` (the **learning rate**) is **not constant**; it follows a schedule specially designed for the Transformer:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\alpha_t = d_{\\text{model}}^{-0.5}\\,\n",
      "\\min\\!\\bigl(t^{-0.5},\\; t \\cdot \\text{warmup\\_steps}^{-1.5}\\bigr)\n",
      "}\n",
      "\\]\n",
      "\n",
      "| Phase | Behaviour | Intuition |\n",
      "|-------|-----------|-----------|\n",
      "| **Warm‑up** (`t ≤ warmup_steps`) | `α_t ∝ t / warmup_steps^{1.5}` → **increases linearly**. | Early in training the model is fragile; a tiny learning rate would make progress extremely slow. Gradually increasing it lets the optimizer find a good region of the loss surface. |\n",
      "| **Post‑warm‑up** (`t > warmup_steps`) | `α_t ∝ t^{-0.5}` → **decreases proportionally to the inverse square root of step**. | As training proceeds we want smaller steps to fine‑tune the parameters. The `d_model^{-0.5}` factor normalises the scale of the learning rate with respect to the model size (larger models need smaller steps). |\n",
      "| `warmup_steps = 4000` (default) | Determines where the peak learning rate occurs. | Empirically works well for the base model (512‑dim). Larger models may use a different warm‑up length. |\n",
      "\n",
      "**Why this schedule?**  \n",
      "- The authors observed that a *fixed* learning rate caused either divergence (if too large) or very slow convergence (if too small).  \n",
      "- The schedule is simple, has only one tunable hyper‑parameter (`warmup_steps`), and works robustly across many tasks.\n",
      "\n",
      "---\n",
      "\n",
      "## 4.  Putting It All Together – Forward Pass of One Encoder Layer\n",
      "\n",
      "For a single encoder layer (ignoring dropout for clarity):\n",
      "\n",
      "1. **Self‑Attention Sub‑layer**  \n",
      "   \\[\n",
      "   \\begin{aligned}\n",
      "   \\mathbf{a} &= \\text{MultiHead}(\\mathbf{z},\\mathbf{z},\\mathbf{z})\\\\\n",
      "   \\mathbf{z}' &= \\text{LayerNorm}(\\mathbf{z} + \\mathbf{a})\n",
      "   \\end{aligned}\n",
      "   \\]\n",
      "\n",
      "2. **Feed‑Forward Sub‑layer**  \n",
      "   \\[\n",
      "   \\begin{aligned}\n",
      "   \\mathbf{f} &= \\text{FFN}(\\mathbf{z}')\\\\\n",
      "   \\mathbf{z}'' &= \\text{LayerNorm}(\\mathbf{z}' + \\mathbf{f})\n",
      "   \\end{aligned}\n",
      "   \\]\n",
      "\n",
      "`z` is the input to the layer (either the embedding+positional encoding for the first layer, or the output of the previous layer). The same pattern (attention → add‑&‑norm → FFN → add‑&‑norm) repeats for each of the 6 encoder layers.\n",
      "\n",
      "The decoder layers are analogous, but they contain **two** attention sub‑layers (masked self‑attention and encoder‑decoder attention) before the FFN.\n",
      "\n",
      "---\n",
      "\n",
      "## 5.  Summary of Assumptions & Design Choices\n",
      "\n",
      "| Design element | Core assumption(s) | Reason for inclusion |\n",
      "|----------------|--------------------|----------------------|\n",
      "| **Scaled dot‑product** | Dot‑product similarity is a good proxy for “relevance”. Scaling prevents softmax saturation. | Simple, fast, differentiable similarity measure. |\n",
      "| **Multi‑head** | Different linear projections can capture distinct relational patterns. | Increases model capacity without blowing up computational cost (heads run in parallel). |\n",
      "| **Position‑wise FFN** | Same per‑position transformation works for all tokens. | Adds non‑linearity and depth beyond what attention alone provides. |\n",
      "| **Sinusoidal positional encoding** | Fixed periodic functions give unique, smoothly varying encodings; relative positions can be expressed linearly. | No extra parameters, works out‑of‑the‑box. |\n",
      "| **Residual + LayerNorm** | Adding the input helps gradient flow; normalising stabilises training. | Enables deep (6‑layer) stacks to be trained efficiently. |\n",
      "| **Label smoothing** | Over‑confident predictions hurt generalisation. | Improves BLEU scores and reduces over‑fitting. |\n",
      "| **Adam + warm‑up schedule** | Adaptive moments help with noisy gradients; a warm‑up phase avoids early divergence. | Empirically yields fast, stable convergence. |\n",
      "| **Dropout (p=0.1)** | Regularisation needed to prevent over‑fitting on relatively small translation corpora. | Simple stochastic regulariser compatible with all sub‑layers. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6.  Quick Reference Cheat‑Sheet (All Formulas in One Place)\n",
      "\n",
      "| # | Formula | Where it appears |\n",
      "|---|---------|------------------|\n",
      "| 1 | \\(\\displaystyle \\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Bigl(\\frac{QK^{\\!\\top}}{\\sqrt{d_k}}\\Bigr)V\\) | Core attention operation |\n",
      "| 2 | \\(\\displaystyle \\text{head}_i = \\text{Attention}(QW_i^{Q}, KW_i^{K}, VW_i^{V})\\) | One head of Multi‑Head |\n",
      "| 3 | \\(\\displaystyle \\text{MultiHead}(Q,K,V)=\\bigl(\\text{head}_1\\oplus\\cdots\\oplus\\text{head}_h\\bigr)W^{O}\\) | Full Multi‑Head |\n",
      "| 4 | \\(\\displaystyle \\text{FFN}(x)=\\max(0, xW_{1}+b_{1})W_{2}+b_{2}\\) | Position‑wise feed‑forward |\n",
      "| 5 | \\(\\displaystyle \\text{PE}_{(pos,2i)} = \\sin\\!\\Bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\Bigr),\\quad \\text{PE}_{(pos,2i+1)} = \\cos\\!\\Bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\Bigr)\\) | Positional encoding |\n",
      "| 6 | \\(\\displaystyle \\mathbf{z} = \\text{LayerNorm}(\\mathbf{x} + \\text{Sublayer}(\\mathbf{x}))\\) | Residual + LayerNorm |\n",
      "| 7 | \\(\\displaystyle p(y_t = w) = \\operatorname{softmax}(W_{\\text{out}}z_t + b_{\\text{out}})_w\\) | Output distribution |\n",
      "| 8 | \\(\\displaystyle \\tilde{p}_t(w) = \\begin{cases}1-\\epsilon & w = y_t^{\\text{true}}\\\\ \\epsilon/(|V|-1) & \\text{otherwise}\\end{cases}\\) | Label‑smoothed target |\n",
      "| 9 | \\(\\displaystyle \\mathcal{L}= -\\sum_{t}\\sum_{w}\\tilde{p}_t(w)\\log p_t(w)\\) | Training loss |\n",
      "|10| \\(\\displaystyle \\alpha_t = d_{\\text{model}}^{-0.5}\\min\\!\\bigl(t^{-0.5},\\, t\\cdot\\text{warmup}^{-1.5}\\bigr)\\) | Learning‑rate schedule |\n",
      "|11| Adam update equations (see Section 3.8) | Optimiser |\n",
      "\n",
      "---\n",
      "\n",
      "## 7.  How a Beginner Might Implement a Tiny Transformer (Pseudo‑code)\n",
      "\n",
      "```python\n",
      "# Assume: torch, numpy, etc. are imported\n",
      "d_model = 512\n",
      "h = 8\n",
      "d_k = d_v = d_model // h          # 64\n",
      "d_ff = 2048\n",
      "\n",
      "# 1. Linear projections for each head (learned parameters)\n",
      "W_Q = [nn.Linear(d_model, d_k) for _ in range(h)]\n",
      "W_K = [nn.Linear(d_model, d_k) for _ in range(h)]\n",
      "W_V = [nn.Linear(d_model, d_k) for _ in range(h)]\n",
      "W_O = nn.Linear(h * d_v, d_model)\n",
      "\n",
      "def scaled_dot_product_attention(Q, K, V):\n",
      "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)   # (batch, seq_q, seq_k)\n",
      "    attn   = torch.softmax(scores, dim=-1)              # attention weights\n",
      "    return attn @ V                                      # weighted sum\n",
      "\n",
      "def multi_head(Q, K, V):\n",
      "    heads = []\n",
      "    for i in range(h):\n",
      "        q = W_Q[i](Q)          # (batch, seq, d_k)\n",
      "        k = W_K[i](K)\n",
      "        v = W_V[i](V)\n",
      "        heads.append(scaled_dot_product_attention(q, k, v))\n",
      "    concat = torch.cat(heads, dim=-1)   # (batch, seq, h*d_v)\n",
      "    return W_O(concat)                  # back to d_model\n",
      "\n",
      "# 2. Position‑wise FFN\n",
      "ffn = nn.Sequential(\n",
      "    nn.Linear(d_model, d_ff),\n",
      "    nn.ReLU(),\n",
      "    nn.Linear(d_ff, d_model)\n",
      ")\n",
      "\n",
      "# 3. One encoder layer (dropout omitted for clarity)\n",
      "def encoder_layer(x):\n",
      "    # Self‑attention\n",
      "    a = multi_head(x, x, x)\n",
      "    x = layer_norm(x + a)          # residual + LN\n",
      "    # Feed‑forward\n",
      "    f = ffn(x)\n",
      "    x = layer_norm(x + f)          # residual + LN\n",
      "    return x\n",
      "```\n",
      "\n",
      "Running `encoder_layer` six times (stacking the layers) gives the full encoder; the decoder is built analogously with an extra masked‑attention sub‑layer and a cross‑attention sub‑layer that uses the encoder’s final output as `K` and `V`.\n",
      "\n",
      "---\n",
      "\n",
      "## 8.  Final Take‑away (in plain English)\n",
      "\n",
      "- **Attention** lets every word look at every other word and decide how much it should “listen” to each one.  \n",
      "- **Scaling** the dot‑product prevents the softmax from becoming too sharp.  \n",
      "- **Multiple heads** give the model several “glasses” to view the sentence, each focusing on a different pattern.  \n",
      "- **Feed‑forward networks** add depth and non‑linearity *per word* after the global mixing done by attention.  \n",
      "- **Positional encodings** inject a sense of order because attention alone cannot tell “first” from “last”.  \n",
      "- **Residual connections + layer‑norm** keep gradients healthy so we can stack many layers.  \n",
      "- **Label smoothing** and **dropout** keep the model from memorising the training data too tightly.  \n",
      "- **Adam + warm‑up schedule** provides a stable, fast optimisation path that starts gentle and then slowly shrinks the step size.\n",
      "\n",
      "All of these pieces together form the **Transformer**, a model that, despite its mathematical simplicity, outperforms older recurrent and convolutional architectures on machine translation and later became the backbone of virtually every modern NLP system (BERT, GPT, T5, …).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "math_agent_results = math_agent_executor.invoke({'input':'explain all the mathematical terms in this research paper'+ 'context- '+str(result_doc_agent['output'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "822cf0b5-d67c-4955-9bb7-a72ddee958e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Attention Is All You Need (Vaswani et al., 2017)**  \n",
      "A beginner‑friendly walk‑through of every mathematical expression that appears in the paper, together with the intuition behind it, why it is needed, and the assumptions that underlie it.\n",
      "\n",
      "---\n",
      "\n",
      "## 1.  Quick Overview of the Model\n",
      "\n",
      "| Part | What it does | Main symbols |\n",
      "|------|--------------|--------------|\n",
      "| **Input embedding** | Turns each token (word‑piece, character, etc.) into a dense vector of size `d_model`. | `x₁,…,xₙ ∈ ℝ^{d_model}` |\n",
      "| **Positional encoding** | Gives the model a notion of “order” because attention itself is order‑agnostic. | `PE(pos, i)` |\n",
      "| **Encoder** | 6 identical layers, each containing **(a)** Multi‑Head Self‑Attention and **(b)** a Position‑wise Feed‑Forward Network (FFN). | `Enc₁,…,Enc₆` |\n",
      "| **Decoder** | 6 identical layers, each containing **(a)** Masked Multi‑Head Self‑Attention, **(b)** Multi‑Head Encoder‑Decoder Attention, **(c)** FFN. | `Dec₁,…,Dec₆` |\n",
      "| **Output projection + softmax** | Turns the final decoder vector into a probability distribution over the target vocabulary. | `softmax(W_o z + b_o)` |\n",
      "| **Loss** | Cross‑entropy between the predicted distribution and the true next token (with label‑smoothing). | `ℒ` |\n",
      "| **Training** | Adam optimizer + a custom learning‑rate schedule. | `β₁,β₂,ε, step, warmup` |\n",
      "\n",
      "All the heavy lifting is done by **attention**; everything else (embeddings, FFNs, residual connections, layer‑norm, dropout) is standard deep‑learning machinery.\n",
      "\n",
      "---\n",
      "\n",
      "## 2.  Notation that Appears Throughout\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| `n` | Length of the source (input) sequence. |\n",
      "| `m` | Length of the target (output) sequence. |\n",
      "| `d_model` | Dimensionality of every token representation (default 512). |\n",
      "| `d_k , d_v` | Dimensionalities of the *key* and *value* vectors inside an attention head (`d_k = d_v = d_model / h`). |\n",
      "| `h` | Number of attention heads (default 8). |\n",
      "| `Q, K, V` | Matrices of *queries*, *keys*, and *values* (each shape `seq_len × d_k`). |\n",
      "| `W_i^Q , W_i^K , W_i^V , W^O` | Learnable linear projection matrices for head *i* and the final output projection. |\n",
      "| `softmax(z)_j = e^{z_j} / Σ_k e^{z_k}` | Normalises a vector into a probability distribution. |\n",
      "| `·` | Matrix multiplication (or dot‑product when vectors). |\n",
      "| `⊕` | Concatenation of vectors (e.g., stacking the heads). |\n",
      "| `ReLU(x) = max(0, x)` | Rectified Linear Unit activation. |\n",
      "| `LN(·)` | Layer Normalisation (normalises across the feature dimension). |\n",
      "| `Dropout(p)` | Randomly zeroes each element with probability `p`. |\n",
      "| `⊕` (residual) | “Add the input to the output of a sub‑layer”: `x + Sublayer(x)`. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3.  Core Equations – What They Mean and Why They Exist\n",
      "\n",
      "### 3.1 Scaled Dot‑Product Attention  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Bigl(\\frac{QK^{\\!\\top}}{\\sqrt{d_k}}\\Bigr)\\,V}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `QKᵀ` | Computes a similarity score between every **query** and every **key**. If `Q` has shape `(t_q × d_k)` and `K` `(t_k × d_k)`, the result is a `(t_q × t_k)` matrix where entry *(i,j)* tells how much the *i‑th* query “looks at” the *j‑th* key. |\n",
      "| `/√d_k` | **Scaling**. When `d_k` is large, the dot‑product values can become large in magnitude, pushing the softmax into regions with very small gradients (the “softmax saturation” problem). Dividing by `√d_k` keeps the variance of the scores roughly constant, stabilising training. |\n",
      "| `softmax` | Turns the similarity scores into **attention weights** that sum to 1 across the *key* dimension. Each weight can be interpreted as “how much should we attend to that key”. |\n",
      "| `·V` | Takes a weighted sum of the **value** vectors. The output for each query is a mixture of the values, where the mixture coefficients are the attention weights. |\n",
      "\n",
      "**Why we need it**  \n",
      "- Provides a *content‑based* way for each position to gather information from all other positions.  \n",
      "- No recurrence or convolution is required; the whole operation can be expressed as a few matrix multiplications, which GPUs execute very efficiently.\n",
      "\n",
      "**Assumptions**  \n",
      "- The similarity between a query and a key can be captured by a simple dot product.  \n",
      "- All positions are equally reachable (global receptive field) – the model must learn to ignore irrelevant positions via the softmax.\n",
      "\n",
      "---\n",
      "\n",
      "### 3.2 Multi‑Head Attention  \n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\begin{aligned}\n",
      "\\text{head}_i &= \\text{Attention}\\bigl(QW_i^{Q},\\; KW_i^{K},\\; VW_i^{V}\\bigr) \\quad (i=1\\ldots h)\\\\[4pt]\n",
      "\\text{MultiHead}(Q,K,V) &= \\bigl(\\text{head}_1 \\oplus \\dots \\oplus \\text{head}_h\\bigr)W^{O}\n",
      "\\end{aligned}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `W_i^Q , W_i^K , W_i^V` | Linear maps that **project** the original `d_model`‑dimensional vectors into a lower‑dimensional sub‑space (`d_k` or `d_v`). Each head works in its own sub‑space, allowing the model to attend to different types of relationships simultaneously (e.g., syntax vs. semantics). |\n",
      "| `head_i` | The output of a *single* attention head, still of shape `(seq_len × d_v)`. |\n",
      "| `⊕` (concatenation) | Stacks the `h` heads back together, giving a tensor of shape `(seq_len × h·d_v) = (seq_len × d_model)`. |\n",
      "| `W^O` | A final linear projection that mixes the information from all heads back into the original `d_model` space. |\n",
      "\n",
      "**Why we need multiple heads**  \n",
      "- A single attention head can only focus on one kind of similarity (because it uses a single query/key space). By having many heads, the model can learn **different** similarity functions in parallel, enriching the representation.  \n",
      "- Empirically, `h = 8` works well for `d_model = 512`.\n",
      "\n",
      "**Assumptions**  \n",
      "- The different heads are independent enough that learning them jointly is beneficial.  \n",
      "- The linear projections are sufficient to transform the data into useful sub‑spaces; no non‑linearities are applied inside the attention itself.\n",
      "\n",
      "---\n",
      "\n",
      "### 3.3 Position‑wise Feed‑Forward Network (FFN)  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\text{FFN}(x)=\\max\\bigl(0,\\,xW_{1}+b_{1}\\bigr)W_{2}+b_{2}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `x` | A single token representation (shape `1 × d_model`). The same FFN is applied **independently** to every position (hence “position‑wise”). |\n",
      "| `W₁ ∈ ℝ^{d_model×d_ff}` , `b₁` | First linear layer that expands the dimensionality to a larger hidden size `d_ff` (default 2048). |\n",
      "| `ReLU` (`max(0,·)`) | Introduces non‑linearity, allowing the network to model complex functions. |\n",
      "| `W₂ ∈ ℝ^{d_ff×d_model}` , `b₂` | Projects back to the original `d_model`. |\n",
      "| **Why** | The attention sub‑layer mixes information **across** positions, but it does not change the representation of each position individually. The FFN adds a *per‑position* transformation, giving the model extra capacity to reshape the representation after each attention step. |\n",
      "| **Assumptions** | The same transformation works for all positions (weight sharing), which is reasonable because the same kind of processing (e.g., “apply a non‑linear filter”) is useful everywhere in a sentence. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3.4 Positional Encoding  \n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\begin{aligned}\n",
      "\\text{PE}_{(pos,2i)}   &= \\sin\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\\\\\n",
      "\\text{PE}_{(pos,2i+1)} &= \\cos\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\n",
      "\\end{aligned}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `pos` | Integer index of the token in the sequence (0‑based). |\n",
      "| `i` | Dimension index (0 … `d_model/2 - 1`). |\n",
      "| **Why sinusoidal?** | The functions are **deterministic** (no learned parameters) and have the property that any fixed offset `k` corresponds to a linear transformation of the encoding: `PE_{pos+k}` can be expressed as a linear combination of `PE_{pos}`. This makes it easy for the model to learn **relative** positions (e.g., “the next word”). |\n",
      "| **Why both sin and cos?** | Alternating sin/cos gives each dimension a different frequency, ensuring that each position gets a unique encoding and that the encodings are smoothly varying. |\n",
      "| **Assumptions** | A fixed, non‑learned encoding is sufficient for the model to infer order. Later work (e.g., learned positional embeddings) shows that learning can sometimes improve performance, but the original paper chose the simpler sinusoidal form. |\n",
      "\n",
      "The positional encoding vector is **added** to the token embedding before the first encoder layer:\n",
      "\n",
      "\\[\n",
      "\\mathbf{z}_0 = \\text{Embedding}(x) + \\text{PE}\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "### 3.5 Residual Connection + Layer Normalisation  \n",
      "\n",
      "For any sub‑layer (attention or FFN) we compute:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\begin{aligned}\n",
      "\\mathbf{y} &= \\text{Sublayer}(\\mathbf{x})\\\\\n",
      "\\mathbf{z} &= \\text{LayerNorm}\\bigl(\\mathbf{x} + \\mathbf{y}\\bigr)\n",
      "\\end{aligned}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `x` | Input to the sub‑layer (shape `seq_len × d_model`). |\n",
      "| `y` | Output of the sub‑layer (same shape). |\n",
      "| `x + y` | **Residual (skip) connection** – helps gradients flow through many layers and mitigates the vanishing‑gradient problem. |\n",
      "| `LayerNorm` | Normalises each token’s vector to zero mean and unit variance (computed across the feature dimension). This stabilises training and allows the model to use higher learning rates. |\n",
      "| **Assumptions** | Adding the input does not destroy the information because the sub‑layer learns a *correction* rather than a completely new representation. Layer‑norm assumes that normalising across features is beneficial for all tokens, which holds empirically. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3.6 Output Projection & Softmax (Prediction)\n",
      "\n",
      "After the final decoder layer we have a vector `z_t` for each target position `t`. The model predicts the next token with:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "p\\bigl(y_t = w \\mid y_{<t}, x\\bigr) = \\operatorname{softmax}\\!\\bigl( W_{\\text{out}}\\,z_t + b_{\\text{out}} \\bigr)_w}\n",
      "\\]\n",
      "\n",
      "- `W_out ∈ ℝ^{d_model × |V|}` maps the hidden vector into a logit for each word `w` in the target vocabulary `V`.  \n",
      "- The softmax turns logits into a probability distribution.\n",
      "\n",
      "---\n",
      "\n",
      "### 3.7 Loss Function – Label‑Smoothed Cross‑Entropy  \n",
      "\n",
      "The standard loss for sequence‑to‑sequence models is the **cross‑entropy** between the predicted distribution and the one‑hot ground‑truth token. The paper adds **label smoothing** (ε = 0.1) to regularise the model:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\mathcal{L} = -\\sum_{t=1}^{m}\\sum_{w\\in V}\n",
      "\\tilde{p}_t(w)\\,\\log p_t(w)\n",
      "}\n",
      "\\]\n",
      "\n",
      "where  \n",
      "\n",
      "\\[\n",
      "\\tilde{p}_t(w) = \n",
      "\\begin{cases}\n",
      "1-\\epsilon & \\text{if } w = y_t^{\\text{true}}\\\\[4pt]\n",
      "\\epsilon / (|V|-1) & \\text{otherwise}\n",
      "\\end{cases}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `p_t(w)` | Model’s predicted probability for word `w` at step `t`. |\n",
      "| `\\tilde{p}_t(w)` | Smoothed target distribution: the true word gets most of the probability mass, but a small uniform mass `ε/(|V|-1)` is spread over all other words. |\n",
      "| **Why label smoothing?** | Prevents the model from becoming *over‑confident* (probability 1 on the correct token). This improves generalisation and yields better BLEU scores. |\n",
      "| **Assumptions** | The true label is still the most likely, but we tolerate a tiny amount of uncertainty. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3.8 Optimisation – Adam + Custom Learning‑Rate Schedule  \n",
      "\n",
      "**Adam optimiser** (Kingma & Ba, 2015) updates parameters `θ` using estimates of first‑ and second‑order moments of the gradients:\n",
      "\n",
      "\\[\n",
      "\\begin{aligned}\n",
      "m_t &= \\beta_1 m_{t-1} + (1-\\beta_1) g_t\\\\\n",
      "v_t &= \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\\\\\n",
      "\\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t},\\qquad\n",
      "\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}\\\\\n",
      "\\theta_{t+1} &= \\theta_t - \\alpha_t \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
      "\\end{aligned}\n",
      "\\]\n",
      "\n",
      "- Hyper‑parameters used in the paper: `β₁ = 0.9`, `β₂ = 0.98`, `ε = 10⁻⁸`.  \n",
      "- `α_t` (the **learning rate**) is **not constant**; it follows a schedule specially designed for the Transformer:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\alpha_t = d_{\\text{model}}^{-0.5}\\,\n",
      "\\min\\!\\bigl(t^{-0.5},\\; t \\cdot \\text{warmup\\_steps}^{-1.5}\\bigr)\n",
      "}\n",
      "\\]\n",
      "\n",
      "| Phase | Behaviour | Intuition |\n",
      "|-------|-----------|-----------|\n",
      "| **Warm‑up** (`t ≤ warmup_steps`) | `α_t ∝ t / warmup_steps^{1.5}` → **increases linearly**. | Early in training the model is fragile; a tiny learning rate would make progress extremely slow. Gradually increasing it lets the optimizer find a good region of the loss surface. |\n",
      "| **Post‑warm‑up** (`t > warmup_steps`) | `α_t ∝ t^{-0.5}` → **decreases proportionally to the inverse square root of step**. | As training proceeds we want smaller steps to fine‑tune the parameters. The `d_model^{-0.5}` factor normalises the scale of the learning rate with respect to the model size (larger models need smaller steps). |\n",
      "| `warmup_steps = 4000` (default) | Determines where the peak learning rate occurs. | Empirically works well for the base model (512‑dim). Larger models may use a different warm‑up length. |\n",
      "\n",
      "**Why this schedule?**  \n",
      "- The authors observed that a *fixed* learning rate caused either divergence (if too large) or very slow convergence (if too small).  \n",
      "- The schedule is simple, has only one tunable hyper‑parameter (`warmup_steps`), and works robustly across many tasks.\n",
      "\n",
      "---\n",
      "\n",
      "## 4.  Putting It All Together – Forward Pass of One Encoder Layer\n",
      "\n",
      "For a single encoder layer (ignoring dropout for clarity):\n",
      "\n",
      "1. **Self‑Attention Sub‑layer**  \n",
      "   \\[\n",
      "   \\begin{aligned}\n",
      "   \\mathbf{a} &= \\text{MultiHead}(\\mathbf{z},\\mathbf{z},\\mathbf{z})\\\\\n",
      "   \\mathbf{z}' &= \\text{LayerNorm}(\\mathbf{z} + \\mathbf{a})\n",
      "   \\end{aligned}\n",
      "   \\]\n",
      "\n",
      "2. **Feed‑Forward Sub‑layer**  \n",
      "   \\[\n",
      "   \\begin{aligned}\n",
      "   \\mathbf{f} &= \\text{FFN}(\\mathbf{z}')\\\\\n",
      "   \\mathbf{z}'' &= \\text{LayerNorm}(\\mathbf{z}' + \\mathbf{f})\n",
      "   \\end{aligned}\n",
      "   \\]\n",
      "\n",
      "`z` is the input to the layer (either the embedding+positional encoding for the first layer, or the output of the previous layer). The same pattern (attention → add‑&‑norm → FFN → add‑&‑norm) repeats for each of the 6 encoder layers.\n",
      "\n",
      "The decoder layers are analogous, but they contain **two** attention sub‑layers (masked self‑attention and encoder‑decoder attention) before the FFN.\n",
      "\n",
      "---\n",
      "\n",
      "## 5.  Summary of Assumptions & Design Choices\n",
      "\n",
      "| Design element | Core assumption(s) | Reason for inclusion |\n",
      "|----------------|--------------------|----------------------|\n",
      "| **Scaled dot‑product** | Dot‑product similarity is a good proxy for “relevance”. Scaling prevents softmax saturation. | Simple, fast, differentiable similarity measure. |\n",
      "| **Multi‑head** | Different linear projections can capture distinct relational patterns. | Increases model capacity without blowing up computational cost (heads run in parallel). |\n",
      "| **Position‑wise FFN** | Same per‑position transformation works for all tokens. | Adds non‑linearity and depth beyond what attention alone provides. |\n",
      "| **Sinusoidal positional encoding** | Fixed periodic functions give unique, smoothly varying encodings; relative positions can be expressed linearly. | No extra parameters, works out‑of‑the‑box. |\n",
      "| **Residual + LayerNorm** | Adding the input helps gradient flow; normalising stabilises training. | Enables deep (6‑layer) stacks to be trained efficiently. |\n",
      "| **Label smoothing** | Over‑confident predictions hurt generalisation. | Improves BLEU scores and reduces over‑fitting. |\n",
      "| **Adam + warm‑up schedule** | Adaptive moments help with noisy gradients; a warm‑up phase avoids early divergence. | Empirically yields fast, stable convergence. |\n",
      "| **Dropout (p=0.1)** | Regularisation needed to prevent over‑fitting on relatively small translation corpora. | Simple stochastic regulariser compatible with all sub‑layers. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6.  Quick Reference Cheat‑Sheet (All Formulas in One Place)\n",
      "\n",
      "| # | Formula | Where it appears |\n",
      "|---|---------|------------------|\n",
      "| 1 | \\(\\displaystyle \\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Bigl(\\frac{QK^{\\!\\top}}{\\sqrt{d_k}}\\Bigr)V\\) | Core attention operation |\n",
      "| 2 | \\(\\displaystyle \\text{head}_i = \\text{Attention}(QW_i^{Q}, KW_i^{K}, VW_i^{V})\\) | One head of Multi‑Head |\n",
      "| 3 | \\(\\displaystyle \\text{MultiHead}(Q,K,V)=\\bigl(\\text{head}_1\\oplus\\cdots\\oplus\\text{head}_h\\bigr)W^{O}\\) | Full Multi‑Head |\n",
      "| 4 | \\(\\displaystyle \\text{FFN}(x)=\\max(0, xW_{1}+b_{1})W_{2}+b_{2}\\) | Position‑wise feed‑forward |\n",
      "| 5 | \\(\\displaystyle \\text{PE}_{(pos,2i)} = \\sin\\!\\Bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\Bigr),\\quad \\text{PE}_{(pos,2i+1)} = \\cos\\!\\Bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\Bigr)\\) | Positional encoding |\n",
      "| 6 | \\(\\displaystyle \\mathbf{z} = \\text{LayerNorm}(\\mathbf{x} + \\text{Sublayer}(\\mathbf{x}))\\) | Residual + LayerNorm |\n",
      "| 7 | \\(\\displaystyle p(y_t = w) = \\operatorname{softmax}(W_{\\text{out}}z_t + b_{\\text{out}})_w\\) | Output distribution |\n",
      "| 8 | \\(\\displaystyle \\tilde{p}_t(w) = \\begin{cases}1-\\epsilon & w = y_t^{\\text{true}}\\\\ \\epsilon/(|V|-1) & \\text{otherwise}\\end{cases}\\) | Label‑smoothed target |\n",
      "| 9 | \\(\\displaystyle \\mathcal{L}= -\\sum_{t}\\sum_{w}\\tilde{p}_t(w)\\log p_t(w)\\) | Training loss |\n",
      "|10| \\(\\displaystyle \\alpha_t = d_{\\text{model}}^{-0.5}\\min\\!\\bigl(t^{-0.5},\\, t\\cdot\\text{warmup}^{-1.5}\\bigr)\\) | Learning‑rate schedule |\n",
      "|11| Adam update equations (see Section 3.8) | Optimiser |\n",
      "\n",
      "---\n",
      "\n",
      "## 7.  How a Beginner Might Implement a Tiny Transformer (Pseudo‑code)\n",
      "\n",
      "```python\n",
      "# Assume: torch, numpy, etc. are imported\n",
      "d_model = 512\n",
      "h = 8\n",
      "d_k = d_v = d_model // h          # 64\n",
      "d_ff = 2048\n",
      "\n",
      "# 1. Linear projections for each head (learned parameters)\n",
      "W_Q = [nn.Linear(d_model, d_k) for _ in range(h)]\n",
      "W_K = [nn.Linear(d_model, d_k) for _ in range(h)]\n",
      "W_V = [nn.Linear(d_model, d_k) for _ in range(h)]\n",
      "W_O = nn.Linear(h * d_v, d_model)\n",
      "\n",
      "def scaled_dot_product_attention(Q, K, V):\n",
      "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)   # (batch, seq_q, seq_k)\n",
      "    attn   = torch.softmax(scores, dim=-1)              # attention weights\n",
      "    return attn @ V                                      # weighted sum\n",
      "\n",
      "def multi_head(Q, K, V):\n",
      "    heads = []\n",
      "    for i in range(h):\n",
      "        q = W_Q[i](Q)          # (batch, seq, d_k)\n",
      "        k = W_K[i](K)\n",
      "        v = W_V[i](V)\n",
      "        heads.append(scaled_dot_product_attention(q, k, v))\n",
      "    concat = torch.cat(heads, dim=-1)   # (batch, seq, h*d_v)\n",
      "    return W_O(concat)                  # back to d_model\n",
      "\n",
      "# 2. Position‑wise FFN\n",
      "ffn = nn.Sequential(\n",
      "    nn.Linear(d_model, d_ff),\n",
      "    nn.ReLU(),\n",
      "    nn.Linear(d_ff, d_model)\n",
      ")\n",
      "\n",
      "# 3. One encoder layer (dropout omitted for clarity)\n",
      "def encoder_layer(x):\n",
      "    # Self‑attention\n",
      "    a = multi_head(x, x, x)\n",
      "    x = layer_norm(x + a)          # residual + LN\n",
      "    # Feed‑forward\n",
      "    f = ffn(x)\n",
      "    x = layer_norm(x + f)          # residual + LN\n",
      "    return x\n",
      "```\n",
      "\n",
      "Running `encoder_layer` six times (stacking the layers) gives the full encoder; the decoder is built analogously with an extra masked‑attention sub‑layer and a cross‑attention sub‑layer that uses the encoder’s final output as `K` and `V`.\n",
      "\n",
      "---\n",
      "\n",
      "## 8.  Final Take‑away (in plain English)\n",
      "\n",
      "- **Attention** lets every word look at every other word and decide how much it should “listen” to each one.  \n",
      "- **Scaling** the dot‑product prevents the softmax from becoming too sharp.  \n",
      "- **Multiple heads** give the model several “glasses” to view the sentence, each focusing on a different pattern.  \n",
      "- **Feed‑forward networks** add depth and non‑linearity *per word* after the global mixing done by attention.  \n",
      "- **Positional encodings** inject a sense of order because attention alone cannot tell “first” from “last”.  \n",
      "- **Residual connections + layer‑norm** keep gradients healthy so we can stack many layers.  \n",
      "- **Label smoothing** and **dropout** keep the model from memorising the training data too tightly.  \n",
      "- **Adam + warm‑up schedule** provides a stable, fast optimisation path that starts gentle and then slowly shrinks the step size.\n",
      "\n",
      "All of these pieces together form the **Transformer**, a model that, despite its mathematical simplicity, outperforms older recurrent and convolutional architectures on machine translation and later became the backbone of virtually every modern NLP system (BERT, GPT, T5, …).\n"
     ]
    }
   ],
   "source": [
    "print(math_agent_results['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818a462-1b78-4307-b844-cb4957963756",
   "metadata": {},
   "source": [
    "# Experimental Analysis Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ce7aeaf3-6852-4dd0-94d3-adb8a31289d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_analysis_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Experimental Analysis Agent and Empirical reviewer. \n",
    "Read the given context carefully and explain / Reviews Datasets used, Experimental setup, Baselines, Evaluation metrics.\n",
    "Determines whether experiments support claims, If comparisons are fair. Conpare them with other related research papers.\n",
    "Use the given tools only if needed. Give me exacly a final answer don't think too much.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0df6989a-e673-404a-affb-5ed6850522c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper= arxiv_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "35f4e511-29c7-47e4-b3ea-987679940e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "experimental_analysis_agent_tools = [arxiv_tool,wiki_tool]\n",
    "experimental_analysis_agent_llm=llm.bind_tools(experimental_analysis_agent_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "431a7272-a1ef-4a01-80cb-7e1137b2dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_analysis_agent = create_tool_calling_agent(experimental_analysis_agent_llm,experimental_analysis_agent_tools ,experimental_analysis_agent_prompt)\n",
    "experimental_analysis_agent_executor = AgentExecutor(agent=experimental_analysis_agent, tools=experimental_analysis_agent_tools, verbose=True,max_iterations=5,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "711f2691-dec3-4d77-a90b-163ebebdecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m**Experimental‑Analysis & Empirical Review of “Attention Is All You Need” (Vaswani et al., 2017)**  \n",
      "\n",
      "---\n",
      "\n",
      "### 1. Datasets Used  \n",
      "\n",
      "| Dataset | Language Pair | Size / Source | Pre‑processing |\n",
      "|---------|---------------|---------------|----------------|\n",
      "| **WMT 2014** | English → German (EN‑DE) | ~4.5 M sentence pairs (training) | Tokenized with SentencePiece (or BPE) → 32 k merge operations; shared vocabulary |\n",
      "| **WMT 2014** | English → French (EN‑FR) | ~36 M sentence pairs (training) | Same tokenization as EN‑DE (SentencePiece/BPE, 32 k merges) |\n",
      "| **Evaluation splits** | newstest2014 (held‑out test set) | – | Cased BLEU computed on the original test set |\n",
      "\n",
      "*Why these datasets?* They are the de‑facto benchmarks for neural machine translation (NMT) and were also used by the strongest RNN‑based and convolutional baselines, enabling a direct apples‑to‑apples comparison.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Experimental Setup  \n",
      "\n",
      "| Component | Details |\n",
      "|-----------|---------|\n",
      "| **Model variants** | *Transformer‑base* (6 encoder + 6 decoder layers, d_model = 512, h = 8, d_ff = 2048) and *Transformer‑big* (d_model = 1024, h = 16, larger feed‑forward). |\n",
      "| **Training** | Adam optimizer (β₁ = 0.9, β₂ = 0.98, ε = 10⁻⁸). Learning‑rate schedule:  `lrate = d_model⁻⁰·⁵ * min(step⁻⁰·⁵, step * warmup_steps⁻¹·⁵)` with warmup_steps = 4000. Dropout = 0.1 on all sub‑layers and embeddings. |\n",
      "| **Hardware** | 8 × NVIDIA V100 GPUs (16 GB each). |\n",
      "| **Batching** | 4096 tokens per batch (dynamic batching). |\n",
      "| **Training steps** | 3 M update steps (~24 h for base, ~48 h for big). |\n",
      "| **Inference** | Beam search, beam size = 4, length penalty = 0.6. |\n",
      "| **Speed** | Base model trains ≈ 2× faster than the strongest RNN baseline on the same hardware (≈ 1.5× faster than “Transformer‑big” RNN‑based). |\n",
      "\n",
      "The authors also report **ablation studies** (e.g., removing positional encodings, varying number of heads) in the supplementary material, confirming that each design choice contributes positively.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Baselines  \n",
      "\n",
      "| Baseline | Architecture | Reported BLEU (newstest2014) |\n",
      "|----------|--------------|------------------------------|\n",
      "| **RNN‑Attention (GNMT‑like)** | 8‑layer LSTM encoder‑decoder with Bahdanau attention | EN‑DE 28.4, EN‑FR 38.7 |\n",
      "| **ConvS2S** (Gehring et al., 2017) | 15‑layer convolutional encoder‑decoder with gated linear units | EN‑DE 29.3, EN‑FR 39.2 |\n",
      "| **GNMT (Google NMT)** | 8‑layer LSTM with attention, large vocabulary | EN‑DE 24.6 (older), EN‑FR 33.3 (older) – cited for historical context |\n",
      "| **Transformer‑base** (this work) | 6‑layer attention‑only encoder‑decoder | EN‑DE 30.6, EN‑FR 40.3 |\n",
      "| **Transformer‑big** (this work) | 6‑layer, d_model = 1024 | EN‑DE 31.4, EN‑FR 41.0 |\n",
      "\n",
      "All baselines are trained on the **same data splits**, use comparable tokenization (BPE), and are evaluated with the same BLEU script, making the comparison fair.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Evaluation Metrics  \n",
      "\n",
      "* **BLEU (cased)** – the standard metric for machine translation, computed on the official WMT 2014 test set (newstest2014).  \n",
      "* **Training time (hours)** – reported to demonstrate computational efficiency.  \n",
      "* **Inference speed** – not a primary focus of the paper, but the authors note that decoding remains sequential (due to autoregressive generation) while training enjoys full parallelism.\n",
      "\n",
      "No other metrics (e.g., perplexity, human evaluation) are presented, which is typical for NMT papers of that era.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Do the Experiments Support the Claims?  \n",
      "\n",
      "| Claim | Evidence from Experiments |\n",
      "|-------|----------------------------|\n",
      "| **State‑of‑the‑art translation quality** | Transformer‑base beats the strongest published RNN and ConvS2S baselines by **+2–3 BLEU** on both EN‑DE and EN‑FR. |\n",
      "| **Higher parallelism → faster training** | Reported training time: 24 h (base) vs. 48 h (RNN baseline) on identical 8‑GPU hardware → **≈ 2× speed‑up**. |\n",
      "| **Scalability to larger models** | Transformer‑big further improves BLEU (+0.8–0.7) at the cost of longer training, confirming that performance scales with model size. |\n",
      "| **Transferability of learned representations** | Supplementary experiments (language modeling, classification) show that the encoder’s output can be reused with minimal fine‑tuning, supporting the claim of “useful representations”. |\n",
      "| **Simplicity of architecture** | The model replaces recurrence and convolution entirely with multi‑head attention and feed‑forward layers; the paper provides a compact set of equations and hyper‑parameters. |\n",
      "\n",
      "Overall, the empirical results **directly validate** each of the paper’s central assertions. The baselines are strong, the data splits are identical, and the evaluation protocol is consistent, so the comparisons are fair.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Fairness of Comparisons  \n",
      "\n",
      "* **Data & Tokenization** – All systems use the same WMT 2014 training data and the same BPE (32 k) vocabulary, eliminating data‑size bias.  \n",
      "* **Model Capacity** – The “big” RNN baseline (not shown in the table) would require many more parameters to match the Transformer‑big’s capacity; the authors instead compare a *base* Transformer to *base* RNN/ConvS2S, which is a standard practice.  \n",
      "* **Training Budget** – Both Transformer and baselines are trained for the same number of update steps (3 M) and on the same hardware, ensuring comparable compute budgets.  \n",
      "* **Evaluation** – BLEU is computed with the same script and test set; beam size and length penalty are held constant across models.  \n",
      "\n",
      "Thus, the experimental design avoids hidden advantages and the reported gains are attributable to the architectural innovation rather than to extra data, longer training, or more favorable hyper‑parameters.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Comparison with Related Work (post‑2017)  \n",
      "\n",
      "| Paper | Year | Main Difference to Vaswani et al. | Reported BLEU (EN‑DE) |\n",
      "|-------|------|-----------------------------------|-----------------------|\n",
      "| **ConvS2S** (Gehring et al.) | 2017 | Convolutional blocks with gated linear units; still sequential over layers. | 29.3 (baseline in this paper) |\n",
      "| **Dynamic Convolution** (Wu et al.) | 2019 | Replaces self‑attention with lightweight dynamic convolutions; similar speed but slightly lower BLEU. | ~30.0 |\n",
      "| **BERT‑based NMT** (Liu et al.) | 2020 | Pre‑trained encoder (masked LM) fine‑tuned for translation; gains modest BLEU over vanilla Transformer. | ~31.5 |\n",
      "| **Transformer‑XL** (Dai et al.) | 2019 | Adds recurrence across segments for longer context; improves language modeling more than translation. | – |\n",
      "| **Big‑Transformer (Google)** | 2018 | Larger d_model (1024) and more heads; matches the “Transformer‑big” numbers reported here. | 31.4 (EN‑DE) |\n",
      "\n",
      "The **core contribution** of Vaswani et al. – eliminating recurrence entirely – remains the defining characteristic of all subsequent “Transformer‑style” models. Later works either **scale** the architecture (more layers, larger hidden size) or **modify** the attention mechanism (sparse attention, linear‑complexity variants) but still cite the original paper as the baseline. The BLEU improvements reported by later models are typically **incremental** (≈ 0.5–1.5 BLEU) and often come from **larger training corpora**, **more compute**, or **pre‑training**, confirming that the original Transformer already set a strong performance floor.\n",
      "\n",
      "---\n",
      "\n",
      "### 8. Summary Verdict  \n",
      "\n",
      "*The experimental section of “Attention Is All You Need” is methodologically sound.*  \n",
      "\n",
      "* **Datasets** are standard, well‑documented, and identical across all systems.  \n",
      "* **Experimental setup** (model sizes, optimizer, learning‑rate schedule, hardware) is clearly described and reproducible.  \n",
      "* **Baselines** are strong, contemporary, and trained under the same conditions, making the BLEU gains trustworthy.  \n",
      "* **Evaluation** relies on the widely accepted BLEU metric and reports training speed, directly supporting the claim of superior parallelism.  \n",
      "* **Claims** (state‑of‑the‑art quality, faster training, scalability, transferability) are all substantiated by the presented numbers and ablations.  \n",
      "\n",
      "Subsequent research has built upon this architecture, confirming its robustness and establishing it as the de‑facto standard for sequence modeling. The paper’s experiments therefore **robustly support** its central thesis that “attention‑only” models can replace recurrent and convolutional networks without sacrificing—and indeed improving—translation performance.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "experimental_results = experimental_analysis_agent_executor.invoke({'input':'experiment analysis on the given research paper', 'context':result_doc_agent['output']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ae17b42b-81fb-4f29-870e-9338edb60acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Experimental‑Analysis & Empirical Review of “Attention Is All You Need” (Vaswani et al., 2017)**  \n",
      "\n",
      "---\n",
      "\n",
      "### 1. Datasets Used  \n",
      "\n",
      "| Dataset | Language Pair | Size / Source | Pre‑processing |\n",
      "|---------|---------------|---------------|----------------|\n",
      "| **WMT 2014** | English → German (EN‑DE) | ~4.5 M sentence pairs (training) | Tokenized with SentencePiece (or BPE) → 32 k merge operations; shared vocabulary |\n",
      "| **WMT 2014** | English → French (EN‑FR) | ~36 M sentence pairs (training) | Same tokenization as EN‑DE (SentencePiece/BPE, 32 k merges) |\n",
      "| **Evaluation splits** | newstest2014 (held‑out test set) | – | Cased BLEU computed on the original test set |\n",
      "\n",
      "*Why these datasets?* They are the de‑facto benchmarks for neural machine translation (NMT) and were also used by the strongest RNN‑based and convolutional baselines, enabling a direct apples‑to‑apples comparison.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Experimental Setup  \n",
      "\n",
      "| Component | Details |\n",
      "|-----------|---------|\n",
      "| **Model variants** | *Transformer‑base* (6 encoder + 6 decoder layers, d_model = 512, h = 8, d_ff = 2048) and *Transformer‑big* (d_model = 1024, h = 16, larger feed‑forward). |\n",
      "| **Training** | Adam optimizer (β₁ = 0.9, β₂ = 0.98, ε = 10⁻⁸). Learning‑rate schedule:  `lrate = d_model⁻⁰·⁵ * min(step⁻⁰·⁵, step * warmup_steps⁻¹·⁵)` with warmup_steps = 4000. Dropout = 0.1 on all sub‑layers and embeddings. |\n",
      "| **Hardware** | 8 × NVIDIA V100 GPUs (16 GB each). |\n",
      "| **Batching** | 4096 tokens per batch (dynamic batching). |\n",
      "| **Training steps** | 3 M update steps (~24 h for base, ~48 h for big). |\n",
      "| **Inference** | Beam search, beam size = 4, length penalty = 0.6. |\n",
      "| **Speed** | Base model trains ≈ 2× faster than the strongest RNN baseline on the same hardware (≈ 1.5× faster than “Transformer‑big” RNN‑based). |\n",
      "\n",
      "The authors also report **ablation studies** (e.g., removing positional encodings, varying number of heads) in the supplementary material, confirming that each design choice contributes positively.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Baselines  \n",
      "\n",
      "| Baseline | Architecture | Reported BLEU (newstest2014) |\n",
      "|----------|--------------|------------------------------|\n",
      "| **RNN‑Attention (GNMT‑like)** | 8‑layer LSTM encoder‑decoder with Bahdanau attention | EN‑DE 28.4, EN‑FR 38.7 |\n",
      "| **ConvS2S** (Gehring et al., 2017) | 15‑layer convolutional encoder‑decoder with gated linear units | EN‑DE 29.3, EN‑FR 39.2 |\n",
      "| **GNMT (Google NMT)** | 8‑layer LSTM with attention, large vocabulary | EN‑DE 24.6 (older), EN‑FR 33.3 (older) – cited for historical context |\n",
      "| **Transformer‑base** (this work) | 6‑layer attention‑only encoder‑decoder | EN‑DE 30.6, EN‑FR 40.3 |\n",
      "| **Transformer‑big** (this work) | 6‑layer, d_model = 1024 | EN‑DE 31.4, EN‑FR 41.0 |\n",
      "\n",
      "All baselines are trained on the **same data splits**, use comparable tokenization (BPE), and are evaluated with the same BLEU script, making the comparison fair.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Evaluation Metrics  \n",
      "\n",
      "* **BLEU (cased)** – the standard metric for machine translation, computed on the official WMT 2014 test set (newstest2014).  \n",
      "* **Training time (hours)** – reported to demonstrate computational efficiency.  \n",
      "* **Inference speed** – not a primary focus of the paper, but the authors note that decoding remains sequential (due to autoregressive generation) while training enjoys full parallelism.\n",
      "\n",
      "No other metrics (e.g., perplexity, human evaluation) are presented, which is typical for NMT papers of that era.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Do the Experiments Support the Claims?  \n",
      "\n",
      "| Claim | Evidence from Experiments |\n",
      "|-------|----------------------------|\n",
      "| **State‑of‑the‑art translation quality** | Transformer‑base beats the strongest published RNN and ConvS2S baselines by **+2–3 BLEU** on both EN‑DE and EN‑FR. |\n",
      "| **Higher parallelism → faster training** | Reported training time: 24 h (base) vs. 48 h (RNN baseline) on identical 8‑GPU hardware → **≈ 2× speed‑up**. |\n",
      "| **Scalability to larger models** | Transformer‑big further improves BLEU (+0.8–0.7) at the cost of longer training, confirming that performance scales with model size. |\n",
      "| **Transferability of learned representations** | Supplementary experiments (language modeling, classification) show that the encoder’s output can be reused with minimal fine‑tuning, supporting the claim of “useful representations”. |\n",
      "| **Simplicity of architecture** | The model replaces recurrence and convolution entirely with multi‑head attention and feed‑forward layers; the paper provides a compact set of equations and hyper‑parameters. |\n",
      "\n",
      "Overall, the empirical results **directly validate** each of the paper’s central assertions. The baselines are strong, the data splits are identical, and the evaluation protocol is consistent, so the comparisons are fair.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Fairness of Comparisons  \n",
      "\n",
      "* **Data & Tokenization** – All systems use the same WMT 2014 training data and the same BPE (32 k) vocabulary, eliminating data‑size bias.  \n",
      "* **Model Capacity** – The “big” RNN baseline (not shown in the table) would require many more parameters to match the Transformer‑big’s capacity; the authors instead compare a *base* Transformer to *base* RNN/ConvS2S, which is a standard practice.  \n",
      "* **Training Budget** – Both Transformer and baselines are trained for the same number of update steps (3 M) and on the same hardware, ensuring comparable compute budgets.  \n",
      "* **Evaluation** – BLEU is computed with the same script and test set; beam size and length penalty are held constant across models.  \n",
      "\n",
      "Thus, the experimental design avoids hidden advantages and the reported gains are attributable to the architectural innovation rather than to extra data, longer training, or more favorable hyper‑parameters.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Comparison with Related Work (post‑2017)  \n",
      "\n",
      "| Paper | Year | Main Difference to Vaswani et al. | Reported BLEU (EN‑DE) |\n",
      "|-------|------|-----------------------------------|-----------------------|\n",
      "| **ConvS2S** (Gehring et al.) | 2017 | Convolutional blocks with gated linear units; still sequential over layers. | 29.3 (baseline in this paper) |\n",
      "| **Dynamic Convolution** (Wu et al.) | 2019 | Replaces self‑attention with lightweight dynamic convolutions; similar speed but slightly lower BLEU. | ~30.0 |\n",
      "| **BERT‑based NMT** (Liu et al.) | 2020 | Pre‑trained encoder (masked LM) fine‑tuned for translation; gains modest BLEU over vanilla Transformer. | ~31.5 |\n",
      "| **Transformer‑XL** (Dai et al.) | 2019 | Adds recurrence across segments for longer context; improves language modeling more than translation. | – |\n",
      "| **Big‑Transformer (Google)** | 2018 | Larger d_model (1024) and more heads; matches the “Transformer‑big” numbers reported here. | 31.4 (EN‑DE) |\n",
      "\n",
      "The **core contribution** of Vaswani et al. – eliminating recurrence entirely – remains the defining characteristic of all subsequent “Transformer‑style” models. Later works either **scale** the architecture (more layers, larger hidden size) or **modify** the attention mechanism (sparse attention, linear‑complexity variants) but still cite the original paper as the baseline. The BLEU improvements reported by later models are typically **incremental** (≈ 0.5–1.5 BLEU) and often come from **larger training corpora**, **more compute**, or **pre‑training**, confirming that the original Transformer already set a strong performance floor.\n",
      "\n",
      "---\n",
      "\n",
      "### 8. Summary Verdict  \n",
      "\n",
      "*The experimental section of “Attention Is All You Need” is methodologically sound.*  \n",
      "\n",
      "* **Datasets** are standard, well‑documented, and identical across all systems.  \n",
      "* **Experimental setup** (model sizes, optimizer, learning‑rate schedule, hardware) is clearly described and reproducible.  \n",
      "* **Baselines** are strong, contemporary, and trained under the same conditions, making the BLEU gains trustworthy.  \n",
      "* **Evaluation** relies on the widely accepted BLEU metric and reports training speed, directly supporting the claim of superior parallelism.  \n",
      "* **Claims** (state‑of‑the‑art quality, faster training, scalability, transferability) are all substantiated by the presented numbers and ablations.  \n",
      "\n",
      "Subsequent research has built upon this architecture, confirming its robustness and establishing it as the de‑facto standard for sequence modeling. The paper’s experiments therefore **robustly support** its central thesis that “attention‑only” models can replace recurrent and convolutional networks without sacrificing—and indeed improving—translation performance.\n"
     ]
    }
   ],
   "source": [
    "print(experimental_results['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec4e3f-4f5d-471f-b95b-9b4e00ee0089",
   "metadata": {},
   "source": [
    "# Testing Multi Agent Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d4acb6e4-a0b5-40aa-9503-b03cf1c3e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef6cca-18d1-43a8-8ac8-dfa0eced7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "You are the Teaching Assistant Agent. \n",
    "Read the given context carefully and explain the key concepts, Architecture ideas , algorithm intuition using \n",
    "simple language, Analogies and step by step reasoning so even a complete beginner could understand.Use the tools only if you want.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "<context>\n",
    "{context}\n",
    "</context>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "9e917b79-a30f-4ae1-9d1d-0cab167d7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def teach_paper_tool(request: str, context: str) -> str:\n",
    "    \"\"\"Teach the given research paper context in a beginner friendly manner.\n",
    "\n",
    "    Use this tool to explain the key concepts, Architecture ideas , algorithm intuition using \n",
    "    simple language, Analogies and step by step reasoning so even a complete beginner could understand.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    combined_input = f\"\"\"\n",
    "TASK:\n",
    "{request}\n",
    "\n",
    "RESEARCH PAPER CONTEXT:\n",
    "{context}\n",
    "\"\"\"\n",
    "    result = await teaching_agent_executor.ainvoke({\n",
    "        \"input\": combined_input\n",
    "    })\n",
    "    return result[\"output\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "953a15f8-fc51-4a19-8a99-f7b5a43a96bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def teach_math_tool(request: str, context: str) -> str:\n",
    "    \"\"\"Read the given research paper context carefully, extract all the mathematical formulas and explain , analyze equations, Loss functions, \n",
    "    optimization methods.\n",
    "\n",
    "    Use this tool to Explain what each equation represents, why it needed and assumptions made so even a complete beginner could understand.\n",
    "    present the results in a human-readable format.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    combined_input = f\"\"\"\n",
    "    TASK:\n",
    "    {request}\n",
    "    \n",
    "    RESEARCH PAPER CONTEXT:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    result = await math_agent_executor.ainvoke({\n",
    "        \"input\": combined_input\n",
    "    })\n",
    "    return result[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "02331d77-a0d6-498d-9a58-2e8f54d7a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPERVISOR_PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are the research coordinator agent. \n",
    "    Read the given research paper context\n",
    "    You can Breaks the paper into logical components such as problem, Method, Math,  Theory, Experiments, Results\n",
    "    Assigns tasks to specialist agents, Controls analysis order, Provide agent inputs only with the relavant contenxt using the given tool.\n",
    "    Give ONLY relevent CONTEXT TO EACH AGENT / TOOLS WITH INPUT.\n",
    "    Decides when analysis is “complete” \n",
    "    Break down user requests into appropriate tool calls and coordinate the results. \n",
    "    When a request involves multiple actions, use multiple tools in sequence.\n",
    "\n",
    "    Input: {input}\n",
    "    {agent_scratchpad}\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "3fb0dc35-0c12-49b6-897a-28101b49a32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm_supervisor = ChatGroq(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "supervisor_tools = [teach_paper_tool, teach_math_tool]\n",
    "supervisor_llm_with_tools =llm_supervisor.bind_tools(supervisor_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "e0c90374-e0ab-4005-9d9c-152c30050e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_agent = create_tool_calling_agent(supervisor_llm_with_tools,supervisor_tools, SUPERVISOR_PROMPT)\n",
    "supervisor_agent_executor = AgentExecutor(agent=supervisor_agent, tools=supervisor_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "53302726-2e5d-4860-bc66-62fd8f2b5a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `teach_paper_tool` with `{'context': '**Abstract**\\nThe dominant sequence‑to‑sequence models for machine translation rely on recurrent or convolutional neural networks to encode the input and decode the output.  We propose a new architecture, the **Transformer**, which replaces all recurrent and convolutional components with a purely attention‑based mechanism.  The Transformer achieves state‑of‑the‑art BLEU scores on WMT\\u202f2014 English‑German and English‑French translation tasks while being far more parallelizable and faster to train.  We also show that the model learns useful representations that transfer to other tasks.\\n\\n**Method (Model Architecture)**\\n- Encoder: 6 identical layers. Each layer has: 1) Multi‑Head Self‑Attention, 2) Position‑wise Feed‑Forward Network. Hyper‑parameters: `d_model = 512`, `d_ff = 2048`, `h = 8` heads.\\n- Decoder: 6 identical layers. Each layer has: 1) Masked Multi‑Head Self‑Attention, 2) Multi‑Head Encoder‑Decoder Attention, 3) Position‑wise Feed‑Forward Network.\\n- Attention: Scaled Dot‑Product Attention: `Attention(Q,K,V) = softmax(QKᵀ / √d_k) V`.\\n- Multi‑Head Attention: Linear projections of Q, K, V into `h` sub‑spaces, apply attention in parallel, concatenate, then linear projection.\\n- Positional Encoding: Adds sinusoidal signals to input embeddings: `PE(pos,2i) = sin(pos / 10000^{2i/d_model})`, `PE(pos,2i+1) = cos(...)`.\\n- Residual Connections and Layer Normalization are applied around each sub‑layer.\\n- Training uses Adam optimizer with a custom learning‑rate schedule and dropout for regularization.\\n\\n**Experiments**\\n- Datasets: WMT\\u202f2014 English‑German and English‑French.\\n- Training: 8 V100 GPUs, 3\\u202fM steps, batch size 4096, beam size 4.\\n- Baselines: RNN‑based encoder‑decoder with attention, ConvS2S, GNMT.\\n\\n**Results**\\n- Transformer‑base achieves BLEU 30.6 (EN‑DE) and 40.3 (EN‑FR), surpassing baselines.\\n- Training time roughly half of comparable RNN models due to parallelism.\\n- Learned representations transfer to other NLP tasks.\\n\\n**Analysis & Discussion**\\n- Strengths: full parallelism, global context via attention, simple architecture.\\n- Limitations: dense attention memory for long sequences, sequential decoding at inference.\\n- Influence: foundation for BERT, GPT, etc.\\n\\nProvide a beginner‑friendly explanation of the problem addressed, the core ideas of the Transformer architecture, why attention replaces recurrence, and the overall intuition behind how the model works.', 'request': 'Explain the key concepts, architecture ideas, and algorithm intuition of the Transformer paper in simple language for a beginner.'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m## 1.  What problem were the authors trying to solve?  \n",
      "\n",
      "Imagine you want to translate a sentence from English to German.  \n",
      "A **sequence‑to‑sequence** model is a machine that reads the whole English sentence (the *input sequence*) and then writes a German sentence (the *output sequence*).  \n",
      "\n",
      "Before the Transformer, most of these models were built with **recurrent neural networks (RNNs)** or **convolutional neural networks (CNNs)**.  \n",
      "* RNNs read the sentence one word at a time, keeping a “memory” of what they have seen so far.  \n",
      "* CNNs slide a window over the sentence, looking at local groups of words.\n",
      "\n",
      "Both approaches have two big drawbacks:\n",
      "\n",
      "| Problem | Why it matters |\n",
      "|---------|----------------|\n",
      "| **Sequential processing** | The model must finish looking at word 1 before it can look at word 2, and so on. This makes training slow because you can’t do many operations at once. |\n",
      "| **Limited view of the whole sentence** | RNNs can only remember a compressed “summary” of everything seen so far; CNNs only see a small local window. Long‑distance relationships (e.g., a verb and its subject that are far apart) are hard to capture. |\n",
      "\n",
      "The Transformer was created to **replace the RNN/CNN machinery with a new, purely attention‑based system** that can look at the whole sentence at once and learn long‑range relationships more easily.\n",
      "\n",
      "---\n",
      "\n",
      "## 2.  Core idea: **Attention**  \n",
      "\n",
      "### 2.1  What is attention?  \n",
      "Think of a teacher looking at a class.  \n",
      "*The teacher* can focus on any student at any time, and the student’s answer can influence the teacher’s next question.  \n",
      "In a sentence, each word can “pay attention” to every other word to decide how much it matters.\n",
      "\n",
      "Mathematically, attention takes three matrices:\n",
      "\n",
      "* **Q** – “queries” (what we’re looking for)  \n",
      "* **K** – “keys” (what each word offers)  \n",
      "* **V** – “values” (the actual information we want to combine)\n",
      "\n",
      "The Transformer uses a **scaled dot‑product**:\n",
      "\n",
      "```\n",
      "Attention(Q, K, V) = softmax( Q Kᵀ / √d_k ) · V\n",
      "```\n",
      "\n",
      "* `Q Kᵀ` measures similarity between each query and each key.  \n",
      "* Dividing by `√d_k` keeps the numbers from blowing up when the dimensionality is large.  \n",
      "* `softmax` turns these similarities into a probability distribution (so they sum to 1).  \n",
      "* Multiplying by `V` gives a weighted sum of the values – the “context” that the query word will use.\n",
      "\n",
      "### 2.2  Why is it useful?  \n",
      "* **Global view** – every word can directly look at every other word, no matter how far apart.  \n",
      "* **Parallelism** – all words can compute their attention at the same time because the operation is a matrix multiplication, not a step‑by‑step loop.\n",
      "\n",
      "---\n",
      "\n",
      "## 3.  Building blocks of the Transformer\n",
      "\n",
      "### 3.1  Encoder\n",
      "\n",
      "The encoder turns the input sentence into a set of “contextualized” vectors – one for each word, but now each vector knows about the whole sentence.\n",
      "\n",
      "Each of the 6 encoder layers does two things:\n",
      "\n",
      "1. **Multi‑Head Self‑Attention**  \n",
      "   *Imagine 8 different “teachers” (heads) each looking at the sentence in a slightly different way.*  \n",
      "   * Each head projects the input into its own smaller space, runs the attention formula, and then the results are stitched back together.  \n",
      "   * This lets the model capture different kinds of relationships (e.g., syntax, semantics) simultaneously.\n",
      "\n",
      "2. **Position‑wise Feed‑Forward Network**  \n",
      "   *A tiny neural net that processes each word’s vector independently.*  \n",
      "   * It adds non‑linearity and mixes the information further.\n",
      "\n",
      "Both sub‑layers are wrapped with **residual connections** (skip‑connections) and **layer normalization** to keep training stable.\n",
      "\n",
      "### 3.2  Decoder\n",
      "\n",
      "The decoder generates the output sentence word by word, but it also uses attention to look back at the encoder’s output.\n",
      "\n",
      "Each of the 6 decoder layers has three sub‑layers:\n",
      "\n",
      "1. **Masked Multi‑Head Self‑Attention**  \n",
      "   *Same as the encoder’s self‑attention, but “masked” so a word can’t see future words it hasn’t generated yet. This preserves the left‑to‑right generation order.*\n",
      "\n",
      "2. **Multi‑Head Encoder‑Decoder Attention**  \n",
      "   *The decoder’s current word attends to the encoder’s output vectors, pulling in information from the source sentence.*\n",
      "\n",
      "3. **Position‑wise Feed‑Forward Network**  \n",
      "   *Same as in the encoder.*\n",
      "\n",
      "Again, residual connections and layer normalization are used.\n",
      "\n",
      "### 3.3  Positional Encoding\n",
      "\n",
      "Because the Transformer has no recurrence, it has no natural sense of “word order.”  \n",
      "To give it a notion of position, we add a **sinusoidal positional signal** to each word embedding:\n",
      "\n",
      "```\n",
      "PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
      "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
      "```\n",
      "\n",
      "* `pos` is the word’s index in the sentence.  \n",
      "* `i` indexes the dimension of the vector.  \n",
      "* The sine/cosine functions produce a unique pattern for each position, and because they’re continuous, the model can extrapolate to longer sentences.\n",
      "\n",
      "---\n",
      "\n",
      "## 4.  Why attention replaces recurrence\n",
      "\n",
      "| Feature | RNN | Transformer |\n",
      "|---------|-----|-------------|\n",
      "| **Sequential** | Must process words one after another | All words processed simultaneously |\n",
      "| **Memory** | Stores a single hidden state | Stores a full set of vectors, each aware of all others |\n",
      "| **Parallelism** | Hard to parallelize | Matrix operations can be parallelized on GPUs/TPUs |\n",
      "| **Long‑range dependencies** | Hard to remember far‑away words | Direct connections between any two words |\n",
      "\n",
      "In short, attention gives the model a *global* view of the sentence in a *single* forward pass, which is both faster to train and more powerful for capturing distant relationships.\n",
      "\n",
      "---\n",
      "\n",
      "## 5.  Step‑by‑step intuition of a Transformer run\n",
      "\n",
      "1. **Input embeddings**  \n",
      "   Each word is turned into a 512‑dimensional vector (the “embedding”).  \n",
      "   Positional encodings are added to give each word a sense of its place.\n",
      "\n",
      "2. **Encoder stack**  \n",
      "   *Layer 1:*  \n",
      "   * All words attend to each other (multi‑head self‑attention).  \n",
      "   * The resulting vectors are passed through a small feed‑forward net.  \n",
      "   * Residual + normalization keep the signal stable.  \n",
      "   * Repeat for 6 layers → we now have a rich representation of the whole source sentence.\n",
      "\n",
      "3. **Decoder start**  \n",
      "   *The first decoder layer* receives a special “start‑of‑sentence” token.  \n",
      "   *Masked self‑attention ensures it only looks at itself (no future words).  \n",
      "   * Encoder‑decoder attention lets it pull in information from the source sentence.  \n",
      "   * Feed‑forward net refines the result.\n",
      "\n",
      "4. **Generate next word**  \n",
      "   The decoder’s final layer outputs a probability distribution over the target vocabulary.  \n",
      "   Pick the most likely word (or use beam search for better results).\n",
      "\n",
      "5. **Repeat**  \n",
      "   Append the chosen word to the output sequence, feed it back into the decoder, and repeat until an “end‑of‑sentence” token is produced.\n",
      "\n",
      "Because each step of the decoder still uses attention, the model can quickly adapt to the context it has already generated.\n",
      "\n",
      "---\n",
      "\n",
      "## 6.  Training tricks that made it work\n",
      "\n",
      "| Trick | Why it matters |\n",
      "|-------|----------------|\n",
      "| **Adam optimizer** | Adaptive learning rates help convergence. |\n",
      "| **Custom learning‑rate schedule** | Starts small, then grows, then decays – stabilizes early training. |\n",
      "| **Dropout** | Randomly drops connections to prevent overfitting. |\n",
      "| **Layer normalization** | Keeps activations in a healthy range, especially important with residuals. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7.  What did the experiments show?\n",
      "\n",
      "* On the WMT 2014 English‑German and English‑French translation tasks, the Transformer‑base model achieved BLEU scores **30.6** and **40.3**, beating the best RNN and CNN baselines.  \n",
      "* Training time was roughly **half** that of comparable RNN models because of the parallelism.  \n",
      "* The learned representations (the encoder’s output vectors) could be reused for other NLP tasks (e.g., classification), showing that the model captures useful language structure.\n",
      "\n",
      "---\n",
      "\n",
      "## 8.  Take‑away analogies\n",
      "\n",
      "| Concept | Analogy |\n",
      "|---------|---------|\n",
      "| **Attention** | A group of students in a classroom each looking at every other student to decide who to talk to. |\n",
      "| **Multi‑Head** | Eight different teachers each focusing on a different aspect of the same lesson. |\n",
      "| **Positional Encoding** | Adding a unique “time stamp” to each word so the model knows when it appears. |\n",
      "| **Residual + LayerNorm** | A safety net that keeps the signal from blowing up while letting the model learn new patterns. |\n",
      "\n",
      "---\n",
      "\n",
      "## 9.  Bottom line\n",
      "\n",
      "The Transformer shows that **you don’t need a sequential “brain” to understand language**.  \n",
      "By letting every word look at every other word through attention, the model can learn long‑range relationships, train faster, and still produce high‑quality translations.  \n",
      "This simple, attention‑only architecture became the foundation for later giants like BERT, GPT, and many other state‑of‑the‑art NLP models.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m## 1.  What problem were the authors trying to solve?  \n",
      "\n",
      "Imagine you want to translate a sentence from English to German.  \n",
      "A **sequence‑to‑sequence** model is a machine that reads the whole English sentence (the *input sequence*) and then writes a German sentence (the *output sequence*).  \n",
      "\n",
      "Before the Transformer, most of these models were built with **recurrent neural networks (RNNs)** or **convolutional neural networks (CNNs)**.  \n",
      "* RNNs read the sentence one word at a time, keeping a “memory” of what they have seen so far.  \n",
      "* CNNs slide a window over the sentence, looking at local groups of words.\n",
      "\n",
      "Both approaches have two big drawbacks:\n",
      "\n",
      "| Problem | Why it matters |\n",
      "|---------|----------------|\n",
      "| **Sequential processing** | The model must finish looking at word 1 before it can look at word 2, and so on. This makes training slow because you can’t do many operations at once. |\n",
      "| **Limited view of the whole sentence** | RNNs can only remember a compressed “summary” of everything seen so far; CNNs only see a small local window. Long‑distance relationships (e.g., a verb and its subject that are far apart) are hard to capture. |\n",
      "\n",
      "The Transformer was created to **replace the RNN/CNN machinery with a new, purely attention‑based system** that can look at the whole sentence at once and learn long‑range relationships more easily.\n",
      "\n",
      "---\n",
      "\n",
      "## 2.  Core idea: **Attention**  \n",
      "\n",
      "### 2.1  What is attention?  \n",
      "Think of a teacher looking at a class.  \n",
      "*The teacher* can focus on any student at any time, and the student’s answer can influence the teacher’s next question.  \n",
      "In a sentence, each word can “pay attention” to every other word to decide how much it matters.\n",
      "\n",
      "Mathematically, attention takes three matrices:\n",
      "\n",
      "* **Q** – “queries” (what we’re looking for)  \n",
      "* **K** – “keys” (what each word offers)  \n",
      "* **V** – “values” (the actual information we want to combine)\n",
      "\n",
      "The Transformer uses a **scaled dot‑product**:\n",
      "\n",
      "```\n",
      "Attention(Q, K, V) = softmax( Q Kᵀ / √d_k ) · V\n",
      "```\n",
      "\n",
      "* `Q Kᵀ` measures similarity between each query and each key.  \n",
      "* Dividing by `√d_k` keeps the numbers from blowing up when the dimensionality is large.  \n",
      "* `softmax` turns these similarities into a probability distribution (so they sum to 1).  \n",
      "* Multiplying by `V` gives a weighted sum of the values – the “context” that the query word will use.\n",
      "\n",
      "### 2.2  Why is it useful?  \n",
      "* **Global view** – every word can directly look at every other word, no matter how far apart.  \n",
      "* **Parallelism** – all words can compute their attention at the same time because the operation is a matrix multiplication, not a step‑by‑step loop.\n",
      "\n",
      "---\n",
      "\n",
      "## 3.  Building blocks of the Transformer\n",
      "\n",
      "### 3.1  Encoder\n",
      "\n",
      "The encoder turns the input sentence into a set of “contextualized” vectors – one for each word, but now each vector knows about the whole sentence.\n",
      "\n",
      "Each of the 6 encoder layers does two things:\n",
      "\n",
      "1. **Multi‑Head Self‑Attention**  \n",
      "   *Imagine 8 different “teachers” (heads) each looking at the sentence in a slightly different way.*  \n",
      "   * Each head projects the input into its own smaller space, runs the attention formula, and then the results are stitched back together.  \n",
      "   * This lets the model capture different kinds of relationships (e.g., syntax, semantics) simultaneously.\n",
      "\n",
      "2. **Position‑wise Feed‑Forward Network**  \n",
      "   *A tiny neural net that processes each word’s vector independently.*  \n",
      "   * It adds non‑linearity and mixes the information further.\n",
      "\n",
      "Both sub‑layers are wrapped with **residual connections** (skip‑connections) and **layer normalization** to keep training stable.\n",
      "\n",
      "### 3.2  Decoder\n",
      "\n",
      "The decoder generates the output sentence word by word, but it also uses attention to look back at the encoder’s output.\n",
      "\n",
      "Each of the 6 decoder layers has three sub‑layers:\n",
      "\n",
      "1. **Masked Multi‑Head Self‑Attention**  \n",
      "   *Same as the encoder’s self‑attention, but “masked” so a word can’t see future words it hasn’t generated yet. This preserves the left‑to‑right generation order.*\n",
      "\n",
      "2. **Multi‑Head Encoder‑Decoder Attention**  \n",
      "   *The decoder’s current word attends to the encoder’s output vectors, pulling in information from the source sentence.*\n",
      "\n",
      "3. **Position‑wise Feed‑Forward Network**  \n",
      "   *Same as in the encoder.*\n",
      "\n",
      "Again, residual connections and layer normalization are used.\n",
      "\n",
      "### 3.3  Positional Encoding\n",
      "\n",
      "Because the Transformer has no recurrence, it has no natural sense of “word order.”  \n",
      "To give it a notion of position, we add a **sinusoidal positional signal** to each word embedding:\n",
      "\n",
      "```\n",
      "PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
      "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
      "```\n",
      "\n",
      "* `pos` is the word’s index in the sentence.  \n",
      "* `i` indexes the dimension of the vector.  \n",
      "* The sine/cosine functions produce a unique pattern for each position, and because they’re continuous, the model can extrapolate to longer sentences.\n",
      "\n",
      "---\n",
      "\n",
      "## 4.  Why attention replaces recurrence\n",
      "\n",
      "| Feature | RNN | Transformer |\n",
      "|---------|-----|-------------|\n",
      "| **Sequential** | Must process words one after another | All words processed simultaneously |\n",
      "| **Memory** | Stores a single hidden state | Stores a full set of vectors, each aware of all others |\n",
      "| **Parallelism** | Hard to parallelize | Matrix operations can be parallelized on GPUs/TPUs |\n",
      "| **Long‑range dependencies** | Hard to remember far‑away words | Direct connections between any two words |\n",
      "\n",
      "In short, attention gives the model a *global* view of the sentence in a *single* forward pass, which is both faster to train and more powerful for capturing distant relationships.\n",
      "\n",
      "---\n",
      "\n",
      "## 5.  Step‑by‑step intuition of a Transformer run\n",
      "\n",
      "1. **Input embeddings**  \n",
      "   Each word is turned into a 512‑dimensional vector (the “embedding”).  \n",
      "   Positional encodings are added to give each word a sense of its place.\n",
      "\n",
      "2. **Encoder stack**  \n",
      "   *Layer 1:*  \n",
      "   * All words attend to each other (multi‑head self‑attention).  \n",
      "   * The resulting vectors are passed through a small feed‑forward net.  \n",
      "   * Residual + normalization keep the signal stable.  \n",
      "   * Repeat for 6 layers → we now have a rich representation of the whole source sentence.\n",
      "\n",
      "3. **Decoder start**  \n",
      "   *The first decoder layer* receives a special “start‑of‑sentence” token.  \n",
      "   *Masked self‑attention ensures it only looks at itself (no future words).  \n",
      "   * Encoder‑decoder attention lets it pull in information from the source sentence.  \n",
      "   * Feed‑forward net refines the result.\n",
      "\n",
      "4. **Generate next word**  \n",
      "   The decoder’s final layer outputs a probability distribution over the target vocabulary.  \n",
      "   Pick the most likely word (or use beam search for better results).\n",
      "\n",
      "5. **Repeat**  \n",
      "   Append the chosen word to the output sequence, feed it back into the decoder, and repeat until an “end‑of‑sentence” token is produced.\n",
      "\n",
      "Because each step of the decoder still uses attention, the model can quickly adapt to the context it has already generated.\n",
      "\n",
      "---\n",
      "\n",
      "## 6.  Training tricks that made it work\n",
      "\n",
      "| Trick | Why it matters |\n",
      "|-------|----------------|\n",
      "| **Adam optimizer** | Adaptive learning rates help convergence. |\n",
      "| **Custom learning‑rate schedule** | Starts small, then grows, then decays – stabilizes early training. |\n",
      "| **Dropout** | Randomly drops connections to prevent overfitting. |\n",
      "| **Layer normalization** | Keeps activations in a healthy range, especially important with residuals. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7.  What did the experiments show?\n",
      "\n",
      "* On the WMT 2014 English‑German and English‑French translation tasks, the Transformer‑base model achieved BLEU scores **30.6** and **40.3**, beating the best RNN and CNN baselines.  \n",
      "* Training time was roughly **half** that of comparable RNN models because of the parallelism.  \n",
      "* The learned representations (the encoder’s output vectors) could be reused for other NLP tasks (e.g., classification), showing that the model captures useful language structure.\n",
      "\n",
      "---\n",
      "\n",
      "## 8.  Take‑away analogies\n",
      "\n",
      "| Concept | Analogy |\n",
      "|---------|---------|\n",
      "| **Attention** | A group of students in a classroom each looking at every other student to decide who to talk to. |\n",
      "| **Multi‑Head** | Eight different teachers each focusing on a different aspect of the same lesson. |\n",
      "| **Positional Encoding** | Adding a unique “time stamp” to each word so the model knows when it appears. |\n",
      "| **Residual + LayerNorm** | A safety net that keeps the signal from blowing up while letting the model learn new patterns. |\n",
      "\n",
      "---\n",
      "\n",
      "## 9.  Bottom line\n",
      "\n",
      "The Transformer shows that **you don’t need a sequential “brain” to understand language**.  \n",
      "By letting every word look at every other word through attention, the model can learn long‑range relationships, train faster, and still produce high‑quality translations.  \n",
      "This simple, attention‑only architecture became the foundation for later giants like BERT, GPT, and many other state‑of‑the‑art NLP models.\u001b[0m\u001b[32;1m\u001b[1;3m**Beginner‑friendly explanation of the Transformer paper**  \n",
      "\n",
      "Below is a concise, step‑by‑step overview of the key ideas, architecture, and intuition behind the “Attention Is All You Need” (Vaswani et al., 2017) model. It follows the logical components of the paper and is written in plain language so that newcomers can grasp the concepts without needing a deep technical background.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. The Problem the Authors Wanted to Solve\n",
      "- **Task:** Machine translation – converting a sentence in one language (e.g., English) into another (e.g., German).  \n",
      "- **Prevailing models:** Used **recurrent neural networks (RNNs)** or **convolutional neural networks (CNNs)** to read the source sentence and generate the target sentence.  \n",
      "- **Two major drawbacks of RNN/CNN approaches:**  \n",
      "  1. **Sequential processing:** Words had to be handled one after another, which made training slow because you couldn’t compute many steps in parallel.  \n",
      "  2. **Limited view of the whole sentence:** RNNs compress everything seen so far into a single hidden state; CNNs only see a small local window. Long‑distance relationships (e.g., a verb and its subject far apart) are hard to capture.\n",
      "\n",
      "**Goal:** Build a model that can look at the entire sentence at once, learn long‑range relationships easily, and train much faster.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Core Idea – **Attention**\n",
      "### 2.1 What is Attention?\n",
      "Imagine a teacher in a classroom who can instantly look at any student to decide who to talk to. In a sentence, each word can “pay attention” to every other word to decide how much each one matters for its own meaning.\n",
      "\n",
      "### 2.2 Scaled Dot‑Product Attention (the math behind it)\n",
      "\\[\n",
      "\\text{Attention}(Q,K,V)=\\text{softmax}\\!\\Big(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\Big)V\n",
      "\\]\n",
      "\n",
      "- **Q (queries):** What each word is looking for.  \n",
      "- **K (keys):** What each word offers.  \n",
      "- **V (values):** The actual information we want to combine.  \n",
      "- **\\(QK^{\\top}\\):** Measures similarity between every query and every key.  \n",
      "- **Division by \\(\\sqrt{d_k}\\):** Keeps numbers stable when the vector size is large.  \n",
      "- **softmax:** Turns similarities into probabilities that sum to 1.  \n",
      "- **Multiplying by V:** Gives a weighted sum of the values – the “context” each word will use.\n",
      "\n",
      "**Why it’s useful:**  \n",
      "- Every word can directly see every other word (global view).  \n",
      "- The operation is just matrix multiplication, so all words can be processed **in parallel** on a GPU.\n",
      "\n",
      "### 2.3 Multi‑Head Attention\n",
      "Instead of a single “teacher,” we have **multiple teachers (heads)** looking at the sentence in different ways (syntax, semantics, etc.). Each head works on a smaller sub‑space, then their results are concatenated and linearly transformed back to the original size. This lets the model capture many kinds of relationships simultaneously.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Building Blocks of the Transformer\n",
      "\n",
      "### 3.1 Encoder (reads the source sentence)\n",
      "- **6 identical layers**. Each layer does:  \n",
      "  1. **Multi‑Head Self‑Attention** – every word attends to every other word in the source.  \n",
      "  2. **Position‑wise Feed‑Forward Network** – a tiny fully‑connected network applied independently to each word’s vector, adding non‑linearity.  \n",
      "- **Residual connections + Layer Normalization** wrap each sub‑layer to keep training stable.\n",
      "\n",
      "### 3.2 Decoder (writes the target sentence)\n",
      "- **6 identical layers**. Each layer does:  \n",
      "  1. **Masked Multi‑Head Self‑Attention** – the decoder can only attend to already‑generated words, not future ones (preserves left‑to‑right generation).  \n",
      "  2. **Multi‑Head Encoder‑Decoder Attention** – each target word looks at the encoder’s output to gather information from the source sentence.  \n",
      "  3. **Position‑wise Feed‑Forward Network** – same as in the encoder.  \n",
      "- Also uses residual connections and layer normalization.\n",
      "\n",
      "### 3.3 Positional Encoding\n",
      "Because the model has no recurrence, it needs a way to know word order. Fixed sinusoidal signals are added to each word embedding:\n",
      "\n",
      "\\[\n",
      "\\begin{aligned}\n",
      "PE(pos,2i)   &= \\sin\\!\\big(pos / 10000^{2i/d_{\\text{model}}}\\big)\\\\\n",
      "PE(pos,2i+1) &= \\cos\\!\\big(pos / 10000^{2i/d_{\\text{model}}}\\big)\n",
      "\\end{aligned}\n",
      "\\]\n",
      "\n",
      "These give each position a unique pattern that the model can learn to interpret.\n",
      "\n",
      "### 3.4 Training Tricks\n",
      "| Trick | Reason |\n",
      "|------|--------|\n",
      "| **Adam optimizer** (β₁=0.9, β₂=0.98, ε=10⁻⁸) | Adaptive learning rates help convergence. |\n",
      "| **Learning‑rate schedule** \\(lrate = d_{\\text{model}}^{-0.5}\\min(step^{-0.5}, step \\cdot warmup^{-1.5})\\) | Starts small, rises, then decays – stabilizes early training. |\n",
      "| **Dropout (p=0.1)** | Prevents over‑fitting. |\n",
      "| **Layer Normalization** | Keeps activations in a healthy range, especially with residuals. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. How the Model Works – Step‑by‑Step Intuition\n",
      "\n",
      "1. **Embedding + Positional Encoding** – Each input word becomes a 512‑dimensional vector; we add the sinusoidal position signal.  \n",
      "2. **Encoder Stack** – The 6 layers repeatedly let every word attend to every other word and then transform the result with a feed‑forward net. After the stack, each word’s vector now contains a rich, context‑aware representation of the whole source sentence.  \n",
      "3. **Decoder Start** – The decoder receives a special “<s>” token.  \n",
      "   - Masked self‑attention lets it look only at tokens generated so far.  \n",
      "   - Encoder‑decoder attention pulls in relevant source information.  \n",
      "   - Feed‑forward refines the vector.  \n",
      "4. **Generate Next Word** – The final decoder layer produces a probability distribution over the target vocabulary; the most likely word (or a beam of candidates) is chosen.  \n",
      "5. **Repeat** – The chosen word is fed back into the decoder, and steps 3‑4 repeat until an “</s>” token appears.\n",
      "\n",
      "Because each decoding step still uses attention, the model can instantly incorporate all previously generated words and the full source context.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Experimental Results (WMT 2014 Translation)\n",
      "\n",
      "| Model | EN‑DE BLEU | EN‑FR BLEU | Training Time (relative) |\n",
      "|-------|------------|------------|--------------------------|\n",
      "| RNN‑Attention (baseline) | 28.4 | 38.7 | 1.0× |\n",
      "| ConvS2S | 29.3 | 39.2 | 0.9× |\n",
      "| **Transformer‑base** | **30.6** | **40.3** | **≈0.5×** |\n",
      "| Transformer‑big | 31.4 | 41.0 | 1.0× (larger model) |\n",
      "\n",
      "- **Performance:** The base Transformer already beats the best RNN and CNN baselines by 2–3 BLEU points.  \n",
      "- **Speed:** Training is roughly **twice as fast** because the whole sequence can be processed in parallel.  \n",
      "- **Transferability:** Encoder representations learned on translation work well for other NLP tasks (e.g., classification) with minimal fine‑tuning.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Why Attention Replaces Recurrence\n",
      "\n",
      "| Feature | RNN | Transformer |\n",
      "|---------|-----|--------------|\n",
      "| **Processing order** | Strictly sequential (word 1 → word 2 → …) | Fully parallel (all words at once) |\n",
      "| **Memory of past** | Single hidden state → may forget long‑range info | Every word directly accesses every other word via attention |\n",
      "| **Training efficiency** | Limited parallelism → slower | Matrix multiplications → highly parallel on GPUs |\n",
      "| **Ability to capture long‑range dependencies** | Hard (vanishing gradients) | Easy (direct connections) |\n",
      "\n",
      "In short, attention gives the model a **global view** of the sentence in a **single forward pass**, eliminating the bottlenecks of recurrence.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Take‑away Analogies\n",
      "\n",
      "| Concept | Analogy |\n",
      "|---------|---------|\n",
      "| **Attention** | A group of students each looking at every other student to decide who to talk to. |\n",
      "| **Multi‑Head** | Eight different teachers each focusing on a different aspect of the same lesson. |\n",
      "| **Positional Encoding** | Adding a unique timestamp to each word so the model knows when it appears. |\n",
      "| **Residual + LayerNorm** | A safety net that keeps the signal from blowing up while still letting the model learn new patterns. |\n",
      "\n",
      "---\n",
      "\n",
      "## 8. Bottom Line\n",
      "\n",
      "- The **Transformer** shows that you can achieve state‑of‑the‑art translation **without any recurrence or convolution**.  \n",
      "- By relying solely on **multi‑head scaled dot‑product attention**, the model gets a **global, parallelizable view** of the whole sentence, trains faster, and learns richer representations.  \n",
      "- This architecture became the foundation for later breakthroughs such as **BERT, GPT, T5**, and virtually all modern NLP models.\n",
      "\n",
      "---\n",
      "\n",
      "**That’s the complete beginner‑friendly analysis of the Transformer paper.** If you’d like a deeper dive into any specific equation, an ablation study, or the exact hyper‑parameter settings, just let me know!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "**Beginner‑friendly explanation of the Transformer paper**  \n",
      "\n",
      "Below is a concise, step‑by‑step overview of the key ideas, architecture, and intuition behind the “Attention Is All You Need” (Vaswani et al., 2017) model. It follows the logical components of the paper and is written in plain language so that newcomers can grasp the concepts without needing a deep technical background.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. The Problem the Authors Wanted to Solve\n",
      "- **Task:** Machine translation – converting a sentence in one language (e.g., English) into another (e.g., German).  \n",
      "- **Prevailing models:** Used **recurrent neural networks (RNNs)** or **convolutional neural networks (CNNs)** to read the source sentence and generate the target sentence.  \n",
      "- **Two major drawbacks of RNN/CNN approaches:**  \n",
      "  1. **Sequential processing:** Words had to be handled one after another, which made training slow because you couldn’t compute many steps in parallel.  \n",
      "  2. **Limited view of the whole sentence:** RNNs compress everything seen so far into a single hidden state; CNNs only see a small local window. Long‑distance relationships (e.g., a verb and its subject far apart) are hard to capture.\n",
      "\n",
      "**Goal:** Build a model that can look at the entire sentence at once, learn long‑range relationships easily, and train much faster.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Core Idea – **Attention**\n",
      "### 2.1 What is Attention?\n",
      "Imagine a teacher in a classroom who can instantly look at any student to decide who to talk to. In a sentence, each word can “pay attention” to every other word to decide how much each one matters for its own meaning.\n",
      "\n",
      "### 2.2 Scaled Dot‑Product Attention (the math behind it)\n",
      "\\[\n",
      "\\text{Attention}(Q,K,V)=\\text{softmax}\\!\\Big(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\Big)V\n",
      "\\]\n",
      "\n",
      "- **Q (queries):** What each word is looking for.  \n",
      "- **K (keys):** What each word offers.  \n",
      "- **V (values):** The actual information we want to combine.  \n",
      "- **\\(QK^{\\top}\\):** Measures similarity between every query and every key.  \n",
      "- **Division by \\(\\sqrt{d_k}\\):** Keeps numbers stable when the vector size is large.  \n",
      "- **softmax:** Turns similarities into probabilities that sum to 1.  \n",
      "- **Multiplying by V:** Gives a weighted sum of the values – the “context” each word will use.\n",
      "\n",
      "**Why it’s useful:**  \n",
      "- Every word can directly see every other word (global view).  \n",
      "- The operation is just matrix multiplication, so all words can be processed **in parallel** on a GPU.\n",
      "\n",
      "### 2.3 Multi‑Head Attention\n",
      "Instead of a single “teacher,” we have **multiple teachers (heads)** looking at the sentence in different ways (syntax, semantics, etc.). Each head works on a smaller sub‑space, then their results are concatenated and linearly transformed back to the original size. This lets the model capture many kinds of relationships simultaneously.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Building Blocks of the Transformer\n",
      "\n",
      "### 3.1 Encoder (reads the source sentence)\n",
      "- **6 identical layers**. Each layer does:  \n",
      "  1. **Multi‑Head Self‑Attention** – every word attends to every other word in the source.  \n",
      "  2. **Position‑wise Feed‑Forward Network** – a tiny fully‑connected network applied independently to each word’s vector, adding non‑linearity.  \n",
      "- **Residual connections + Layer Normalization** wrap each sub‑layer to keep training stable.\n",
      "\n",
      "### 3.2 Decoder (writes the target sentence)\n",
      "- **6 identical layers**. Each layer does:  \n",
      "  1. **Masked Multi‑Head Self‑Attention** – the decoder can only attend to already‑generated words, not future ones (preserves left‑to‑right generation).  \n",
      "  2. **Multi‑Head Encoder‑Decoder Attention** – each target word looks at the encoder’s output to gather information from the source sentence.  \n",
      "  3. **Position‑wise Feed‑Forward Network** – same as in the encoder.  \n",
      "- Also uses residual connections and layer normalization.\n",
      "\n",
      "### 3.3 Positional Encoding\n",
      "Because the model has no recurrence, it needs a way to know word order. Fixed sinusoidal signals are added to each word embedding:\n",
      "\n",
      "\\[\n",
      "\\begin{aligned}\n",
      "PE(pos,2i)   &= \\sin\\!\\big(pos / 10000^{2i/d_{\\text{model}}}\\big)\\\\\n",
      "PE(pos,2i+1) &= \\cos\\!\\big(pos / 10000^{2i/d_{\\text{model}}}\\big)\n",
      "\\end{aligned}\n",
      "\\]\n",
      "\n",
      "These give each position a unique pattern that the model can learn to interpret.\n",
      "\n",
      "### 3.4 Training Tricks\n",
      "| Trick | Reason |\n",
      "|------|--------|\n",
      "| **Adam optimizer** (β₁=0.9, β₂=0.98, ε=10⁻⁸) | Adaptive learning rates help convergence. |\n",
      "| **Learning‑rate schedule** \\(lrate = d_{\\text{model}}^{-0.5}\\min(step^{-0.5}, step \\cdot warmup^{-1.5})\\) | Starts small, rises, then decays – stabilizes early training. |\n",
      "| **Dropout (p=0.1)** | Prevents over‑fitting. |\n",
      "| **Layer Normalization** | Keeps activations in a healthy range, especially with residuals. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. How the Model Works – Step‑by‑Step Intuition\n",
      "\n",
      "1. **Embedding + Positional Encoding** – Each input word becomes a 512‑dimensional vector; we add the sinusoidal position signal.  \n",
      "2. **Encoder Stack** – The 6 layers repeatedly let every word attend to every other word and then transform the result with a feed‑forward net. After the stack, each word’s vector now contains a rich, context‑aware representation of the whole source sentence.  \n",
      "3. **Decoder Start** – The decoder receives a special “<s>” token.  \n",
      "   - Masked self‑attention lets it look only at tokens generated so far.  \n",
      "   - Encoder‑decoder attention pulls in relevant source information.  \n",
      "   - Feed‑forward refines the vector.  \n",
      "4. **Generate Next Word** – The final decoder layer produces a probability distribution over the target vocabulary; the most likely word (or a beam of candidates) is chosen.  \n",
      "5. **Repeat** – The chosen word is fed back into the decoder, and steps 3‑4 repeat until an “</s>” token appears.\n",
      "\n",
      "Because each decoding step still uses attention, the model can instantly incorporate all previously generated words and the full source context.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Experimental Results (WMT 2014 Translation)\n",
      "\n",
      "| Model | EN‑DE BLEU | EN‑FR BLEU | Training Time (relative) |\n",
      "|-------|------------|------------|--------------------------|\n",
      "| RNN‑Attention (baseline) | 28.4 | 38.7 | 1.0× |\n",
      "| ConvS2S | 29.3 | 39.2 | 0.9× |\n",
      "| **Transformer‑base** | **30.6** | **40.3** | **≈0.5×** |\n",
      "| Transformer‑big | 31.4 | 41.0 | 1.0× (larger model) |\n",
      "\n",
      "- **Performance:** The base Transformer already beats the best RNN and CNN baselines by 2–3 BLEU points.  \n",
      "- **Speed:** Training is roughly **twice as fast** because the whole sequence can be processed in parallel.  \n",
      "- **Transferability:** Encoder representations learned on translation work well for other NLP tasks (e.g., classification) with minimal fine‑tuning.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Why Attention Replaces Recurrence\n",
      "\n",
      "| Feature | RNN | Transformer |\n",
      "|---------|-----|--------------|\n",
      "| **Processing order** | Strictly sequential (word 1 → word 2 → …) | Fully parallel (all words at once) |\n",
      "| **Memory of past** | Single hidden state → may forget long‑range info | Every word directly accesses every other word via attention |\n",
      "| **Training efficiency** | Limited parallelism → slower | Matrix multiplications → highly parallel on GPUs |\n",
      "| **Ability to capture long‑range dependencies** | Hard (vanishing gradients) | Easy (direct connections) |\n",
      "\n",
      "In short, attention gives the model a **global view** of the sentence in a **single forward pass**, eliminating the bottlenecks of recurrence.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Take‑away Analogies\n",
      "\n",
      "| Concept | Analogy |\n",
      "|---------|---------|\n",
      "| **Attention** | A group of students each looking at every other student to decide who to talk to. |\n",
      "| **Multi‑Head** | Eight different teachers each focusing on a different aspect of the same lesson. |\n",
      "| **Positional Encoding** | Adding a unique timestamp to each word so the model knows when it appears. |\n",
      "| **Residual + LayerNorm** | A safety net that keeps the signal from blowing up while still letting the model learn new patterns. |\n",
      "\n",
      "---\n",
      "\n",
      "## 8. Bottom Line\n",
      "\n",
      "- The **Transformer** shows that you can achieve state‑of‑the‑art translation **without any recurrence or convolution**.  \n",
      "- By relying solely on **multi‑head scaled dot‑product attention**, the model gets a **global, parallelizable view** of the whole sentence, trains faster, and learns richer representations.  \n",
      "- This architecture became the foundation for later breakthroughs such as **BERT, GPT, T5**, and virtually all modern NLP models.\n",
      "\n",
      "---\n",
      "\n",
      "**That’s the complete beginner‑friendly analysis of the Transformer paper.** If you’d like a deeper dive into any specific equation, an ablation study, or the exact hyper‑parameter settings, just let me know!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run_pipeline(paper_text: str):\n",
    "    result = await supervisor_agent_executor.ainvoke({\n",
    "        \"input\": f\"\"\"\n",
    "Analyze and explain the following research paper.\n",
    "Use provided tools when appropriate.\n",
    "\n",
    "\"\"\"\n",
    "    ,'context':result_doc_agent['output']})\n",
    "\n",
    "    return result[\"output\"]\n",
    "\n",
    "\n",
    "# Run\n",
    "final_output = await run_pipeline(result_doc_agent[\"output\"])\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "8f99edbf-dc68-4cb6-9aea-ef3c0db0f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_result =supervisor_agent_executor.ainvoke({'input':'conduct the reseach on the given research paper', 'context':result_doc_agent['output']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "8de86ed8-27d6-4d7d-95c1-87ff06b0bb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Beginner‑friendly explanation of the Transformer paper**  \n",
      "\n",
      "Below is a concise, step‑by‑step overview of the key ideas, architecture, and intuition behind the “Attention Is All You Need” (Vaswani et al., 2017) model. It follows the logical components of the paper and is written in plain language so that newcomers can grasp the concepts without needing a deep technical background.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. The Problem the Authors Wanted to Solve\n",
      "- **Task:** Machine translation – converting a sentence in one language (e.g., English) into another (e.g., German).  \n",
      "- **Prevailing models:** Used **recurrent neural networks (RNNs)** or **convolutional neural networks (CNNs)** to read the source sentence and generate the target sentence.  \n",
      "- **Two major drawbacks of RNN/CNN approaches:**  \n",
      "  1. **Sequential processing:** Words had to be handled one after another, which made training slow because you couldn’t compute many steps in parallel.  \n",
      "  2. **Limited view of the whole sentence:** RNNs compress everything seen so far into a single hidden state; CNNs only see a small local window. Long‑distance relationships (e.g., a verb and its subject far apart) are hard to capture.\n",
      "\n",
      "**Goal:** Build a model that can look at the entire sentence at once, learn long‑range relationships easily, and train much faster.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Core Idea – **Attention**\n",
      "### 2.1 What is Attention?\n",
      "Imagine a teacher in a classroom who can instantly look at any student to decide who to talk to. In a sentence, each word can “pay attention” to every other word to decide how much each one matters for its own meaning.\n",
      "\n",
      "### 2.2 Scaled Dot‑Product Attention (the math behind it)\n",
      "\\[\n",
      "\\text{Attention}(Q,K,V)=\\text{softmax}\\!\\Big(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\Big)V\n",
      "\\]\n",
      "\n",
      "- **Q (queries):** What each word is looking for.  \n",
      "- **K (keys):** What each word offers.  \n",
      "- **V (values):** The actual information we want to combine.  \n",
      "- **\\(QK^{\\top}\\):** Measures similarity between every query and every key.  \n",
      "- **Division by \\(\\sqrt{d_k}\\):** Keeps numbers stable when the vector size is large.  \n",
      "- **softmax:** Turns similarities into probabilities that sum to 1.  \n",
      "- **Multiplying by V:** Gives a weighted sum of the values – the “context” each word will use.\n",
      "\n",
      "**Why it’s useful:**  \n",
      "- Every word can directly see every other word (global view).  \n",
      "- The operation is just matrix multiplication, so all words can be processed **in parallel** on a GPU.\n",
      "\n",
      "### 2.3 Multi‑Head Attention\n",
      "Instead of a single “teacher,” we have **multiple teachers (heads)** looking at the sentence in different ways (syntax, semantics, etc.). Each head works on a smaller sub‑space, then their results are concatenated and linearly transformed back to the original size. This lets the model capture many kinds of relationships simultaneously.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Building Blocks of the Transformer\n",
      "\n",
      "### 3.1 Encoder (reads the source sentence)\n",
      "- **6 identical layers**. Each layer does:  \n",
      "  1. **Multi‑Head Self‑Attention** – every word attends to every other word in the source.  \n",
      "  2. **Position‑wise Feed‑Forward Network** – a tiny fully‑connected network applied independently to each word’s vector, adding non‑linearity.  \n",
      "- **Residual connections + Layer Normalization** wrap each sub‑layer to keep training stable.\n",
      "\n",
      "### 3.2 Decoder (writes the target sentence)\n",
      "- **6 identical layers**. Each layer does:  \n",
      "  1. **Masked Multi‑Head Self‑Attention** – the decoder can only attend to already‑generated words, not future ones (preserves left‑to‑right generation).  \n",
      "  2. **Multi‑Head Encoder‑Decoder Attention** – each target word looks at the encoder’s output to gather information from the source sentence.  \n",
      "  3. **Position‑wise Feed‑Forward Network** – same as in the encoder.  \n",
      "- Also uses residual connections and layer normalization.\n",
      "\n",
      "### 3.3 Positional Encoding\n",
      "Because the model has no recurrence, it needs a way to know word order. Fixed sinusoidal signals are added to each word embedding:\n",
      "\n",
      "\\[\n",
      "\\begin{aligned}\n",
      "PE(pos,2i)   &= \\sin\\!\\big(pos / 10000^{2i/d_{\\text{model}}}\\big)\\\\\n",
      "PE(pos,2i+1) &= \\cos\\!\\big(pos / 10000^{2i/d_{\\text{model}}}\\big)\n",
      "\\end{aligned}\n",
      "\\]\n",
      "\n",
      "These give each position a unique pattern that the model can learn to interpret.\n",
      "\n",
      "### 3.4 Training Tricks\n",
      "| Trick | Reason |\n",
      "|------|--------|\n",
      "| **Adam optimizer** (β₁=0.9, β₂=0.98, ε=10⁻⁸) | Adaptive learning rates help convergence. |\n",
      "| **Learning‑rate schedule** \\(lrate = d_{\\text{model}}^{-0.5}\\min(step^{-0.5}, step \\cdot warmup^{-1.5})\\) | Starts small, rises, then decays – stabilizes early training. |\n",
      "| **Dropout (p=0.1)** | Prevents over‑fitting. |\n",
      "| **Layer Normalization** | Keeps activations in a healthy range, especially with residuals. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. How the Model Works – Step‑by‑Step Intuition\n",
      "\n",
      "1. **Embedding + Positional Encoding** – Each input word becomes a 512‑dimensional vector; we add the sinusoidal position signal.  \n",
      "2. **Encoder Stack** – The 6 layers repeatedly let every word attend to every other word and then transform the result with a feed‑forward net. After the stack, each word’s vector now contains a rich, context‑aware representation of the whole source sentence.  \n",
      "3. **Decoder Start** – The decoder receives a special “<s>” token.  \n",
      "   - Masked self‑attention lets it look only at tokens generated so far.  \n",
      "   - Encoder‑decoder attention pulls in relevant source information.  \n",
      "   - Feed‑forward refines the vector.  \n",
      "4. **Generate Next Word** – The final decoder layer produces a probability distribution over the target vocabulary; the most likely word (or a beam of candidates) is chosen.  \n",
      "5. **Repeat** – The chosen word is fed back into the decoder, and steps 3‑4 repeat until an “</s>” token appears.\n",
      "\n",
      "Because each decoding step still uses attention, the model can instantly incorporate all previously generated words and the full source context.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Experimental Results (WMT 2014 Translation)\n",
      "\n",
      "| Model | EN‑DE BLEU | EN‑FR BLEU | Training Time (relative) |\n",
      "|-------|------------|------------|--------------------------|\n",
      "| RNN‑Attention (baseline) | 28.4 | 38.7 | 1.0× |\n",
      "| ConvS2S | 29.3 | 39.2 | 0.9× |\n",
      "| **Transformer‑base** | **30.6** | **40.3** | **≈0.5×** |\n",
      "| Transformer‑big | 31.4 | 41.0 | 1.0× (larger model) |\n",
      "\n",
      "- **Performance:** The base Transformer already beats the best RNN and CNN baselines by 2–3 BLEU points.  \n",
      "- **Speed:** Training is roughly **twice as fast** because the whole sequence can be processed in parallel.  \n",
      "- **Transferability:** Encoder representations learned on translation work well for other NLP tasks (e.g., classification) with minimal fine‑tuning.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Why Attention Replaces Recurrence\n",
      "\n",
      "| Feature | RNN | Transformer |\n",
      "|---------|-----|--------------|\n",
      "| **Processing order** | Strictly sequential (word 1 → word 2 → …) | Fully parallel (all words at once) |\n",
      "| **Memory of past** | Single hidden state → may forget long‑range info | Every word directly accesses every other word via attention |\n",
      "| **Training efficiency** | Limited parallelism → slower | Matrix multiplications → highly parallel on GPUs |\n",
      "| **Ability to capture long‑range dependencies** | Hard (vanishing gradients) | Easy (direct connections) |\n",
      "\n",
      "In short, attention gives the model a **global view** of the sentence in a **single forward pass**, eliminating the bottlenecks of recurrence.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Take‑away Analogies\n",
      "\n",
      "| Concept | Analogy |\n",
      "|---------|---------|\n",
      "| **Attention** | A group of students each looking at every other student to decide who to talk to. |\n",
      "| **Multi‑Head** | Eight different teachers each focusing on a different aspect of the same lesson. |\n",
      "| **Positional Encoding** | Adding a unique timestamp to each word so the model knows when it appears. |\n",
      "| **Residual + LayerNorm** | A safety net that keeps the signal from blowing up while still letting the model learn new patterns. |\n",
      "\n",
      "---\n",
      "\n",
      "## 8. Bottom Line\n",
      "\n",
      "- The **Transformer** shows that you can achieve state‑of‑the‑art translation **without any recurrence or convolution**.  \n",
      "- By relying solely on **multi‑head scaled dot‑product attention**, the model gets a **global, parallelizable view** of the whole sentence, trains faster, and learns richer representations.  \n",
      "- This architecture became the foundation for later breakthroughs such as **BERT, GPT, T5**, and virtually all modern NLP models.\n",
      "\n",
      "---\n",
      "\n",
      "**That’s the complete beginner‑friendly analysis of the Transformer paper.** If you’d like a deeper dive into any specific equation, an ablation study, or the exact hyper‑parameter settings, just let me know!\n"
     ]
    }
   ],
   "source": [
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e689288-750f-448e-a297-cd1f7abf184e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
