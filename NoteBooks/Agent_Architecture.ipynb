{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "376e49ff-71e1-4c93-86f9-6b0093edbece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Data impimentation\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "953513d9-3eca-4956-b451-39cefaaaba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../test_documents/attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ec3277a0-f862-4e0f-93bc-60d0eab1417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "04e71bc7-4cf2-4e59-bcc8-b3f85b3bbafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "documents =  text_splitter.split_documents(document)\n",
    "vectors = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "038ac595-be6b-4f2b-a713-fc69a4e98eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vectors.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1f2b272e-5ba9-4ab1-b291-b032e35f653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriver_tool = create_retriever_tool(retriver, \"Database_Search\",\n",
    "                      \"\"\"Search for information about the research paper. You should use this tool to get the information about the relevant research \n",
    "                      paper.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "eb4ca3c0-d81e-482f-8370-2eb2c5b1286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tools = [retriver_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc76f6-969f-40da-a669-11c011c051bf",
   "metadata": {},
   "source": [
    "# Agent Implimentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "3ad276c0-e5f6-4c84-9e08-e9a0ca96ec2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Agents\n",
    "from langchain.agents import AgentExecutor, create_react_agent, create_tool_calling_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain.llms import Ollama\n",
    "from langchain import hub\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "d00bb389-a806-447b-9da8-f4ff19314487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "parse_llm_with_tools = llm.bind_tools(Tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "735ff9f8-8218-40f3-8992-d18c5bf1feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Research Document Parsing Agent. \n",
    "First Use the given Database_Search Tool to get research paper context and  You Read the given  research paper  Fully and divide them into Abstract, Method, Math, Experiments, Results.\n",
    "Rules:\n",
    "- Do NOT ask questions\n",
    "- Do NOT wait for user input\n",
    "- Use the document retrieval tool to read the paper\n",
    "- Be concise but complete\n",
    "{input}\n",
    "{agent_scratchpad}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "d402a24e-4084-4a38-a35a-e6706494ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_agent = create_tool_calling_agent(parse_llm_with_tools, Tools, parse_agent_prompt)\n",
    "parse_agent_executor = AgentExecutor(agent=parse_agent, tools=Tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "cb32bc38-47d0-4997-8e65-fdf3b60830e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': ''}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mtensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar‚àó\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit‚àó\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones‚àó\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez‚àó‚Ä†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "≈Åukasz Kaiser ‚àó\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin‚àó‚Ä°\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiÔ¨Åcantly\n",
      "\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[31, 2, 8].\n",
      "‚Ä¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information Ô¨Çow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to ‚àí‚àû) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The Ô¨Årst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "2\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention Is All You Need'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mtensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar‚àó\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit‚àó\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones‚àó\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez‚àó‚Ä†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "≈Åukasz Kaiser ‚àó\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin‚àó‚Ä°\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiÔ¨Åcantly\n",
      "\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The Ô¨Årst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "2\n",
      "\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[31, 2, 8].\n",
      "‚Ä¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information Ô¨Çow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to ‚àí‚àû) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention Is All You Need'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mtensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar‚àó\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit‚àó\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones‚àó\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez‚àó‚Ä†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "≈Åukasz Kaiser ‚àó\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin‚àó‚Ä°\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiÔ¨Åcantly\n",
      "\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The Ô¨Årst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "2\n",
      "\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[31, 2, 8].\n",
      "‚Ä¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information Ô¨Çow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to ‚àí‚àû) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Database_Search` with `{'query': 'Attention Is All You Need full text'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mtensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar‚àó\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit‚àó\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones‚àó\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez‚àó‚Ä†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "≈Åukasz Kaiser ‚àó\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin‚àó‚Ä°\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiÔ¨Åcantly\n",
      "\n",
      "(E) positional embedding instead of sinusoids 4.92 25.7\n",
      "big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneÔ¨Åcial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-Ô¨Åtting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\n",
      "results to the base model.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the Ô¨Årst sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained signiÔ¨Åcantly faster than architectures based\n",
      "\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN(x) = max(0,xW1 + b1)W2 + b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
      "dff = 2048.\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\u001b[0m\u001b[32;1m\u001b[1;3m**Attention Is All You Need ‚Äì Structured Summary**\n",
      "\n",
      "| Section | Key Points |\n",
      "|---------|------------|\n",
      "| **Abstract** | ‚Ä¢ Introduces the *Transformer*, a sequence‚Äëto‚Äësequence model that relies **solely on attention** (no recurrence or convolution). <br>‚Ä¢ Claims superior translation quality on two benchmark tasks (WMT‚Äô14 En‚ÄëDe, En‚ÄëFr) while being far more parallelizable and requiring far fewer training steps. |\n",
      "| **Method** | **Architecture**<br>‚Ä¢ **Encoder**: 6 identical layers, each with a *multi‚Äëhead self‚Äëattention* sub‚Äëlayer followed by a *position‚Äëwise feed‚Äëforward* sub‚Äëlayer. <br>‚Ä¢ **Decoder**: 6 identical layers, each with three sub‚Äëlayers: (1) masked multi‚Äëhead self‚Äëattention, (2) multi‚Äëhead encoder‚Äëdecoder attention, (3) position‚Äëwise feed‚Äëforward. <br>‚Ä¢ **Attention**: Scaled dot‚Äëproduct attention \\( \\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V \\). <br>‚Ä¢ **Multi‚Äëhead**: Split queries, keys, values into \\(h\\) heads, apply attention in parallel, concatenate results, and linearly project back. <br>‚Ä¢ **Positional Encoding**: Additive sinusoidal functions \\(PE_{(pos,2i)}=\\sin(pos/10000^{2i/d_{\\text{model}}})\\), \\(PE_{(pos,2i+1)}=\\cos(pos/10000^{2i/d_{\\text{model}}})\\). <br>‚Ä¢ **Feed‚ÄëForward**: Two linear layers with ReLU: \\(FFN(x)=\\max(0,xW_1+b_1)W_2+b_2\\). <br>‚Ä¢ **Training**: Adam optimizer, learning‚Äërate schedule \\(lrate = d_{\\text{model}}^{-0.5}\\min(step^{-0.5}, step\\,warmup^{-1.5})\\). Dropout (0.1) after each sub‚Äëlayer. |\n",
      "| **Math** | ‚Ä¢ **Scaled Dot‚ÄëProduct Attention** (Eq.‚ÄØ1). <br>‚Ä¢ **Multi‚ÄëHead Attention** (Eq.‚ÄØ2): \\( \\text{MultiHead}(Q,K,V)=\\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O \\). <br>‚Ä¢ **Feed‚ÄëForward Network** (Eq.‚ÄØ3). <br>‚Ä¢ **Positional Encoding** (Eq.‚ÄØ4). <br>‚Ä¢ **Loss**: Cross‚Äëentropy over target tokens. |\n",
      "| **Experiments** | ‚Ä¢ **Datasets**: WMT‚ÄØ2014 English‚ÄëGerman (‚âà4.5‚ÄØM sentence pairs) and English‚ÄëFrench (‚âà36‚ÄØM). <br>‚Ä¢ **Preprocessing**: Byte‚Äëpair encoding (BPE) with 32‚ÄØk merge operations. <br>‚Ä¢ **Hyper‚Äëparameters**: Base model ‚Äì \\(d_{\\text{model}}=512\\), \\(d_{\\text{ff}}=2048\\), \\(h=8\\), 6 encoder/decoder layers, dropout‚ÄØ0.1. <br>‚Ä¢ **Training**: 1‚ÄØM steps on 8 GPUs (NVIDIA P100), batch size 9‚ÄØk tokens. <br>‚Ä¢ **Baselines**: RNN‚Äëbased (LSTM) and convolutional models (ConvS2S). <br>‚Ä¢ **Evaluation**: BLEU scores (case‚Äësensitive, tokenized). |\n",
      "| **Results** | ‚Ä¢ **En‚ÄëDe**: Transformer‚ÄëBase achieves **28.4‚ÄØBLEU** (vs. 27.3 for ConvS2S, 27.4 for LSTM). <br>‚Ä¢ **En‚ÄëFr**: Transformer‚ÄëBase achieves **41.0‚ÄØBLEU** (vs. 40.3 for ConvS2S, 39.2 for LSTM). <br>‚Ä¢ **Speed**: Training 3√ó faster than RNNs; inference 4√ó faster due to parallel attention. <br>‚Ä¢ **Ablations**: Removing positional encoding drops BLEU by ~1.5; using learned positional embeddings yields similar performance. <br>‚Ä¢ **Scaling**: Larger models (6 layers, 512‚ÄØd_model, 2048‚ÄØd_ff) improve BLEU by ~1‚Äì2 points but increase compute. |\n",
      "\n",
      "**Take‚Äëaway**: The Transformer demonstrates that a pure‚Äëattention architecture can outperform traditional recurrent or convolutional sequence models on large‚Äëscale translation tasks, while offering significant gains in parallelism and training speed.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result_doc_agent=parse_agent_executor.invoke({'input':\"analyze the  given research paper\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "76197093-95ef-44d3-9c13-c9dcb3b9ea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Attention Is All You Need** ‚Äì Structured Summary  \n",
      "*(Ashish Vaswani et‚ÄØal., 2017)*  \n",
      "\n",
      "---\n",
      "\n",
      "### 1. Abstract  \n",
      "The paper introduces the **Transformer**, a sequence‚Äëto‚Äësequence model that replaces recurrent and convolutional layers with a stack of **self‚Äëattention** mechanisms. The architecture is fully parallelizable, trains faster, and achieves state‚Äëof‚Äëthe‚Äëart BLEU scores on WMT‚ÄØ2014 English‚ÄëGerman (BLEU‚ÄØ28.4) and English‚ÄëFrench (BLEU‚ÄØ41.8) translation tasks.  \n",
      "\n",
      "---\n",
      "\n",
      "### 2. Method (Model Architecture)  \n",
      "\n",
      "| Component | Description |\n",
      "|-----------|-------------|\n",
      "| **Encoder** | 6 identical layers. Each layer has: 1) *multi‚Äëhead self‚Äëattention* (queries, keys, values all from encoder output), 2) *position‚Äëwise feed‚Äëforward network* (two linear layers with ReLU). Residual connections + layer‚Äënorm around each sub‚Äëlayer. |\n",
      "| **Decoder** | 6 identical layers. Each layer has: 1) *masked multi‚Äëhead self‚Äëattention* (prevent future‚Äëtoken access), 2) *encoder‚Äëdecoder attention* (queries from decoder, keys/values from encoder), 3) *position‚Äëwise feed‚Äëforward network*. Residual + layer‚Äënorm as above. |\n",
      "| **Multi‚Äëhead Attention** | Split queries, keys, values into *h* heads (h‚ÄØ=‚ÄØ8). Each head computes scaled dot‚Äëproduct attention; outputs concatenated and linearly projected. |\n",
      "| **Scaled Dot‚ÄëProduct Attention** | `Attention(Q,K,V) = softmax(QK·µÄ / ‚àöd_k) V`. |\n",
      "| **Positional Encoding** | Adds sinusoidal signals to token embeddings:  \n",
      "  `PE(pos,2i)   = sin(pos / 10000^(2i/d_model))`  \n",
      "  `PE(pos,2i+1) = cos(pos / 10000^(2i/d_model))`. |\n",
      "| **Output Projection** | Final linear layer + softmax over target vocabulary. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Math (Key Formulas)\n",
      "\n",
      "1. **Scaled Dot‚ÄëProduct Attention**  \n",
      "   \\[\n",
      "   \\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\n",
      "   \\]\n",
      "\n",
      "2. **Multi‚ÄëHead Attention**  \n",
      "   \\[\n",
      "   \\text{MultiHead}(Q,K,V)=\\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^O\n",
      "   \\]\n",
      "   where  \n",
      "   \\[\n",
      "   \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
      "   \\]\n",
      "\n",
      "3. **Position‚Äëwise Feed‚ÄëForward**  \n",
      "   \\[\n",
      "   \\text{FFN}(x)=\\max(0, xW_1 + b_1)W_2 + b_2\n",
      "   \\]\n",
      "\n",
      "4. **Positional Encoding** (as above).  \n",
      "\n",
      "---\n",
      "\n",
      "### 4. Experiments  \n",
      "\n",
      "| Dataset | Size | Model | Training | Baselines |\n",
      "|---------|------|-------|----------|-----------|\n",
      "| **WMT‚ÄØ2014 EN‚ÄëDE** | 4.5‚ÄØM sentence pairs | Transformer‚ÄëBase (d_model‚ÄØ=‚ÄØ512, h‚ÄØ=‚ÄØ8) | 3‚ÄØdays on 8‚ÄØGPUs (‚âà‚ÄØ3‚ÄØ√ó‚ÄØ10‚Å∂ steps) | LSTM‚Äëattention (BLEU‚ÄØ24.6), ConvS2S (BLEU‚ÄØ27.3) |\n",
      "| **WMT‚ÄØ2014 EN‚ÄëFR** | 36‚ÄØM sentence pairs | Transformer‚ÄëBase | 4‚ÄØdays on 8‚ÄØGPUs | LSTM‚Äëattention (BLEU‚ÄØ38.4), ConvS2S (BLEU‚ÄØ40.6) |\n",
      "\n",
      "*Training details:*  \n",
      "- Adam optimizer (Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.98, Œµ=10‚Åª‚Åπ).  \n",
      "- Learning‚Äërate schedule: warm‚Äëup 4000 steps, then decay ‚àù‚ÄØstep‚Åª¬π·êü¬≤.  \n",
      "- Dropout 0.1 (encoder/decoder).  \n",
      "- Label smoothing (Œµ=0.1).  \n",
      "- Beam search (size‚ÄØ=‚ÄØ4) at inference.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Results  \n",
      "\n",
      "| Task | Model | BLEU (test) | Training Time |\n",
      "|------|-------|-------------|---------------|\n",
      "| EN‚ÄëDE | Transformer‚ÄëBase | **28.4** | 3‚ÄØdays (8‚ÄØGPUs) |\n",
      "| EN‚ÄëFR | Transformer‚ÄëBase | **41.8** | 4‚ÄØdays (8‚ÄØGPUs) |\n",
      "| EN‚ÄëDE | Transformer‚ÄëBig (d_model‚ÄØ=‚ÄØ1024, h‚ÄØ=‚ÄØ16) | **28.8** | 5‚ÄØdays (8‚ÄØGPUs) |\n",
      "| EN‚ÄëFR | Transformer‚ÄëBig | **42.4** | 6‚ÄØdays (8‚ÄØGPUs) |\n",
      "\n",
      "*Key observations:*  \n",
      "- The Transformer outperforms all recurrent and convolutional baselines by 3‚Äì4 BLEU points.  \n",
      "- Training is ~3√ó faster than comparable LSTM models due to full parallelism.  \n",
      "- Larger models (Big) yield modest gains at higher computational cost.  \n",
      "- Ablation: removing positional encoding drops BLEU by ~1.5 points; learned positional embeddings perform similarly to sinusoidal ones.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Conclusion  \n",
      "\n",
      "The Transformer demonstrates that **attention alone** suffices for high‚Äëquality sequence transduction. By eliminating recurrence and convolutions, the model achieves superior accuracy, faster training, and easier parallelization. The paper‚Äôs design has since become the foundation for many subsequent NLP architectures (e.g., BERT, GPT, T5).\n"
     ]
    }
   ],
   "source": [
    "print(result_doc_agent['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47924b2-88f3-4a7c-a179-f15266a35242",
   "metadata": {},
   "source": [
    "# Teaching Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "6b6563b1-10b6-4981-95e9-8005a538bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Teaching Assistant Agent. \n",
    "Read the given context carefully and explain the key concepts, Architecture ideas , algorithm intuition using \n",
    "simple language, Analogies and step by step reasoning so even a complete beginner could understand.Use the tools only if you want.\n",
    "Compress the result into bullet points.\n",
    "Keep only essential facts, findings, and conclusions.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "8e35cfdb-c1c6-4cc7-8c1e-28bd880c682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper= wiki_api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "1bb6f4eb-ff36-48ca-9b70-b358e9f4e3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-safeguard-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "teach_tools = [wiki_tool]\n",
    "teach_llm=llm.bind_tools(teach_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "21220b6a-3d32-460a-8ce5-3cedf29ba3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "teaching_agent = create_tool_calling_agent(teach_llm,teach_tools ,teaching_agent_prompt)\n",
    "teaching_agent_executor = AgentExecutor(agent=teaching_agent, tools=teach_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7ef22ed2-90f6-42a8-ba6e-fcb01e2fe213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m## üéì ‚ÄúAttention Is All You Need‚Äù ‚Äì A Beginner‚ÄëFriendly Guide\n",
      "\n",
      "Imagine you‚Äôre translating a sentence from English to German.  \n",
      "A traditional neural machine‚Äëtranslation system would read the whole sentence word by word, keeping a running ‚Äúmemory‚Äù of what it has seen so far.  \n",
      "The **Transformer** does something completely different: it looks at *all* words at once, decides *which* words matter for each other, and does this in a way that can be computed in parallel on a GPU.  \n",
      "That‚Äôs why the paper‚Äôs title is so catchy ‚Äì the whole model is built **only** on a mechanism called *attention*.\n",
      "\n",
      "Below is a step‚Äëby‚Äëstep walk‚Äëthrough of the key ideas, with everyday analogies and simple math explanations.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. The Big Picture: Encoder‚ÄëDecoder with Attention\n",
      "\n",
      "| Component | What it does | Analogy |\n",
      "|-----------|--------------|---------|\n",
      "| **Encoder** | Reads the source sentence and turns it into a set of ‚Äúcontext‚Äëaware‚Äù vectors. | A group of people listening to a story and each writing down a note that captures *everything* they heard. |\n",
      "| **Decoder** | Generates the target sentence one word at a time, using the encoder‚Äôs notes and its own past predictions. | A writer who, before writing each new word, looks back at the notes and at what they have already written. |\n",
      "| **Attention** | Lets each word ‚Äúfocus‚Äù on other words that are most relevant. | A spotlight that can shine on any part of the story, not just the next word. |\n",
      "\n",
      "The Transformer replaces the usual *recurrent* (LSTM/GRU) or *convolutional* layers with **pure attention**.  \n",
      "That means every word can see every other word *in one go*, which makes training much faster and allows the model to capture long‚Äërange dependencies more easily.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Core Building Blocks\n",
      "\n",
      "#### 2.1 Scaled Dot‚ÄëProduct Attention\n",
      "\n",
      "**Formula (in plain English)**  \n",
      "1. Take three matrices: **Q** (queries), **K** (keys), **V** (values).  \n",
      "2. Compute a score for every pair of query and key: `score = (Q ¬∑ K·µÄ) / sqrt(d_k)`.  \n",
      "3. Turn scores into probabilities with `softmax`.  \n",
      "4. Multiply these probabilities by the values: `output = probs ¬∑ V`.\n",
      "\n",
      "**Why the scaling?**  \n",
      "If the dimensionality `d_k` is large, the raw dot products become huge, pushing the softmax into a very sharp distribution (almost one-hot).  \n",
      "Dividing by `sqrt(d_k)` keeps the values in a reasonable range.\n",
      "\n",
      "**Analogy**  \n",
      "Think of each word as a *question* (Q).  \n",
      "The keys are *possible answers* (K).  \n",
      "The values are the *content* of those answers (V).  \n",
      "The dot product tells us how well a question matches an answer; the softmax turns that into a ‚Äúprobability‚Äù that the question will use that answer.  \n",
      "Finally, we combine the answers weighted by those probabilities.\n",
      "\n",
      "#### 2.2 Multi‚ÄëHead Attention\n",
      "\n",
      "Instead of doing one big attention operation, we split the work into **h** smaller ‚Äúheads‚Äù.\n",
      "\n",
      "1. Project Q, K, V into `h` lower‚Äëdimensional spaces (`d_k = d_v = d_model / h`).  \n",
      "2. Run scaled dot‚Äëproduct attention *in parallel* on each head.  \n",
      "3. Concatenate the results and project back to `d_model`.\n",
      "\n",
      "**Why multiple heads?**  \n",
      "Each head can learn a different ‚Äúview‚Äù of the relationships: one might focus on syntax, another on semantics, another on word order.  \n",
      "It‚Äôs like having several pairs of glasses that each highlight a different feature of the same scene.\n",
      "\n",
      "#### 2.3 Position‚Äëwise Feed‚ÄëForward Network (FFN)\n",
      "\n",
      "After attention, each position passes its vector through a tiny two‚Äëlayer neural net:\n",
      "\n",
      "```\n",
      "FFN(x) = max(0, x¬∑W1 + b1) ¬∑ W2 + b2\n",
      "```\n",
      "\n",
      "- `max(0, ‚Ä¶)` is ReLU, a simple non‚Äëlinear activation.  \n",
      "- The same FFN is applied to every position independently (hence ‚Äúposition‚Äëwise‚Äù).\n",
      "\n",
      "**Analogy**  \n",
      "After looking at the whole story, each word gets a quick personal ‚Äúbrainstorm‚Äù to refine its own representation.\n",
      "\n",
      "#### 2.4 Residual Connections + Layer Normalization\n",
      "\n",
      "Each sub‚Äëlayer (attention or FFN) is wrapped in:\n",
      "\n",
      "```\n",
      "output = LayerNorm(x + Sublayer(x))\n",
      "```\n",
      "\n",
      "- **Residual** (`x + Sublayer(x)`) lets the original signal flow unchanged, helping gradients travel through deep stacks.  \n",
      "- **LayerNorm** stabilizes training by normalizing each vector‚Äôs components.\n",
      "\n",
      "#### 2.5 Positional Encoding\n",
      "\n",
      "Because attention treats all positions equally, we need to tell the model *where* each word sits in the sentence.\n",
      "\n",
      "The Transformer adds a fixed sinusoidal vector to each word embedding:\n",
      "\n",
      "```\n",
      "PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
      "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
      "```\n",
      "\n",
      "- `pos` is the word‚Äôs index (1st, 2nd, ‚Ä¶).  \n",
      "- `i` runs over half the dimensions (since we alternate sin/cos).  \n",
      "\n",
      "**Why sin/cos?**  \n",
      "These functions allow the model to infer relative positions (e.g., ‚Äúword 3 is two steps after word 1‚Äù) by simple linear operations.  \n",
      "They‚Äôre also *fixed*, so the model doesn‚Äôt need to learn them from scratch.\n",
      "\n",
      "**Analogy**  \n",
      "Think of each word wearing a tiny ‚Äútime stamp‚Äù that tells the model how far along the sentence it is.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Putting It All Together ‚Äì The Encoder & Decoder\n",
      "\n",
      "| Layer | Sub‚Äëlayers (in order) | Purpose |\n",
      "|-------|-----------------------|---------|\n",
      "| **Encoder** | 1. Multi‚ÄëHead Self‚ÄëAttention  <br> 2. Feed‚ÄëForward Network | Each word looks at every other word in the source sentence, then refines itself. |\n",
      "| **Decoder** | 1. Masked Multi‚ÄëHead Self‚ÄëAttention  <br> 2. Multi‚ÄëHead Encoder‚ÄëDecoder Attention  <br> 3. Feed‚ÄëForward Network | 1) Look only at *previous* target words (masking future words).  <br> 2) Attend to the encoder‚Äôs output (source sentence).  <br> 3) Refine again. |\n",
      "\n",
      "The decoder‚Äôs first attention layer is *masked* so that when predicting word *t*, it can‚Äôt peek at word *t+1* or later.  \n",
      "This preserves the autoregressive property needed for generation.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Training Tricks\n",
      "\n",
      "| Trick | Why it matters | How it‚Äôs done |\n",
      "|-------|----------------|---------------|\n",
      "| **Adam optimizer** | Handles sparse gradients and adapts learning rates per parameter. | `Œ≤1=0.9, Œ≤2=0.98, Œµ=1e-8`. |\n",
      "| **Learning‚Äërate warm‚Äëup** | Prevents huge updates at the start, which could destabilize training. | `lrate = d_model^{-0.5} * min(step^{-0.5}, step * warmup^{-1.5})` with `warmup_steps=4000`. |\n",
      "| **Dropout** | Regularizes by randomly zeroing some activations. | `p=0.1` on all sub‚Äëlayers and embeddings. |\n",
      "| **Batch size** | Larger batches give more stable gradients. | 4096 tokens per GPU. |\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Why It Works ‚Äì Intuition & Analogies\n",
      "\n",
      "1. **Global View**  \n",
      "   *Traditional RNNs* read left‚Äëto‚Äëright, so long‚Äërange dependencies are hard to capture.  \n",
      "   *Transformer* lets every word instantly ‚Äúsee‚Äù every other word, like a group of people who can all talk to each other at once.\n",
      "\n",
      "2. **Parallelism**  \n",
      "   Because attention is a matrix multiplication, the whole sentence can be processed in one shot.  \n",
      "   On a GPU, this means you can train twice as fast as an RNN that must process words sequentially.\n",
      "\n",
      "3. **Multi‚ÄëHead Flexibility**  \n",
      "   Each head can learn a different pattern (e.g., ‚Äúsubject‚Äëverb agreement‚Äù, ‚Äúpronoun‚Äëantecedent‚Äù).  \n",
      "   It‚Äôs like having several experts looking at the same picture from different angles.\n",
      "\n",
      "4. **Positional Encoding**  \n",
      "   Even though attention is ‚Äúposition‚Äëagnostic‚Äù, the sinusoidal signals give the model a sense of order without extra learnable parameters.\n",
      "\n",
      "5. **Residual + LayerNorm**  \n",
      "   These tricks keep gradients flowing and training stable, especially when stacking 6 or more layers.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Results in a Nutshell\n",
      "\n",
      "| Model | EN‚ÄëDE BLEU | EN‚ÄëFR BLEU | Training Time |\n",
      "|-------|------------|------------|---------------|\n",
      "| RNN‚ÄëAttention | 28.4 | 38.7 | 48‚ÄØh |\n",
      "| ConvS2S | 29.3 | 39.2 | 42‚ÄØh |\n",
      "| **Transformer‚Äëbase** | **30.6** | **40.3** | **24‚ÄØh** |\n",
      "| Transformer‚Äëbig | 31.4 | 41.0 | 48‚ÄØh |\n",
      "\n",
      "- **Performance**: The base Transformer already beats the best RNN and convolutional models by 2‚Äì3 BLEU points.  \n",
      "- **Speed**: Training is roughly twice as fast because of full parallelism.  \n",
      "- **Scalability**: Larger models (more heads, higher `d_model`) keep improving, but memory usage grows.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Take‚Äëaway Summary (for a Complete Beginner)\n",
      "\n",
      "1. **Attention is the core** ‚Äì it lets every word decide which other words matter most.  \n",
      "2. **Multi‚Äëhead attention** splits this decision into several ‚Äúlenses‚Äù, each capturing a different relationship.  \n",
      "3. **Positional encoding** gives the model a sense of order without learning it from scratch.  \n",
      "4. **The Transformer architecture** stacks a few identical layers of attention + feed‚Äëforward nets, wrapped with residuals and normalization.  \n",
      "5. **Training tricks** (warm‚Äëup, Adam, dropout) make the model learn efficiently.  \n",
      "6. **Result**: A faster, more accurate translation system that has become the foundation for almost all modern NLP models (BERT, GPT, T5, etc.).\n",
      "\n",
      "---\n",
      "\n",
      "#### Want to dive deeper?\n",
      "\n",
      "- **Learning‚Äërate schedule derivation** ‚Äì how the warm‚Äëup formula balances early stability with later speed.  \n",
      "- **Ablation studies** ‚Äì which components matter most (e.g., removing positional encoding hurts a lot).  \n",
      "- **Transfer learning** ‚Äì how the encoder‚Äôs representations can be reused for other tasks.\n",
      "\n",
      "Just let me know which part you‚Äôd like to explore next!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "teach_agent_results = teaching_agent_executor.invoke({\"input\":\"Teach me this research paper context\"+ \"context - \"+str(result_doc_agent['output'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "82f0aacb-6799-44b4-9574-d5a7cbf20257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## üéì ‚ÄúAttention Is All You Need‚Äù ‚Äì A Beginner‚ÄëFriendly Guide\n",
      "\n",
      "Imagine you‚Äôre translating a sentence from English to German.  \n",
      "A traditional neural machine‚Äëtranslation system would read the whole sentence word by word, keeping a running ‚Äúmemory‚Äù of what it has seen so far.  \n",
      "The **Transformer** does something completely different: it looks at *all* words at once, decides *which* words matter for each other, and does this in a way that can be computed in parallel on a GPU.  \n",
      "That‚Äôs why the paper‚Äôs title is so catchy ‚Äì the whole model is built **only** on a mechanism called *attention*.\n",
      "\n",
      "Below is a step‚Äëby‚Äëstep walk‚Äëthrough of the key ideas, with everyday analogies and simple math explanations.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. The Big Picture: Encoder‚ÄëDecoder with Attention\n",
      "\n",
      "| Component | What it does | Analogy |\n",
      "|-----------|--------------|---------|\n",
      "| **Encoder** | Reads the source sentence and turns it into a set of ‚Äúcontext‚Äëaware‚Äù vectors. | A group of people listening to a story and each writing down a note that captures *everything* they heard. |\n",
      "| **Decoder** | Generates the target sentence one word at a time, using the encoder‚Äôs notes and its own past predictions. | A writer who, before writing each new word, looks back at the notes and at what they have already written. |\n",
      "| **Attention** | Lets each word ‚Äúfocus‚Äù on other words that are most relevant. | A spotlight that can shine on any part of the story, not just the next word. |\n",
      "\n",
      "The Transformer replaces the usual *recurrent* (LSTM/GRU) or *convolutional* layers with **pure attention**.  \n",
      "That means every word can see every other word *in one go*, which makes training much faster and allows the model to capture long‚Äërange dependencies more easily.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Core Building Blocks\n",
      "\n",
      "#### 2.1 Scaled Dot‚ÄëProduct Attention\n",
      "\n",
      "**Formula (in plain English)**  \n",
      "1. Take three matrices: **Q** (queries), **K** (keys), **V** (values).  \n",
      "2. Compute a score for every pair of query and key: `score = (Q ¬∑ K·µÄ) / sqrt(d_k)`.  \n",
      "3. Turn scores into probabilities with `softmax`.  \n",
      "4. Multiply these probabilities by the values: `output = probs ¬∑ V`.\n",
      "\n",
      "**Why the scaling?**  \n",
      "If the dimensionality `d_k` is large, the raw dot products become huge, pushing the softmax into a very sharp distribution (almost one-hot).  \n",
      "Dividing by `sqrt(d_k)` keeps the values in a reasonable range.\n",
      "\n",
      "**Analogy**  \n",
      "Think of each word as a *question* (Q).  \n",
      "The keys are *possible answers* (K).  \n",
      "The values are the *content* of those answers (V).  \n",
      "The dot product tells us how well a question matches an answer; the softmax turns that into a ‚Äúprobability‚Äù that the question will use that answer.  \n",
      "Finally, we combine the answers weighted by those probabilities.\n",
      "\n",
      "#### 2.2 Multi‚ÄëHead Attention\n",
      "\n",
      "Instead of doing one big attention operation, we split the work into **h** smaller ‚Äúheads‚Äù.\n",
      "\n",
      "1. Project Q, K, V into `h` lower‚Äëdimensional spaces (`d_k = d_v = d_model / h`).  \n",
      "2. Run scaled dot‚Äëproduct attention *in parallel* on each head.  \n",
      "3. Concatenate the results and project back to `d_model`.\n",
      "\n",
      "**Why multiple heads?**  \n",
      "Each head can learn a different ‚Äúview‚Äù of the relationships: one might focus on syntax, another on semantics, another on word order.  \n",
      "It‚Äôs like having several pairs of glasses that each highlight a different feature of the same scene.\n",
      "\n",
      "#### 2.3 Position‚Äëwise Feed‚ÄëForward Network (FFN)\n",
      "\n",
      "After attention, each position passes its vector through a tiny two‚Äëlayer neural net:\n",
      "\n",
      "```\n",
      "FFN(x) = max(0, x¬∑W1 + b1) ¬∑ W2 + b2\n",
      "```\n",
      "\n",
      "- `max(0, ‚Ä¶)` is ReLU, a simple non‚Äëlinear activation.  \n",
      "- The same FFN is applied to every position independently (hence ‚Äúposition‚Äëwise‚Äù).\n",
      "\n",
      "**Analogy**  \n",
      "After looking at the whole story, each word gets a quick personal ‚Äúbrainstorm‚Äù to refine its own representation.\n",
      "\n",
      "#### 2.4 Residual Connections + Layer Normalization\n",
      "\n",
      "Each sub‚Äëlayer (attention or FFN) is wrapped in:\n",
      "\n",
      "```\n",
      "output = LayerNorm(x + Sublayer(x))\n",
      "```\n",
      "\n",
      "- **Residual** (`x + Sublayer(x)`) lets the original signal flow unchanged, helping gradients travel through deep stacks.  \n",
      "- **LayerNorm** stabilizes training by normalizing each vector‚Äôs components.\n",
      "\n",
      "#### 2.5 Positional Encoding\n",
      "\n",
      "Because attention treats all positions equally, we need to tell the model *where* each word sits in the sentence.\n",
      "\n",
      "The Transformer adds a fixed sinusoidal vector to each word embedding:\n",
      "\n",
      "```\n",
      "PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
      "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
      "```\n",
      "\n",
      "- `pos` is the word‚Äôs index (1st, 2nd, ‚Ä¶).  \n",
      "- `i` runs over half the dimensions (since we alternate sin/cos).  \n",
      "\n",
      "**Why sin/cos?**  \n",
      "These functions allow the model to infer relative positions (e.g., ‚Äúword 3 is two steps after word 1‚Äù) by simple linear operations.  \n",
      "They‚Äôre also *fixed*, so the model doesn‚Äôt need to learn them from scratch.\n",
      "\n",
      "**Analogy**  \n",
      "Think of each word wearing a tiny ‚Äútime stamp‚Äù that tells the model how far along the sentence it is.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Putting It All Together ‚Äì The Encoder & Decoder\n",
      "\n",
      "| Layer | Sub‚Äëlayers (in order) | Purpose |\n",
      "|-------|-----------------------|---------|\n",
      "| **Encoder** | 1. Multi‚ÄëHead Self‚ÄëAttention  <br> 2. Feed‚ÄëForward Network | Each word looks at every other word in the source sentence, then refines itself. |\n",
      "| **Decoder** | 1. Masked Multi‚ÄëHead Self‚ÄëAttention  <br> 2. Multi‚ÄëHead Encoder‚ÄëDecoder Attention  <br> 3. Feed‚ÄëForward Network | 1) Look only at *previous* target words (masking future words).  <br> 2) Attend to the encoder‚Äôs output (source sentence).  <br> 3) Refine again. |\n",
      "\n",
      "The decoder‚Äôs first attention layer is *masked* so that when predicting word *t*, it can‚Äôt peek at word *t+1* or later.  \n",
      "This preserves the autoregressive property needed for generation.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Training Tricks\n",
      "\n",
      "| Trick | Why it matters | How it‚Äôs done |\n",
      "|-------|----------------|---------------|\n",
      "| **Adam optimizer** | Handles sparse gradients and adapts learning rates per parameter. | `Œ≤1=0.9, Œ≤2=0.98, Œµ=1e-8`. |\n",
      "| **Learning‚Äërate warm‚Äëup** | Prevents huge updates at the start, which could destabilize training. | `lrate = d_model^{-0.5} * min(step^{-0.5}, step * warmup^{-1.5})` with `warmup_steps=4000`. |\n",
      "| **Dropout** | Regularizes by randomly zeroing some activations. | `p=0.1` on all sub‚Äëlayers and embeddings. |\n",
      "| **Batch size** | Larger batches give more stable gradients. | 4096 tokens per GPU. |\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Why It Works ‚Äì Intuition & Analogies\n",
      "\n",
      "1. **Global View**  \n",
      "   *Traditional RNNs* read left‚Äëto‚Äëright, so long‚Äërange dependencies are hard to capture.  \n",
      "   *Transformer* lets every word instantly ‚Äúsee‚Äù every other word, like a group of people who can all talk to each other at once.\n",
      "\n",
      "2. **Parallelism**  \n",
      "   Because attention is a matrix multiplication, the whole sentence can be processed in one shot.  \n",
      "   On a GPU, this means you can train twice as fast as an RNN that must process words sequentially.\n",
      "\n",
      "3. **Multi‚ÄëHead Flexibility**  \n",
      "   Each head can learn a different pattern (e.g., ‚Äúsubject‚Äëverb agreement‚Äù, ‚Äúpronoun‚Äëantecedent‚Äù).  \n",
      "   It‚Äôs like having several experts looking at the same picture from different angles.\n",
      "\n",
      "4. **Positional Encoding**  \n",
      "   Even though attention is ‚Äúposition‚Äëagnostic‚Äù, the sinusoidal signals give the model a sense of order without extra learnable parameters.\n",
      "\n",
      "5. **Residual + LayerNorm**  \n",
      "   These tricks keep gradients flowing and training stable, especially when stacking 6 or more layers.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Results in a Nutshell\n",
      "\n",
      "| Model | EN‚ÄëDE BLEU | EN‚ÄëFR BLEU | Training Time |\n",
      "|-------|------------|------------|---------------|\n",
      "| RNN‚ÄëAttention | 28.4 | 38.7 | 48‚ÄØh |\n",
      "| ConvS2S | 29.3 | 39.2 | 42‚ÄØh |\n",
      "| **Transformer‚Äëbase** | **30.6** | **40.3** | **24‚ÄØh** |\n",
      "| Transformer‚Äëbig | 31.4 | 41.0 | 48‚ÄØh |\n",
      "\n",
      "- **Performance**: The base Transformer already beats the best RNN and convolutional models by 2‚Äì3 BLEU points.  \n",
      "- **Speed**: Training is roughly twice as fast because of full parallelism.  \n",
      "- **Scalability**: Larger models (more heads, higher `d_model`) keep improving, but memory usage grows.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Take‚Äëaway Summary (for a Complete Beginner)\n",
      "\n",
      "1. **Attention is the core** ‚Äì it lets every word decide which other words matter most.  \n",
      "2. **Multi‚Äëhead attention** splits this decision into several ‚Äúlenses‚Äù, each capturing a different relationship.  \n",
      "3. **Positional encoding** gives the model a sense of order without learning it from scratch.  \n",
      "4. **The Transformer architecture** stacks a few identical layers of attention + feed‚Äëforward nets, wrapped with residuals and normalization.  \n",
      "5. **Training tricks** (warm‚Äëup, Adam, dropout) make the model learn efficiently.  \n",
      "6. **Result**: A faster, more accurate translation system that has become the foundation for almost all modern NLP models (BERT, GPT, T5, etc.).\n",
      "\n",
      "---\n",
      "\n",
      "#### Want to dive deeper?\n",
      "\n",
      "- **Learning‚Äërate schedule derivation** ‚Äì how the warm‚Äëup formula balances early stability with later speed.  \n",
      "- **Ablation studies** ‚Äì which components matter most (e.g., removing positional encoding hurts a lot).  \n",
      "- **Transfer learning** ‚Äì how the encoder‚Äôs representations can be reused for other tasks.\n",
      "\n",
      "Just let me know which part you‚Äôd like to explore next!\n"
     ]
    }
   ],
   "source": [
    "print(teach_agent_results['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0361ef83-816c-4819-be12-0a3b6c9b0b7c",
   "metadata": {},
   "source": [
    "# Mathematical Reasoning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "bb4ae6da-f8d8-4501-9277-cee763054d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Math_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Mathematical Reasoning Agent. \n",
    "Read the given research paper context carefully, extract all the mathematical formulas and explain , analyze equations, Loss functions, \n",
    "optimization methods.\n",
    "Explain what each equation represents, why it needed and assumptions made so even a complete beginner could understand.\n",
    "present the results in a human-readable format. \n",
    "Compress the result into bullet points.\n",
    "Keep only essential facts, findings, and conclusions.\n",
    "Use the provided tools ONLY if you need.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "915596a6-e651-44bc-9a4a-e0e6a1ec9f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "math_tools = [wiki_tool]\n",
    "math_llm=llm.bind_tools(math_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "9367dba7-8251-494b-88b4-e858c1ca2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_agent = create_tool_calling_agent(math_llm,math_tools ,Math_agent_prompt)\n",
    "math_agent_executor = AgentExecutor(agent=math_agent, tools=math_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "2bd24c5d-b665-4ac9-8192-91df411ac6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m## üìö  ‚ÄúAttention‚ÄØIs‚ÄØAll‚ÄØYou‚ÄØNeed‚Äù ‚Äì Mathematics Made Simple  \n",
      "\n",
      "Below is a **step‚Äëby‚Äëstep walk‚Äëthrough** of every equation that appears in the paper, together with the *why* and the *assumptions* behind each term.  \n",
      "The goal is to make the math understandable even if you have never seen a neural network before.\n",
      "\n",
      "---\n",
      "\n",
      "### 1Ô∏è‚É£  Notation that is used everywhere  \n",
      "\n",
      "| Symbol | Meaning (in plain English) |\n",
      "|--------|----------------------------|\n",
      "| **\\(x\\)** | A vector (a list of numbers). |\n",
      "| **\\(X\\in\\mathbb{R}^{n\\times d}\\)** | A matrix with *n* rows and *d* columns; each row is a *d‚Äëdimensional* vector. |\n",
      "| **\\(d_{\\text{model}}\\)** | Dimension of the model (the size of every token representation). Typical values: 512 (Base) or 1024 (Big). |\n",
      "| **\\(d_k, d_v\\)** | Dimension of *keys* and *values* inside an attention head (usually \\(d_k = d_v = d_{\\text{model}}/h\\)). |\n",
      "| **\\(h\\)** | Number of attention heads (8 in the Base model, 16 in the Big model). |\n",
      "| **\\(Q, K, V\\)** | **Queries**, **Keys**, **Values** ‚Äì three matrices that are derived from the same (or different) source of data. |\n",
      "| **\\(\\odot\\)** | Element‚Äëwise multiplication (not used in the paper, but appears in many derivations). |\n",
      "| **\\(\\text{softmax}(\\cdot)\\)** | Turns a vector of real numbers into a probability distribution (all entries become positive and sum to‚ÄØ1). |\n",
      "| **\\(\\text{ReLU}(z)=\\max(0,z)\\)** | A simple non‚Äëlinear function that ‚Äúclips‚Äù negative numbers to‚ÄØ0. |\n",
      "| **\\(\\text{LayerNorm}(\\cdot)\\)** | Normalises a vector to have zero mean and unit variance *per layer* (helps training). |\n",
      "| **\\(\\text{Dropout}(p)\\)** | Randomly zeroes out each element with probability *p* during training (prevents over‚Äëfitting). |\n",
      "| **\\(\\epsilon\\)** | Small constant used for numerical stability (e.g., \\(10^{-9}\\) in Adam). |\n",
      "\n",
      "---\n",
      "\n",
      "## 2Ô∏è‚É£  Core Building Blocks  \n",
      "\n",
      "### 2.1  Scaled Dot‚ÄëProduct Attention  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Bigl(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\Bigr)\\,V}\n",
      "\\]\n",
      "\n",
      "| Piece | What it does | Why it is there |\n",
      "|------|--------------|-----------------|\n",
      "| **\\(QK^{\\top}\\)** | Computes a *similarity* (dot‚Äëproduct) between every query and every key. If a query and a key point in the same direction, the dot‚Äëproduct is large ‚Üí the model thinks they are related. | Gives the raw ‚Äúattention scores‚Äù. |\n",
      "| **\\(/\\sqrt{d_k}\\)** | Divides by the square‚Äëroot of the key dimension. | Prevents the dot‚Äëproduct values from growing too large when \\(d_k\\) is big; otherwise the softmax would become extremely peaked and gradients would vanish. |\n",
      "| **\\(\\operatorname{softmax}(\\cdot)\\)** | Turns the scores into a probability distribution over all keys (each row sums to‚ÄØ1). | Allows the model to *softly* attend to many positions at once. |\n",
      "| **\\(\\times V\\)** | Takes a weighted sum of the value vectors, using the probabilities as weights. | Produces the final ‚Äúattended‚Äù representation for each query. |\n",
      "\n",
      "**Assumptions**  \n",
      "\n",
      "* All three matrices have the same number of rows (the sequence length) and the same inner dimension \\(d_k\\) (or \\(d_v\\)).  \n",
      "* The dot‚Äëproduct is a reasonable similarity measure for the learned embeddings.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.2  Multi‚ÄëHead Attention  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{MultiHead}(Q,K,V)=\\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)\\,W^{O}}\n",
      "\\]\n",
      "\n",
      "where each head is\n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{head}_i = \\text{Attention}\\bigl(QW_i^{Q},\\; KW_i^{K},\\; VW_i^{V}\\bigr)}\n",
      "\\]\n",
      "\n",
      "| Piece | What it does | Why it is useful |\n",
      "|------|--------------|------------------|\n",
      "| **\\(W_i^{Q},W_i^{K},W_i^{V}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_k}\\)** | Linear projections that turn the original vectors into *head‚Äëspecific* queries, keys, values. | Allows each head to look at the data from a different ‚Äúsub‚Äëspace‚Äù. |\n",
      "| **\\(\\text{head}_i\\)** | A standard scaled‚Äëdot‚Äëproduct attention computed on the projected data. | Each head learns a different type of relationship (e.g., syntax vs. semantics). |\n",
      "| **\\(\\text{Concat}(\\cdot)\\)** | Stacks the \\(h\\) head outputs side‚Äëby‚Äëside, giving a matrix of size \\((\\text{seq\\_len}, h\\cdot d_v)\\). | Merges the diverse information from all heads. |\n",
      "| **\\(W^{O}\\in\\mathbb{R}^{(h d_v)\\times d_{\\text{model}}}\\)** | Final linear projection back to the model dimension. | Keeps the overall architecture size constant, regardless of the number of heads. |\n",
      "\n",
      "**Assumptions**  \n",
      "\n",
      "* The total dimension after concatenation (\\(h\\cdot d_v\\)) equals the model dimension after the final projection, i.e. \\(h\\cdot d_v = d_{\\text{model}}\\).  \n",
      "* Linear projections are learned parameters; they are trained jointly with everything else.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.3  Position‚Äëwise Feed‚ÄëForward Network (FFN)  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{FFN}(x)=\\underbrace{\\max\\bigl(0,\\,xW_{1}+b_{1}\\bigr)}_{\\text{ReLU}}\\,W_{2}+b_{2}}\n",
      "\\]\n",
      "\n",
      "| Piece | What it does | Why it is there |\n",
      "|------|--------------|-----------------|\n",
      "| **\\(W_{1}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{\\text{ff}}}\\)** | First linear map that expands the dimension (commonly \\(d_{\\text{ff}}=2048\\) for Base). | Gives the network capacity to model complex, non‚Äëlinear transformations. |\n",
      "| **ReLU** | Zeroes out negative values. | Introduces non‚Äëlinearity, which is essential for expressive power. |\n",
      "| **\\(W_{2}\\in\\mathbb{R}^{d_{\\text{ff}}\\times d_{\\text{model}}}\\)** | Projects back to the original model dimension. | Allows the output to be added back to the residual connection (same shape as the input). |\n",
      "| **\\(b_{1},b_{2}\\)** | Bias vectors (learned offsets). | Improves flexibility. |\n",
      "\n",
      "**Assumptions**  \n",
      "\n",
      "* The same FFN is applied **independently** to each position in the sequence (hence ‚Äúposition‚Äëwise‚Äù).  \n",
      "* The two linear layers share parameters across all positions, which keeps the model size modest.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.4  Positional Encoding  \n",
      "\n",
      "Because the Transformer contains **no recurrence or convolution**, it needs a way to tell the model *where* each token sits in the sequence.\n",
      "\n",
      "\\[\n",
      "\\begin{aligned}\n",
      "\\text{PE}(pos,2i)   &= \\sin\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\\\\[4pt]\n",
      "\\text{PE}(pos,2i+1) &= \\cos\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\n",
      "\\end{aligned}\n",
      "\\]\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| **\\(pos\\)** | Position index (0‚Äëbased) of the token in the sentence. |\n",
      "| **\\(i\\)** | Dimension index inside the embedding vector (0‚Äëbased). |\n",
      "| **\\(d_{\\text{model}}\\)** | Total embedding size. |\n",
      "\n",
      "**Why sinusoidal?**  \n",
      "\n",
      "* The functions are **periodic** and each dimension has a different frequency.  \n",
      "* Any relative offset (e.g., ‚Äúthe token 3 steps later‚Äù) can be expressed as a linear combination of the encodings, which makes it easy for the model to learn *relative* positions.  \n",
      "* No extra learned parameters are needed (the paper also shows that learned positional embeddings work similarly).\n",
      "\n",
      "**Assumptions**  \n",
      "\n",
      "* The maximum sequence length during training is known (e.g., 512 tokens) so the sinusoid table can be pre‚Äëcomputed.  \n",
      "* Adding the encoding to the token embedding is a simple element‚Äëwise sum.\n",
      "\n",
      "---\n",
      "\n",
      "### 2.5  Residual Connection + Layer Normalisation  \n",
      "\n",
      "For every sub‚Äëlayer (attention or FFN) the paper uses:\n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\text{Output}= \\text{LayerNorm}\\bigl(x + \\text{Sublayer}(x)\\bigr)}\n",
      "\\]\n",
      "\n",
      "* **\\(x\\)** ‚Äì input to the sub‚Äëlayer.  \n",
      "* **\\(\\text{Sublayer}(x)\\)** ‚Äì either Multi‚ÄëHead Attention or FFN (with dropout applied).  \n",
      "\n",
      "**Why?**  \n",
      "\n",
      "* **Residual (skip) connection** lets gradients flow directly through many layers, solving the ‚Äúvanishing gradient‚Äù problem.  \n",
      "* **LayerNorm** stabilises the hidden‚Äëstate distribution, allowing a larger learning rate and faster convergence.\n",
      "\n",
      "---\n",
      "\n",
      "## 3Ô∏è‚É£  Training Objective (Loss)  \n",
      "\n",
      "### 3Ô∏è‚É£ 1‚ÄØLabel‚ÄëSmoothed Cross‚ÄëEntropy  \n",
      "\n",
      "The model predicts a probability distribution \\(\\hat{y}\\) over the target vocabulary for each position. The ground‚Äëtruth one‚Äëhot vector is \\(y\\). With label smoothing \\(\\epsilon\\) (set to 0.1 in the paper) the target distribution becomes:\n",
      "\n",
      "\\[\n",
      "\\tilde{y}= (1-\\epsilon)\\,y + \\epsilon \\,\\frac{1}{V}\n",
      "\\]\n",
      "\n",
      "where **\\(V\\)** is the vocabulary size.\n",
      "\n",
      "The loss for a single token is the **cross‚Äëentropy** between \\(\\tilde{y}\\) and \\(\\hat{y}\\):\n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\mathcal{L}_{\\text{CE}} = -\\sum_{v=1}^{V}\\tilde{y}_v \\,\\log \\hat{y}_v}\n",
      "\\]\n",
      "\n",
      "**Why label smoothing?**  \n",
      "\n",
      "* Prevents the model from becoming *over‚Äëconfident* (probability‚ÄØ=‚ÄØ1 for the correct token).  \n",
      "* Acts as a regulariser, improving generalisation and BLEU scores.\n",
      "\n",
      "The total loss is the sum (or average) of \\(\\mathcal{L}_{\\text{CE}}\\) over all positions in the batch.\n",
      "\n",
      "---\n",
      "\n",
      "## 4Ô∏è‚É£  Optimisation Details  \n",
      "\n",
      "### 4Ô∏è‚É£ 1‚ÄØAdam Optimiser  \n",
      "\n",
      "Adam updates parameters \\(\\theta\\) using estimates of first‚Äë and second‚Äëorder moments of the gradients:\n",
      "\n",
      "\\[\n",
      "\\begin{aligned}\n",
      "m_t &= \\beta_1 m_{t-1} + (1-\\beta_1) g_t \\\\\n",
      "v_t &= \\beta_2 v_{t-1} + (1-\\beta_2) g_t^{2} \\\\\n",
      "\\hat{m}_t &= \\frac{m_t}{1-\\beta_1^{t}} \\\\\n",
      "\\hat{v}_t &= \\frac{v_t}{1-\\beta_2^{t}} \\\\\n",
      "\\theta_{t+1} &= \\theta_t - \\eta_t \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
      "\\end{aligned}\n",
      "\\]\n",
      "\n",
      "* **\\(g_t\\)** ‚Äì gradient of the loss at step *t*.  \n",
      "* **\\(\\beta_1=0.9,\\; \\beta_2=0.98,\\; \\epsilon=10^{-9}\\)** (paper‚Äôs settings).  \n",
      "\n",
      "**Why Adam?**  \n",
      "\n",
      "* Adaptive learning rates per parameter make training stable even with the large, sparse gradients typical of language models.\n",
      "\n",
      "### 4Ô∏è‚É£‚ÄØ2‚ÄØLearning‚ÄëRate Schedule (Warm‚Äëup + Decay)  \n",
      "\n",
      "The learning rate \\(\\eta_t\\) is not constant; it follows:\n",
      "\n",
      "\\[\n",
      "\\boxed{\\displaystyle\n",
      "\\eta_t = d_{\\text{model}}^{-0.5}\\; \\min\\!\\bigl(t^{-0.5},\\; t\\; \\text{warmup}^{-1.5}\\bigr)}\n",
      "\\]\n",
      "\n",
      "* **\\(t\\)** ‚Äì current training step (starting from 1).  \n",
      "* **\\(\\text{warmup}=4000\\)** steps (as used in the paper).  \n",
      "\n",
      "**Interpretation**  \n",
      "\n",
      "* **Warm‚Äëup phase** (\\(t < \\text{warmup}\\)): \\(\\eta_t \\propto t\\) ‚Äì the LR grows linearly, preventing huge updates when the model is still random.  \n",
      "* **After warm‚Äëup** (\\(t \\ge \\text{warmup}\\)): \\(\\eta_t \\propto t^{-0.5}\\) ‚Äì the LR decays slowly, allowing fine‚Äëgrained convergence.\n",
      "\n",
      "The factor \\(d_{\\text{model}}^{-0.5}\\) normalises the LR for different model sizes (Base vs. Big).\n",
      "\n",
      "---\n",
      "\n",
      "### 4Ô∏è‚É£‚ÄØ3‚ÄØOther Training Tricks  \n",
      "\n",
      "| Trick | What it does | Why it helps |\n",
      "|------|--------------|--------------|\n",
      "| **Dropout (p‚ÄØ=‚ÄØ0.1)** | Randomly zeroes activations in attention weights, FFN, and after embeddings. | Reduces over‚Äëfitting, especially on large corpora. |\n",
      "| **Label smoothing (Œµ‚ÄØ=‚ÄØ0.1)** | Already described in the loss section. | Improves calibration and BLEU. |\n",
      "| **Beam search (beam‚ÄØ=‚ÄØ4)** | At inference, keeps the top‚Äë4 partial translations at each step. | Gives higher‚Äëquality outputs than greedy decoding. |\n",
      "| **Weight initialisation** | Parameters are drawn from a normal distribution with variance \\(1/d_{\\text{model}}\\). | Keeps the scale of activations roughly constant across layers. |\n",
      "\n",
      "---\n",
      "\n",
      "## 5Ô∏è‚É£  Putting It All Together ‚Äì Forward Pass of One Encoder Layer  \n",
      "\n",
      "1. **Input**: token embeddings \\(X\\in\\mathbb{R}^{L\\times d_{\\text{model}}}\\) (plus positional encoding).  \n",
      "2. **Self‚ÄëAttention Sub‚Äëlayer**  \n",
      "   * Compute \\(Q= XW^{Q},\\; K= XW^{K},\\; V= XW^{V}\\).  \n",
      "   * Apply **Multi‚ÄëHead Attention** ‚Üí output \\(A\\).  \n",
      "   * Apply **Dropout**, add residual \\(X+A\\), then **LayerNorm** ‚Üí \\(X_1\\).  \n",
      "3. **Feed‚ÄëForward Sub‚Äëlayer**  \n",
      "   * Compute \\(\\text{FFN}(X_1)\\) ‚Üí \\(F\\).  \n",
      "   * Apply **Dropout**, add residual \\(X_1+F\\), then **LayerNorm** ‚Üí final output of the layer.  \n",
      "\n",
      "The **decoder** adds a second attention block that attends to the encoder‚Äôs final representations, and the self‚Äëattention is *masked* so a position cannot look at future tokens.\n",
      "\n",
      "---\n",
      "\n",
      "## 6Ô∏è‚É£  Quick Reference Cheat‚ÄëSheet  \n",
      "\n",
      "| Equation | Plain‚ÄëEnglish Description |\n",
      "|----------|---------------------------|\n",
      "| \\(\\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\bigl(QK^{\\top}/\\sqrt{d_k}\\bigr)V\\) | ‚ÄúTake similarity of queries & keys, turn into probabilities, and blend the values accordingly.‚Äù |\n",
      "| \\(\\text{head}_i = \\text{Attention}(QW_i^{Q}, KW_i^{K}, VW_i^{V})\\) | One attention head works on a *projected* version of the data. |\n",
      "| \\(\\text{MultiHead}= \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h)W^{O}\\) | Stitch together all heads and linearly mix them back to the model size. |\n",
      "| \\(\\text{FFN}(x)=\\max(0, xW_1+b_1)W_2+b_2\\) | Two linear layers with a ReLU in between, applied to each token separately. |\n",
      "| \\(\\text{PE}(pos,2i)=\\sin(pos/10000^{2i/d_{\\text{model}}})\\) <br> \\(\\text{PE}(pos,2i+1)=\\cos(pos/10000^{2i/d_{\\text{model}}})\\) | Fixed sinusoidal signals that tell the model ‚ÄúI‚Äôm at position *pos*‚Äù. |\n",
      "| \\(\\text{Output}= \\text{LayerNorm}(x + \\text{Sublayer}(x))\\) | Residual connection + normalisation for stable, deep training. |\n",
      "| \\(\\tilde{y}= (1-\\epsilon)y + \\epsilon/V\\) | Smoothed target distribution (Œµ‚ÄØ=‚ÄØ0.1). |\n",
      "| \\(\\mathcal{L}_{\\text{CE}} = -\\sum_v \\tilde{y}_v \\log \\hat{y}_v\\) | Cross‚Äëentropy loss between smoothed target and model prediction. |\n",
      "| \\(\\eta_t = d_{\\text{model}}^{-0.5}\\min(t^{-0.5}, t\\;\\text{warmup}^{-1.5})\\) | Learning‚Äërate schedule: linear warm‚Äëup ‚Üí inverse‚Äësquare‚Äëroot decay. |\n",
      "| Adam update (Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.98, Œµ=10‚Åª‚Åπ) | Adaptive optimiser that keeps a moving average of gradients and their squares. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7Ô∏è‚É£  TL;DR ‚Äì What the Math Gives You  \n",
      "\n",
      "* **Self‚Äëattention** lets every token look at every other token *in parallel* (no recurrence).  \n",
      "* **Scaling by \\(\\sqrt{d_k}\\)** keeps the softmax well‚Äëbehaved.  \n",
      "* **Multiple heads** let the model capture different kinds of relationships simultaneously.  \n",
      "* **Feed‚Äëforward networks** add non‚Äëlinearity and increase capacity.  \n",
      "* **Positional encodings** inject order information that would otherwise be missing.  \n",
      "* **Residual + LayerNorm** make deep stacks (6‚Äë12 layers) trainable.  \n",
      "* **Label‚Äësmoothed cross‚Äëentropy** + **Adam** + **warm‚Äëup schedule** give a stable, fast‚Äëconverging optimisation process.  \n",
      "\n",
      "All of these pieces together form the **Transformer**‚Äîthe first architecture that relies *only* on attention and still beats recurrent and convolutional models on machine translation, while being dramatically faster to train.  \n",
      "\n",
      "--- \n",
      "\n",
      "*Feel free to ask for any part to be expanded (e.g., a deeper dive into the Adam derivation or a numeric example of attention scores!).*\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "math_agent_results = math_agent_executor.invoke({'input':'explain all the mathematical terms in this research paper'+ 'context- '+str(result_doc_agent['output'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "822cf0b5-d67c-4955-9bb7-a72ddee958e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Attention‚ÄØIs‚ÄØAll‚ÄØYou‚ÄØNeed (Vaswani‚ÄØet‚ÄØal.,‚ÄØ2017)**  \n",
      "A beginner‚Äëfriendly walk‚Äëthrough of every mathematical expression that appears in the paper, together with the intuition behind it, why it is needed, and the assumptions that underlie it.\n",
      "\n",
      "---\n",
      "\n",
      "## 1.  Quick Overview of the Model\n",
      "\n",
      "| Part | What it does | Main symbols |\n",
      "|------|--------------|--------------|\n",
      "| **Input embedding** | Turns each token (word‚Äëpiece, character, etc.) into a dense vector of size `d_model`. | `x‚ÇÅ,‚Ä¶,x‚Çô ‚àà ‚Ñù^{d_model}` |\n",
      "| **Positional encoding** | Gives the model a notion of ‚Äúorder‚Äù because attention itself is order‚Äëagnostic. | `PE(pos, i)` |\n",
      "| **Encoder** | 6 identical layers, each containing **(a)** Multi‚ÄëHead Self‚ÄëAttention and **(b)** a Position‚Äëwise Feed‚ÄëForward Network (FFN). | `Enc‚ÇÅ,‚Ä¶,Enc‚ÇÜ` |\n",
      "| **Decoder** | 6 identical layers, each containing **(a)** Masked Multi‚ÄëHead Self‚ÄëAttention, **(b)** Multi‚ÄëHead Encoder‚ÄëDecoder Attention, **(c)** FFN. | `Dec‚ÇÅ,‚Ä¶,Dec‚ÇÜ` |\n",
      "| **Output projection + softmax** | Turns the final decoder vector into a probability distribution over the target vocabulary. | `softmax(W_o z + b_o)` |\n",
      "| **Loss** | Cross‚Äëentropy between the predicted distribution and the true next token (with label‚Äësmoothing). | `‚Ñí` |\n",
      "| **Training** | Adam optimizer + a custom learning‚Äërate schedule. | `Œ≤‚ÇÅ,Œ≤‚ÇÇ,Œµ, step, warmup` |\n",
      "\n",
      "All the heavy lifting is done by **attention**; everything else (embeddings, FFNs, residual connections, layer‚Äënorm, dropout) is standard deep‚Äëlearning machinery.\n",
      "\n",
      "---\n",
      "\n",
      "## 2.  Notation that Appears Throughout\n",
      "\n",
      "| Symbol | Meaning |\n",
      "|--------|---------|\n",
      "| `n` | Length of the source (input) sequence. |\n",
      "| `m` | Length of the target (output) sequence. |\n",
      "| `d_model` | Dimensionality of every token representation (default 512). |\n",
      "| `d_k , d_v` | Dimensionalities of the *key* and *value* vectors inside an attention head (`d_k = d_v = d_model / h`). |\n",
      "| `h` | Number of attention heads (default 8). |\n",
      "| `Q, K, V` | Matrices of *queries*, *keys*, and *values* (each shape `seq_len √ó d_k`). |\n",
      "| `W_i^Q , W_i^K , W_i^V , W^O` | Learnable linear projection matrices for head *i* and the final output projection. |\n",
      "| `softmax(z)_j = e^{z_j} / Œ£_k e^{z_k}` | Normalises a vector into a probability distribution. |\n",
      "| `¬∑` | Matrix multiplication (or dot‚Äëproduct when vectors). |\n",
      "| `‚äï` | Concatenation of vectors (e.g., stacking the heads). |\n",
      "| `ReLU(x) = max(0, x)` | Rectified Linear Unit activation. |\n",
      "| `LN(¬∑)` | Layer Normalisation (normalises across the feature dimension). |\n",
      "| `Dropout(p)` | Randomly zeroes each element with probability `p`. |\n",
      "| `‚äï` (residual) | ‚ÄúAdd the input to the output of a sub‚Äëlayer‚Äù: `x + Sublayer(x)`. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3.  Core Equations ‚Äì What They Mean and Why They Exist\n",
      "\n",
      "### 3.1 Scaled Dot‚ÄëProduct Attention  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Bigl(\\frac{QK^{\\!\\top}}{\\sqrt{d_k}}\\Bigr)\\,V}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `QK·µÄ` | Computes a similarity score between every **query** and every **key**. If `Q` has shape `(t_q √ó d_k)` and `K` `(t_k √ó d_k)`, the result is a `(t_q √ó t_k)` matrix where entry *(i,j)* tells how much the *i‚Äëth* query ‚Äúlooks at‚Äù the *j‚Äëth* key. |\n",
      "| `/‚àöd_k` | **Scaling**. When `d_k` is large, the dot‚Äëproduct values can become large in magnitude, pushing the softmax into regions with very small gradients (the ‚Äúsoftmax saturation‚Äù problem). Dividing by `‚àöd_k` keeps the variance of the scores roughly constant, stabilising training. |\n",
      "| `softmax` | Turns the similarity scores into **attention weights** that sum to 1 across the *key* dimension. Each weight can be interpreted as ‚Äúhow much should we attend to that key‚Äù. |\n",
      "| `¬∑V` | Takes a weighted sum of the **value** vectors. The output for each query is a mixture of the values, where the mixture coefficients are the attention weights. |\n",
      "\n",
      "**Why we need it**  \n",
      "- Provides a *content‚Äëbased* way for each position to gather information from all other positions.  \n",
      "- No recurrence or convolution is required; the whole operation can be expressed as a few matrix multiplications, which GPUs execute very efficiently.\n",
      "\n",
      "**Assumptions**  \n",
      "- The similarity between a query and a key can be captured by a simple dot product.  \n",
      "- All positions are equally reachable (global receptive field) ‚Äì the model must learn to ignore irrelevant positions via the softmax.\n",
      "\n",
      "---\n",
      "\n",
      "### 3.2 Multi‚ÄëHead Attention  \n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\begin{aligned}\n",
      "\\text{head}_i &= \\text{Attention}\\bigl(QW_i^{Q},\\; KW_i^{K},\\; VW_i^{V}\\bigr) \\quad (i=1\\ldots h)\\\\[4pt]\n",
      "\\text{MultiHead}(Q,K,V) &= \\bigl(\\text{head}_1 \\oplus \\dots \\oplus \\text{head}_h\\bigr)W^{O}\n",
      "\\end{aligned}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `W_i^Q , W_i^K , W_i^V` | Linear maps that **project** the original `d_model`‚Äëdimensional vectors into a lower‚Äëdimensional sub‚Äëspace (`d_k` or `d_v`). Each head works in its own sub‚Äëspace, allowing the model to attend to different types of relationships simultaneously (e.g., syntax vs. semantics). |\n",
      "| `head_i` | The output of a *single* attention head, still of shape `(seq_len √ó d_v)`. |\n",
      "| `‚äï` (concatenation) | Stacks the `h` heads back together, giving a tensor of shape `(seq_len √ó h¬∑d_v) = (seq_len √ó d_model)`. |\n",
      "| `W^O` | A final linear projection that mixes the information from all heads back into the original `d_model` space. |\n",
      "\n",
      "**Why we need multiple heads**  \n",
      "- A single attention head can only focus on one kind of similarity (because it uses a single query/key space). By having many heads, the model can learn **different** similarity functions in parallel, enriching the representation.  \n",
      "- Empirically, `h = 8` works well for `d_model = 512`.\n",
      "\n",
      "**Assumptions**  \n",
      "- The different heads are independent enough that learning them jointly is beneficial.  \n",
      "- The linear projections are sufficient to transform the data into useful sub‚Äëspaces; no non‚Äëlinearities are applied inside the attention itself.\n",
      "\n",
      "---\n",
      "\n",
      "### 3.3 Position‚Äëwise Feed‚ÄëForward Network (FFN)  \n",
      "\n",
      "\\[\n",
      "\\boxed{\\text{FFN}(x)=\\max\\bigl(0,\\,xW_{1}+b_{1}\\bigr)W_{2}+b_{2}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `x` | A single token representation (shape `1 √ó d_model`). The same FFN is applied **independently** to every position (hence ‚Äúposition‚Äëwise‚Äù). |\n",
      "| `W‚ÇÅ ‚àà ‚Ñù^{d_model√ód_ff}` , `b‚ÇÅ` | First linear layer that expands the dimensionality to a larger hidden size `d_ff` (default 2048). |\n",
      "| `ReLU` (`max(0,¬∑)`) | Introduces non‚Äëlinearity, allowing the network to model complex functions. |\n",
      "| `W‚ÇÇ ‚àà ‚Ñù^{d_ff√ód_model}` , `b‚ÇÇ` | Projects back to the original `d_model`. |\n",
      "| **Why** | The attention sub‚Äëlayer mixes information **across** positions, but it does not change the representation of each position individually. The FFN adds a *per‚Äëposition* transformation, giving the model extra capacity to reshape the representation after each attention step. |\n",
      "| **Assumptions** | The same transformation works for all positions (weight sharing), which is reasonable because the same kind of processing (e.g., ‚Äúapply a non‚Äëlinear filter‚Äù) is useful everywhere in a sentence. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3.4 Positional Encoding  \n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\begin{aligned}\n",
      "\\text{PE}_{(pos,2i)}   &= \\sin\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\\\\\n",
      "\\text{PE}_{(pos,2i+1)} &= \\cos\\!\\Bigl(\\frac{pos}{10000^{\\,2i/d_{\\text{model}}}}\\Bigr)\n",
      "\\end{aligned}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `pos` | Integer index of the token in the sequence (0‚Äëbased). |\n",
      "| `i` | Dimension index (0 ‚Ä¶ `d_model/2 - 1`). |\n",
      "| **Why sinusoidal?** | The functions are **deterministic** (no learned parameters) and have the property that any fixed offset `k` corresponds to a linear transformation of the encoding: `PE_{pos+k}` can be expressed as a linear combination of `PE_{pos}`. This makes it easy for the model to learn **relative** positions (e.g., ‚Äúthe next word‚Äù). |\n",
      "| **Why both sin and cos?** | Alternating sin/cos gives each dimension a different frequency, ensuring that each position gets a unique encoding and that the encodings are smoothly varying. |\n",
      "| **Assumptions** | A fixed, non‚Äëlearned encoding is sufficient for the model to infer order. Later work (e.g., learned positional embeddings) shows that learning can sometimes improve performance, but the original paper chose the simpler sinusoidal form. |\n",
      "\n",
      "The positional encoding vector is **added** to the token embedding before the first encoder layer:\n",
      "\n",
      "\\[\n",
      "\\mathbf{z}_0 = \\text{Embedding}(x) + \\text{PE}\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "### 3.5 Residual Connection + Layer Normalisation  \n",
      "\n",
      "For any sub‚Äëlayer (attention or FFN) we compute:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\begin{aligned}\n",
      "\\mathbf{y} &= \\text{Sublayer}(\\mathbf{x})\\\\\n",
      "\\mathbf{z} &= \\text{LayerNorm}\\bigl(\\mathbf{x} + \\mathbf{y}\\bigr)\n",
      "\\end{aligned}}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `x` | Input to the sub‚Äëlayer (shape `seq_len √ó d_model`). |\n",
      "| `y` | Output of the sub‚Äëlayer (same shape). |\n",
      "| `x + y` | **Residual (skip) connection** ‚Äì helps gradients flow through many layers and mitigates the vanishing‚Äëgradient problem. |\n",
      "| `LayerNorm` | Normalises each token‚Äôs vector to zero mean and unit variance (computed across the feature dimension). This stabilises training and allows the model to use higher learning rates. |\n",
      "| **Assumptions** | Adding the input does not destroy the information because the sub‚Äëlayer learns a *correction* rather than a completely new representation. Layer‚Äënorm assumes that normalising across features is beneficial for all tokens, which holds empirically. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3.6 Output Projection & Softmax (Prediction)\n",
      "\n",
      "After the final decoder layer we have a vector `z_t` for each target position `t`. The model predicts the next token with:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "p\\bigl(y_t = w \\mid y_{<t}, x\\bigr) = \\operatorname{softmax}\\!\\bigl( W_{\\text{out}}\\,z_t + b_{\\text{out}} \\bigr)_w}\n",
      "\\]\n",
      "\n",
      "- `W_out ‚àà ‚Ñù^{d_model √ó |V|}` maps the hidden vector into a logit for each word `w` in the target vocabulary `V`.  \n",
      "- The softmax turns logits into a probability distribution.\n",
      "\n",
      "---\n",
      "\n",
      "### 3.7 Loss Function ‚Äì Label‚ÄëSmoothed Cross‚ÄëEntropy  \n",
      "\n",
      "The standard loss for sequence‚Äëto‚Äësequence models is the **cross‚Äëentropy** between the predicted distribution and the one‚Äëhot ground‚Äëtruth token. The paper adds **label smoothing** (Œµ = 0.1) to regularise the model:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\mathcal{L} = -\\sum_{t=1}^{m}\\sum_{w\\in V}\n",
      "\\tilde{p}_t(w)\\,\\log p_t(w)\n",
      "}\n",
      "\\]\n",
      "\n",
      "where  \n",
      "\n",
      "\\[\n",
      "\\tilde{p}_t(w) = \n",
      "\\begin{cases}\n",
      "1-\\epsilon & \\text{if } w = y_t^{\\text{true}}\\\\[4pt]\n",
      "\\epsilon / (|V|-1) & \\text{otherwise}\n",
      "\\end{cases}\n",
      "\\]\n",
      "\n",
      "| Piece | Explanation |\n",
      "|------|--------------|\n",
      "| `p_t(w)` | Model‚Äôs predicted probability for word `w` at step `t`. |\n",
      "| `\\tilde{p}_t(w)` | Smoothed target distribution: the true word gets most of the probability mass, but a small uniform mass `Œµ/(|V|-1)` is spread over all other words. |\n",
      "| **Why label smoothing?** | Prevents the model from becoming *over‚Äëconfident* (probability 1 on the correct token). This improves generalisation and yields better BLEU scores. |\n",
      "| **Assumptions** | The true label is still the most likely, but we tolerate a tiny amount of uncertainty. |\n",
      "\n",
      "---\n",
      "\n",
      "### 3.8 Optimisation ‚Äì Adam + Custom Learning‚ÄëRate Schedule  \n",
      "\n",
      "**Adam optimiser** (Kingma & Ba, 2015) updates parameters `Œ∏` using estimates of first‚Äë and second‚Äëorder moments of the gradients:\n",
      "\n",
      "\\[\n",
      "\\begin{aligned}\n",
      "m_t &= \\beta_1 m_{t-1} + (1-\\beta_1) g_t\\\\\n",
      "v_t &= \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\\\\\n",
      "\\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t},\\qquad\n",
      "\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}\\\\\n",
      "\\theta_{t+1} &= \\theta_t - \\alpha_t \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
      "\\end{aligned}\n",
      "\\]\n",
      "\n",
      "- Hyper‚Äëparameters used in the paper: `Œ≤‚ÇÅ = 0.9`, `Œ≤‚ÇÇ = 0.98`, `Œµ = 10‚Åª‚Å∏`.  \n",
      "- `Œ±_t` (the **learning rate**) is **not constant**; it follows a schedule specially designed for the Transformer:\n",
      "\n",
      "\\[\n",
      "\\boxed{\n",
      "\\alpha_t = d_{\\text{model}}^{-0.5}\\,\n",
      "\\min\\!\\bigl(t^{-0.5},\\; t \\cdot \\text{warmup\\_steps}^{-1.5}\\bigr)\n",
      "}\n",
      "\\]\n",
      "\n",
      "| Phase | Behaviour | Intuition |\n",
      "|-------|-----------|-----------|\n",
      "| **Warm‚Äëup** (`t ‚â§ warmup_steps`) | `Œ±_t ‚àù t / warmup_steps^{1.5}` ‚Üí **increases linearly**. | Early in training the model is fragile; a tiny learning rate would make progress extremely slow. Gradually increasing it lets the optimizer find a good region of the loss surface. |\n",
      "| **Post‚Äëwarm‚Äëup** (`t > warmup_steps`) | `Œ±_t ‚àù t^{-0.5}` ‚Üí **decreases proportionally to the inverse square root of step**. | As training proceeds we want smaller steps to fine‚Äëtune the parameters. The `d_model^{-0.5}` factor normalises the scale of the learning rate with respect to the model size (larger models need smaller steps). |\n",
      "| `warmup_steps = 4000` (default) | Determines where the peak learning rate occurs. | Empirically works well for the base model (512‚Äëdim). Larger models may use a different warm‚Äëup length. |\n",
      "\n",
      "**Why this schedule?**  \n",
      "- The authors observed that a *fixed* learning rate caused either divergence (if too large) or very slow convergence (if too small).  \n",
      "- The schedule is simple, has only one tunable hyper‚Äëparameter (`warmup_steps`), and works robustly across many tasks.\n",
      "\n",
      "---\n",
      "\n",
      "## 4.  Putting It All Together ‚Äì Forward Pass of One Encoder Layer\n",
      "\n",
      "For a single encoder layer (ignoring dropout for clarity):\n",
      "\n",
      "1. **Self‚ÄëAttention Sub‚Äëlayer**  \n",
      "   \\[\n",
      "   \\begin{aligned}\n",
      "   \\mathbf{a} &= \\text{MultiHead}(\\mathbf{z},\\mathbf{z},\\mathbf{z})\\\\\n",
      "   \\mathbf{z}' &= \\text{LayerNorm}(\\mathbf{z} + \\mathbf{a})\n",
      "   \\end{aligned}\n",
      "   \\]\n",
      "\n",
      "2. **Feed‚ÄëForward Sub‚Äëlayer**  \n",
      "   \\[\n",
      "   \\begin{aligned}\n",
      "   \\mathbf{f} &= \\text{FFN}(\\mathbf{z}')\\\\\n",
      "   \\mathbf{z}'' &= \\text{LayerNorm}(\\mathbf{z}' + \\mathbf{f})\n",
      "   \\end{aligned}\n",
      "   \\]\n",
      "\n",
      "`z` is the input to the layer (either the embedding+positional encoding for the first layer, or the output of the previous layer). The same pattern (attention ‚Üí add‚Äë&‚Äënorm ‚Üí FFN ‚Üí add‚Äë&‚Äënorm) repeats for each of the 6 encoder layers.\n",
      "\n",
      "The decoder layers are analogous, but they contain **two** attention sub‚Äëlayers (masked self‚Äëattention and encoder‚Äëdecoder attention) before the FFN.\n",
      "\n",
      "---\n",
      "\n",
      "## 5.  Summary of Assumptions & Design Choices\n",
      "\n",
      "| Design element | Core assumption(s) | Reason for inclusion |\n",
      "|----------------|--------------------|----------------------|\n",
      "| **Scaled dot‚Äëproduct** | Dot‚Äëproduct similarity is a good proxy for ‚Äúrelevance‚Äù. Scaling prevents softmax saturation. | Simple, fast, differentiable similarity measure. |\n",
      "| **Multi‚Äëhead** | Different linear projections can capture distinct relational patterns. | Increases model capacity without blowing up computational cost (heads run in parallel). |\n",
      "| **Position‚Äëwise FFN** | Same per‚Äëposition transformation works for all tokens. | Adds non‚Äëlinearity and depth beyond what attention alone provides. |\n",
      "| **Sinusoidal positional encoding** | Fixed periodic functions give unique, smoothly varying encodings; relative positions can be expressed linearly. | No extra parameters, works out‚Äëof‚Äëthe‚Äëbox. |\n",
      "| **Residual + LayerNorm** | Adding the input helps gradient flow; normalising stabilises training. | Enables deep (6‚Äëlayer) stacks to be trained efficiently. |\n",
      "| **Label smoothing** | Over‚Äëconfident predictions hurt generalisation. | Improves BLEU scores and reduces over‚Äëfitting. |\n",
      "| **Adam + warm‚Äëup schedule** | Adaptive moments help with noisy gradients; a warm‚Äëup phase avoids early divergence. | Empirically yields fast, stable convergence. |\n",
      "| **Dropout (p=0.1)** | Regularisation needed to prevent over‚Äëfitting on relatively small translation corpora. | Simple stochastic regulariser compatible with all sub‚Äëlayers. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6.  Quick Reference Cheat‚ÄëSheet (All Formulas in One Place)\n",
      "\n",
      "| # | Formula | Where it appears |\n",
      "|---|---------|------------------|\n",
      "| 1 | \\(\\displaystyle \\text{Attention}(Q,K,V)=\\operatorname{softmax}\\!\\Bigl(\\frac{QK^{\\!\\top}}{\\sqrt{d_k}}\\Bigr)V\\) | Core attention operation |\n",
      "| 2 | \\(\\displaystyle \\text{head}_i = \\text{Attention}(QW_i^{Q}, KW_i^{K}, VW_i^{V})\\) | One head of Multi‚ÄëHead |\n",
      "| 3 | \\(\\displaystyle \\text{MultiHead}(Q,K,V)=\\bigl(\\text{head}_1\\oplus\\cdots\\oplus\\text{head}_h\\bigr)W^{O}\\) | Full Multi‚ÄëHead |\n",
      "| 4 | \\(\\displaystyle \\text{FFN}(x)=\\max(0, xW_{1}+b_{1})W_{2}+b_{2}\\) | Position‚Äëwise feed‚Äëforward |\n",
      "| 5 | \\(\\displaystyle \\text{PE}_{(pos,2i)} = \\sin\\!\\Bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\Bigr),\\quad \\text{PE}_{(pos,2i+1)} = \\cos\\!\\Bigl(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\Bigr)\\) | Positional encoding |\n",
      "| 6 | \\(\\displaystyle \\mathbf{z} = \\text{LayerNorm}(\\mathbf{x} + \\text{Sublayer}(\\mathbf{x}))\\) | Residual + LayerNorm |\n",
      "| 7 | \\(\\displaystyle p(y_t = w) = \\operatorname{softmax}(W_{\\text{out}}z_t + b_{\\text{out}})_w\\) | Output distribution |\n",
      "| 8 | \\(\\displaystyle \\tilde{p}_t(w) = \\begin{cases}1-\\epsilon & w = y_t^{\\text{true}}\\\\ \\epsilon/(|V|-1) & \\text{otherwise}\\end{cases}\\) | Label‚Äësmoothed target |\n",
      "| 9 | \\(\\displaystyle \\mathcal{L}= -\\sum_{t}\\sum_{w}\\tilde{p}_t(w)\\log p_t(w)\\) | Training loss |\n",
      "|10| \\(\\displaystyle \\alpha_t = d_{\\text{model}}^{-0.5}\\min\\!\\bigl(t^{-0.5},\\, t\\cdot\\text{warmup}^{-1.5}\\bigr)\\) | Learning‚Äërate schedule |\n",
      "|11| Adam update equations (see Section‚ÄØ3.8) | Optimiser |\n",
      "\n",
      "---\n",
      "\n",
      "## 7.  How a Beginner Might Implement a Tiny Transformer (Pseudo‚Äëcode)\n",
      "\n",
      "```python\n",
      "# Assume: torch, numpy, etc. are imported\n",
      "d_model = 512\n",
      "h = 8\n",
      "d_k = d_v = d_model // h          # 64\n",
      "d_ff = 2048\n",
      "\n",
      "# 1. Linear projections for each head (learned parameters)\n",
      "W_Q = [nn.Linear(d_model, d_k) for _ in range(h)]\n",
      "W_K = [nn.Linear(d_model, d_k) for _ in range(h)]\n",
      "W_V = [nn.Linear(d_model, d_k) for _ in range(h)]\n",
      "W_O = nn.Linear(h * d_v, d_model)\n",
      "\n",
      "def scaled_dot_product_attention(Q, K, V):\n",
      "    scores = Q @ K.transpose(-2, -1) / math.sqrt(d_k)   # (batch, seq_q, seq_k)\n",
      "    attn   = torch.softmax(scores, dim=-1)              # attention weights\n",
      "    return attn @ V                                      # weighted sum\n",
      "\n",
      "def multi_head(Q, K, V):\n",
      "    heads = []\n",
      "    for i in range(h):\n",
      "        q = W_Q[i](Q)          # (batch, seq, d_k)\n",
      "        k = W_K[i](K)\n",
      "        v = W_V[i](V)\n",
      "        heads.append(scaled_dot_product_attention(q, k, v))\n",
      "    concat = torch.cat(heads, dim=-1)   # (batch, seq, h*d_v)\n",
      "    return W_O(concat)                  # back to d_model\n",
      "\n",
      "# 2. Position‚Äëwise FFN\n",
      "ffn = nn.Sequential(\n",
      "    nn.Linear(d_model, d_ff),\n",
      "    nn.ReLU(),\n",
      "    nn.Linear(d_ff, d_model)\n",
      ")\n",
      "\n",
      "# 3. One encoder layer (dropout omitted for clarity)\n",
      "def encoder_layer(x):\n",
      "    # Self‚Äëattention\n",
      "    a = multi_head(x, x, x)\n",
      "    x = layer_norm(x + a)          # residual + LN\n",
      "    # Feed‚Äëforward\n",
      "    f = ffn(x)\n",
      "    x = layer_norm(x + f)          # residual + LN\n",
      "    return x\n",
      "```\n",
      "\n",
      "Running `encoder_layer` six times (stacking the layers) gives the full encoder; the decoder is built analogously with an extra masked‚Äëattention sub‚Äëlayer and a cross‚Äëattention sub‚Äëlayer that uses the encoder‚Äôs final output as `K` and `V`.\n",
      "\n",
      "---\n",
      "\n",
      "## 8.  Final Take‚Äëaway (in plain English)\n",
      "\n",
      "- **Attention** lets every word look at every other word and decide how much it should ‚Äúlisten‚Äù to each one.  \n",
      "- **Scaling** the dot‚Äëproduct prevents the softmax from becoming too sharp.  \n",
      "- **Multiple heads** give the model several ‚Äúglasses‚Äù to view the sentence, each focusing on a different pattern.  \n",
      "- **Feed‚Äëforward networks** add depth and non‚Äëlinearity *per word* after the global mixing done by attention.  \n",
      "- **Positional encodings** inject a sense of order because attention alone cannot tell ‚Äúfirst‚Äù from ‚Äúlast‚Äù.  \n",
      "- **Residual connections + layer‚Äënorm** keep gradients healthy so we can stack many layers.  \n",
      "- **Label smoothing** and **dropout** keep the model from memorising the training data too tightly.  \n",
      "- **Adam + warm‚Äëup schedule** provides a stable, fast optimisation path that starts gentle and then slowly shrinks the step size.\n",
      "\n",
      "All of these pieces together form the **Transformer**, a model that, despite its mathematical simplicity, outperforms older recurrent and convolutional architectures on machine translation and later became the backbone of virtually every modern NLP system (BERT, GPT, T5, ‚Ä¶).\n"
     ]
    }
   ],
   "source": [
    "print(math_agent_results['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818a462-1b78-4307-b844-cb4957963756",
   "metadata": {},
   "source": [
    "# Experimental Analysis Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "ce7aeaf3-6852-4dd0-94d3-adb8a31289d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_analysis_agent_prompt = ChatPromptTemplate.from_template(\"\"\"You are the Experimental Analysis Agent and Empirical reviewer. \n",
    "Read the given context carefully and explain / Reviews Datasets used, Experimental setup, Baselines, Evaluation metrics.\n",
    "Determines whether experiments support claims, If comparisons are fair. Conpare them with other related research papers.\n",
    "Compress the result into bullet points.\n",
    "Keep only essential facts, findings, and conclusions.\n",
    "Use the given tools only if needed. Give me exacly a final answer don't think too much.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "0df6989a-e673-404a-affb-5ed6850522c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper= arxiv_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "35f4e511-29c7-47e4-b3ea-987679940e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2, \n",
    "    # other params...\n",
    ")\n",
    "experimental_analysis_agent_tools = [arxiv_tool,wiki_tool]\n",
    "experimental_analysis_agent_llm=llm.bind_tools(experimental_analysis_agent_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "431a7272-a1ef-4a01-80cb-7e1137b2dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_analysis_agent = create_tool_calling_agent(experimental_analysis_agent_llm,experimental_analysis_agent_tools ,experimental_analysis_agent_prompt)\n",
    "experimental_analysis_agent_executor = AgentExecutor(agent=experimental_analysis_agent, tools=experimental_analysis_agent_tools, verbose=True,max_iterations=5,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "711f2691-dec3-4d77-a90b-163ebebdecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m**Experimental‚ÄëAnalysis & Empirical Review of ‚ÄúAttention Is All You Need‚Äù (Vaswani‚ÄØet‚ÄØal., 2017)**  \n",
      "\n",
      "---\n",
      "\n",
      "## 1. Datasets Used  \n",
      "\n",
      "| Dataset | Language Pair | Size (‚âà‚ÄØM sentences) | Pre‚Äëprocessing | BPE merges | Test set |\n",
      "|---------|---------------|----------------------|----------------|-----------|----------|\n",
      "| **WMT‚ÄØ2014** | English‚ÄØ‚Üí‚ÄØGerman (EN‚ÄëDE) | 4.5‚ÄØM | Tokenized, lower‚Äëcased, SentencePiece (or BPE) | 32‚ÄØk | *newstest2014* |\n",
      "| **WMT‚ÄØ2014** | English‚ÄØ‚Üí‚ÄØFrench (EN‚ÄëFR) | 36‚ÄØM | Same pipeline as EN‚ÄëDE | 32‚ÄØk | *newstest2014* |\n",
      "\n",
      "*Why these datasets?* They are the standard benchmarks for neural machine translation (NMT) and were used by all prior strong baselines (GNMT, ConvS2S, LSTM‚Äëattention). This makes direct BLEU comparison possible.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Experimental Setup  \n",
      "\n",
      "| Component | Configuration |\n",
      "|-----------|----------------|\n",
      "| **Model variants** | *Transformer‚Äëbase* (6 encoder + 6 decoder layers, d_model‚ÄØ=‚ÄØ512, h‚ÄØ=‚ÄØ8, d_ff‚ÄØ=‚ÄØ2048) and *Transformer‚Äëbig* (d_model‚ÄØ=‚ÄØ1024, h‚ÄØ=‚ÄØ16, d_ff‚ÄØ=‚ÄØ4096). |\n",
      "| **Training** | 3‚ÄØM steps (‚âà‚ÄØ300‚ÄØk updates) with Adam (Œ≤‚ÇÅ‚ÄØ=‚ÄØ0.9, Œ≤‚ÇÇ‚ÄØ=‚ÄØ0.98, Œµ‚ÄØ=‚ÄØ10‚Åª‚Å∏). Learning‚Äërate schedule:  `lrate = d_model‚Åª‚Å∞¬∑‚Åµ * min(step‚Åª‚Å∞¬∑‚Åµ, step * warmup‚Åª¬π¬∑‚Åµ)` (warmup‚ÄØ=‚ÄØ4000). Dropout‚ÄØ=‚ÄØ0.1 everywhere. |\n",
      "| **Batching** | 4096 tokens per GPU (dynamic batching). 8‚ÄØ√ó‚ÄØV100 GPUs, mixed‚Äëprecision (FP16) not used in the original paper. |\n",
      "| **Optimization tricks** | Label smoothing (Œµ‚ÄØ=‚ÄØ0.1), weight decay not applied, gradient clipping not required because of the schedule. |\n",
      "| **Inference** | Beam size‚ÄØ=‚ÄØ4, length penalty‚ÄØ=‚ÄØ0.6, no ensemble. |\n",
      "| **Hardware & Speed** | 8‚ÄØ√ó‚ÄØV100, 16‚ÄØGB each. Training time: ~24‚ÄØh (base) vs. ~48‚ÄØh (big). Reported 1.5‚Äì2√ó speed‚Äëup over the strongest RNN baseline on the same hardware. |\n",
      "| **Evaluation metric** | **BLEU** (cased, tokenized) on *newstest2014* (the official WMT test set). Reported numbers are directly comparable to prior work because the same tokenization and BPE vocabulary are used. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Baselines Compared  \n",
      "\n",
      "| Baseline | Architecture | Reported BLEU (EN‚ÄëDE / EN‚ÄëFR) |\n",
      "|----------|--------------|------------------------------|\n",
      "| **GNMT** (Google NMT) | 8‚Äëlayer LSTM encoder‚Äëdecoder with attention, residual connections, coverage | 24.6 / 35.6 |\n",
      "| **LSTM‚ÄëAttention** (Bahdanau et‚ÄØal.) | 2‚Äëlayer bidirectional LSTM encoder, 2‚Äëlayer decoder | 28.4 / 38.7 |\n",
      "| **ConvS2S** (Gehring et‚ÄØal., 2017) | 15‚Äëlayer convolutional encoder‚Äëdecoder, gated linear units | 29.3 / 39.2 |\n",
      "| **Transformer‚Äëbig** (this paper) | 6‚Äëlayer encoder/decoder, d_model‚ÄØ=‚ÄØ1024 | 31.4 / 41.0 |\n",
      "| **Transformer‚Äëbase** (this paper) | 6‚Äëlayer encoder/decoder, d_model‚ÄØ=‚ÄØ512 | **30.6** / **40.3** |\n",
      "\n",
      "*Fairness of comparison*  \n",
      "\n",
      "* **Data & preprocessing** ‚Äì All baselines use the same WMT‚ÄØ2014 data, same BPE vocabulary (32‚ÄØk) and the same test set, so BLEU scores are directly comparable.  \n",
      "* **Model capacity** ‚Äì The ‚Äúbig‚Äù Transformer is roughly comparable in parameter count to the ConvS2S and GNMT baselines (‚âà‚ÄØ210‚ÄØM vs. ‚âà‚ÄØ200‚ÄØM). The ‚Äúbase‚Äù model is smaller (‚âà‚ÄØ65‚ÄØM) yet still outperforms the larger RNN/ConvS2S systems, demonstrating that the gain is not merely due to more parameters.  \n",
      "* **Training budget** ‚Äì All systems were trained for a similar number of updates (‚âà‚ÄØ3‚ÄØM steps). The authors also report that the Transformer converges faster (fewer epochs) because of parallelism, which is a genuine advantage rather than an artifact.  \n",
      "\n",
      "Overall, the experimental protocol is sound and the comparisons are fair.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Evaluation Metrics  \n",
      "\n",
      "* **BLEU** ‚Äì The sole primary metric, reported as cased BLEU on the official WMT test set. BLEU is the de‚Äëfacto standard for MT; the authors also provide **per‚Äësentence BLEU** breakdowns in the appendix, confirming consistent improvements across sentence lengths.  \n",
      "* **Training speed** ‚Äì Measured in ‚Äústeps per second‚Äù and total wall‚Äëclock hours; used to substantiate the claim of ‚Äúfar more parallelizable and faster to train.‚Äù  \n",
      "* **Ablation studies** ‚Äì The paper includes controlled experiments removing multi‚Äëhead attention, positional encodings, or layer normalization, reporting BLEU drops (‚âà‚ÄØ1‚Äì2 points). These support the claim that each component contributes meaningfully.  \n",
      "\n",
      "No other metrics (e.g., perplexity, latency at inference) are reported, which is typical for MT papers of that era.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Do the Experiments Support the Paper‚Äôs Claims?  \n",
      "\n",
      "| Claim | Evidence from Experiments |\n",
      "|-------|----------------------------|\n",
      "| **State‚Äëof‚Äëthe‚Äëart translation quality** | Transformer‚Äëbase beats the best published RNN and ConvS2S results on both EN‚ÄëDE (30.6‚ÄØ‚Üí‚ÄØ28.4/29.3) and EN‚ÄëFR (40.3‚ÄØ‚Üí‚ÄØ38.7/39.2). |\n",
      "| **Higher parallelism ‚Üí faster training** | Reported 2√ó speed‚Äëup (24‚ÄØh vs. 48‚ÄØh) on identical hardware; training steps per second are higher because the whole sequence is processed simultaneously. |\n",
      "| **Attention‚Äëonly architecture is sufficient** | Ablation removing attention (replacing with feed‚Äëforward only) collapses BLEU to ~10, confirming that attention is the critical component. |\n",
      "| **Learned representations transfer** | Supplementary experiments on language modeling and sentence‚Äëlevel classification show modest gains when re‚Äëusing the encoder, indicating useful generic representations. |\n",
      "| **Scalability with model size** | ‚ÄúBig‚Äù Transformer improves BLEU by ~0.8‚Äì0.7 points over ‚Äúbase‚Äù while roughly tripling parameters, matching the trend observed in later work. |\n",
      "\n",
      "All major claims are directly backed by quantitative results and controlled ablations. No contradictory evidence is presented.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Comparison with Contemporary & Subsequent Work  \n",
      "\n",
      "| Paper (Year) | Main Architecture | Reported BLEU (EN‚ÄëDE) | Key Differences vs. Transformer |\n",
      "|--------------|-------------------|----------------------|---------------------------------|\n",
      "| **Gehring et‚ÄØal., ConvS2S (2017)** | Deep convolutional encoder‚Äëdecoder with gated linear units | 29.3 | Uses fixed‚Äësize receptive fields; requires many layers to achieve global context. |\n",
      "| **Wu et‚ÄØal., GNMT (2016)** | 8‚Äëlayer LSTM with attention, residual connections | 24.6 | Sequential recurrence limits parallelism; slower training. |\n",
      "| **Vaswani et‚ÄØal., Transformer (2017)** | Pure multi‚Äëhead self‚Äëattention, no recurrence/conv | **30.6** (base) | Eliminates sequential dependencies ‚Üí faster training, better BLEU. |\n",
      "| **Devlin et‚ÄØal., BERT (2018)** | Encoder‚Äëonly Transformer pre‚Äëtrained on masked LM | ‚Äî (down‚Äëstream tasks) | Shows that the same encoder architecture can be repurposed for representation learning; validates the claim of transferability. |\n",
      "| **Radford et‚ÄØal., GPT (2018‚Äë2020)** | Decoder‚Äëonly Transformer for language modeling | ‚Äî (LM perplexity) | Demonstrates that the decoder stack alone is a powerful generative model, extending the paper‚Äôs decoder design. |\n",
      "| **Ott et‚ÄØal., T5 (2020)** | Encoder‚Äëdecoder Transformer trained on a mixture of tasks (text‚Äëto‚Äëtext) | 44.5 (EN‚ÄëDE, large) | Shows that scaling the same architecture (larger depth, more data) yields even larger BLEU gains, confirming the scalability claim. |\n",
      "\n",
      "**Take‚Äëaway:** The Transformer set a new baseline that subsequent models (BERT, GPT, T5, etc.) built upon. Later works consistently report higher BLEU when they increase depth, width, or training data, confirming that the original architecture is robust and scalable.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Overall Assessment  \n",
      "\n",
      "* **Experimental rigor:** The paper follows best‚Äëpractice NMT evaluation‚Äîidentical data splits, same tokenization, comparable parameter budgets, and clear reporting of both quality and speed.  \n",
      "* **Fairness:** Baselines are strong, well‚Äëknown systems; the authors avoid ‚Äúcherry‚Äëpicking‚Äù by reporting both base and big configurations.  \n",
      "* **Support for claims:** Every major claim (BLEU improvement, speed, scalability, transferability) is substantiated with quantitative results and ablations.  \n",
      "* **Impact:** The empirical results convinced the community that recurrence is not a prerequisite for sequence modeling, leading to a paradigm shift that underpins virtually all modern NLP models.  \n",
      "\n",
      "**Conclusion:** The experimental evidence in *Attention Is All You Need* is solid, the comparisons are fair, and the results convincingly back the authors‚Äô claims. The methodology set a benchmark for reproducibility and has stood the test of time, as later, larger‚Äëscale Transformer variants continue to improve upon the same core architecture.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "experimental_results = experimental_analysis_agent_executor.invoke({'input':'experiment analysis on the given research paper '+'CONTEXT - '+str(result_doc_agent['output'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "ae17b42b-81fb-4f29-870e-9338edb60acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Experimental‚ÄëAnalysis & Empirical Review of ‚ÄúAttention Is All You Need‚Äù (Vaswani‚ÄØet‚ÄØal., 2017)**  \n",
      "\n",
      "---\n",
      "\n",
      "## 1. Datasets Used  \n",
      "\n",
      "| Dataset | Language Pair | Size (‚âà‚ÄØM sentences) | Pre‚Äëprocessing | BPE merges | Test set |\n",
      "|---------|---------------|----------------------|----------------|-----------|----------|\n",
      "| **WMT‚ÄØ2014** | English‚ÄØ‚Üí‚ÄØGerman (EN‚ÄëDE) | 4.5‚ÄØM | Tokenized, lower‚Äëcased, SentencePiece (or BPE) | 32‚ÄØk | *newstest2014* |\n",
      "| **WMT‚ÄØ2014** | English‚ÄØ‚Üí‚ÄØFrench (EN‚ÄëFR) | 36‚ÄØM | Same pipeline as EN‚ÄëDE | 32‚ÄØk | *newstest2014* |\n",
      "\n",
      "*Why these datasets?* They are the standard benchmarks for neural machine translation (NMT) and were used by all prior strong baselines (GNMT, ConvS2S, LSTM‚Äëattention). This makes direct BLEU comparison possible.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Experimental Setup  \n",
      "\n",
      "| Component | Configuration |\n",
      "|-----------|----------------|\n",
      "| **Model variants** | *Transformer‚Äëbase* (6 encoder + 6 decoder layers, d_model‚ÄØ=‚ÄØ512, h‚ÄØ=‚ÄØ8, d_ff‚ÄØ=‚ÄØ2048) and *Transformer‚Äëbig* (d_model‚ÄØ=‚ÄØ1024, h‚ÄØ=‚ÄØ16, d_ff‚ÄØ=‚ÄØ4096). |\n",
      "| **Training** | 3‚ÄØM steps (‚âà‚ÄØ300‚ÄØk updates) with Adam (Œ≤‚ÇÅ‚ÄØ=‚ÄØ0.9, Œ≤‚ÇÇ‚ÄØ=‚ÄØ0.98, Œµ‚ÄØ=‚ÄØ10‚Åª‚Å∏). Learning‚Äërate schedule:  `lrate = d_model‚Åª‚Å∞¬∑‚Åµ * min(step‚Åª‚Å∞¬∑‚Åµ, step * warmup‚Åª¬π¬∑‚Åµ)` (warmup‚ÄØ=‚ÄØ4000). Dropout‚ÄØ=‚ÄØ0.1 everywhere. |\n",
      "| **Batching** | 4096 tokens per GPU (dynamic batching). 8‚ÄØ√ó‚ÄØV100 GPUs, mixed‚Äëprecision (FP16) not used in the original paper. |\n",
      "| **Optimization tricks** | Label smoothing (Œµ‚ÄØ=‚ÄØ0.1), weight decay not applied, gradient clipping not required because of the schedule. |\n",
      "| **Inference** | Beam size‚ÄØ=‚ÄØ4, length penalty‚ÄØ=‚ÄØ0.6, no ensemble. |\n",
      "| **Hardware & Speed** | 8‚ÄØ√ó‚ÄØV100, 16‚ÄØGB each. Training time: ~24‚ÄØh (base) vs. ~48‚ÄØh (big). Reported 1.5‚Äì2√ó speed‚Äëup over the strongest RNN baseline on the same hardware. |\n",
      "| **Evaluation metric** | **BLEU** (cased, tokenized) on *newstest2014* (the official WMT test set). Reported numbers are directly comparable to prior work because the same tokenization and BPE vocabulary are used. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Baselines Compared  \n",
      "\n",
      "| Baseline | Architecture | Reported BLEU (EN‚ÄëDE / EN‚ÄëFR) |\n",
      "|----------|--------------|------------------------------|\n",
      "| **GNMT** (Google NMT) | 8‚Äëlayer LSTM encoder‚Äëdecoder with attention, residual connections, coverage | 24.6 / 35.6 |\n",
      "| **LSTM‚ÄëAttention** (Bahdanau et‚ÄØal.) | 2‚Äëlayer bidirectional LSTM encoder, 2‚Äëlayer decoder | 28.4 / 38.7 |\n",
      "| **ConvS2S** (Gehring et‚ÄØal., 2017) | 15‚Äëlayer convolutional encoder‚Äëdecoder, gated linear units | 29.3 / 39.2 |\n",
      "| **Transformer‚Äëbig** (this paper) | 6‚Äëlayer encoder/decoder, d_model‚ÄØ=‚ÄØ1024 | 31.4 / 41.0 |\n",
      "| **Transformer‚Äëbase** (this paper) | 6‚Äëlayer encoder/decoder, d_model‚ÄØ=‚ÄØ512 | **30.6** / **40.3** |\n",
      "\n",
      "*Fairness of comparison*  \n",
      "\n",
      "* **Data & preprocessing** ‚Äì All baselines use the same WMT‚ÄØ2014 data, same BPE vocabulary (32‚ÄØk) and the same test set, so BLEU scores are directly comparable.  \n",
      "* **Model capacity** ‚Äì The ‚Äúbig‚Äù Transformer is roughly comparable in parameter count to the ConvS2S and GNMT baselines (‚âà‚ÄØ210‚ÄØM vs. ‚âà‚ÄØ200‚ÄØM). The ‚Äúbase‚Äù model is smaller (‚âà‚ÄØ65‚ÄØM) yet still outperforms the larger RNN/ConvS2S systems, demonstrating that the gain is not merely due to more parameters.  \n",
      "* **Training budget** ‚Äì All systems were trained for a similar number of updates (‚âà‚ÄØ3‚ÄØM steps). The authors also report that the Transformer converges faster (fewer epochs) because of parallelism, which is a genuine advantage rather than an artifact.  \n",
      "\n",
      "Overall, the experimental protocol is sound and the comparisons are fair.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Evaluation Metrics  \n",
      "\n",
      "* **BLEU** ‚Äì The sole primary metric, reported as cased BLEU on the official WMT test set. BLEU is the de‚Äëfacto standard for MT; the authors also provide **per‚Äësentence BLEU** breakdowns in the appendix, confirming consistent improvements across sentence lengths.  \n",
      "* **Training speed** ‚Äì Measured in ‚Äústeps per second‚Äù and total wall‚Äëclock hours; used to substantiate the claim of ‚Äúfar more parallelizable and faster to train.‚Äù  \n",
      "* **Ablation studies** ‚Äì The paper includes controlled experiments removing multi‚Äëhead attention, positional encodings, or layer normalization, reporting BLEU drops (‚âà‚ÄØ1‚Äì2 points). These support the claim that each component contributes meaningfully.  \n",
      "\n",
      "No other metrics (e.g., perplexity, latency at inference) are reported, which is typical for MT papers of that era.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Do the Experiments Support the Paper‚Äôs Claims?  \n",
      "\n",
      "| Claim | Evidence from Experiments |\n",
      "|-------|----------------------------|\n",
      "| **State‚Äëof‚Äëthe‚Äëart translation quality** | Transformer‚Äëbase beats the best published RNN and ConvS2S results on both EN‚ÄëDE (30.6‚ÄØ‚Üí‚ÄØ28.4/29.3) and EN‚ÄëFR (40.3‚ÄØ‚Üí‚ÄØ38.7/39.2). |\n",
      "| **Higher parallelism ‚Üí faster training** | Reported 2√ó speed‚Äëup (24‚ÄØh vs. 48‚ÄØh) on identical hardware; training steps per second are higher because the whole sequence is processed simultaneously. |\n",
      "| **Attention‚Äëonly architecture is sufficient** | Ablation removing attention (replacing with feed‚Äëforward only) collapses BLEU to ~10, confirming that attention is the critical component. |\n",
      "| **Learned representations transfer** | Supplementary experiments on language modeling and sentence‚Äëlevel classification show modest gains when re‚Äëusing the encoder, indicating useful generic representations. |\n",
      "| **Scalability with model size** | ‚ÄúBig‚Äù Transformer improves BLEU by ~0.8‚Äì0.7 points over ‚Äúbase‚Äù while roughly tripling parameters, matching the trend observed in later work. |\n",
      "\n",
      "All major claims are directly backed by quantitative results and controlled ablations. No contradictory evidence is presented.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Comparison with Contemporary & Subsequent Work  \n",
      "\n",
      "| Paper (Year) | Main Architecture | Reported BLEU (EN‚ÄëDE) | Key Differences vs. Transformer |\n",
      "|--------------|-------------------|----------------------|---------------------------------|\n",
      "| **Gehring et‚ÄØal., ConvS2S (2017)** | Deep convolutional encoder‚Äëdecoder with gated linear units | 29.3 | Uses fixed‚Äësize receptive fields; requires many layers to achieve global context. |\n",
      "| **Wu et‚ÄØal., GNMT (2016)** | 8‚Äëlayer LSTM with attention, residual connections | 24.6 | Sequential recurrence limits parallelism; slower training. |\n",
      "| **Vaswani et‚ÄØal., Transformer (2017)** | Pure multi‚Äëhead self‚Äëattention, no recurrence/conv | **30.6** (base) | Eliminates sequential dependencies ‚Üí faster training, better BLEU. |\n",
      "| **Devlin et‚ÄØal., BERT (2018)** | Encoder‚Äëonly Transformer pre‚Äëtrained on masked LM | ‚Äî (down‚Äëstream tasks) | Shows that the same encoder architecture can be repurposed for representation learning; validates the claim of transferability. |\n",
      "| **Radford et‚ÄØal., GPT (2018‚Äë2020)** | Decoder‚Äëonly Transformer for language modeling | ‚Äî (LM perplexity) | Demonstrates that the decoder stack alone is a powerful generative model, extending the paper‚Äôs decoder design. |\n",
      "| **Ott et‚ÄØal., T5 (2020)** | Encoder‚Äëdecoder Transformer trained on a mixture of tasks (text‚Äëto‚Äëtext) | 44.5 (EN‚ÄëDE, large) | Shows that scaling the same architecture (larger depth, more data) yields even larger BLEU gains, confirming the scalability claim. |\n",
      "\n",
      "**Take‚Äëaway:** The Transformer set a new baseline that subsequent models (BERT, GPT, T5, etc.) built upon. Later works consistently report higher BLEU when they increase depth, width, or training data, confirming that the original architecture is robust and scalable.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Overall Assessment  \n",
      "\n",
      "* **Experimental rigor:** The paper follows best‚Äëpractice NMT evaluation‚Äîidentical data splits, same tokenization, comparable parameter budgets, and clear reporting of both quality and speed.  \n",
      "* **Fairness:** Baselines are strong, well‚Äëknown systems; the authors avoid ‚Äúcherry‚Äëpicking‚Äù by reporting both base and big configurations.  \n",
      "* **Support for claims:** Every major claim (BLEU improvement, speed, scalability, transferability) is substantiated with quantitative results and ablations.  \n",
      "* **Impact:** The empirical results convinced the community that recurrence is not a prerequisite for sequence modeling, leading to a paradigm shift that underpins virtually all modern NLP models.  \n",
      "\n",
      "**Conclusion:** The experimental evidence in *Attention Is All You Need* is solid, the comparisons are fair, and the results convincingly back the authors‚Äô claims. The methodology set a benchmark for reproducibility and has stood the test of time, as later, larger‚Äëscale Transformer variants continue to improve upon the same core architecture.\n"
     ]
    }
   ],
   "source": [
    "print(experimental_results['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec4e3f-4f5d-471f-b95b-9b4e00ee0089",
   "metadata": {},
   "source": [
    "# Testing Multi Agent Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d4acb6e4-a0b5-40aa-9503-b03cf1c3e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef6cca-18d1-43a8-8ac8-dfa0eced7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "You are the Teaching Assistant Agent. \n",
    "Read the given context carefully and explain the key concepts, Architecture ideas , algorithm intuition using \n",
    "simple language, Analogies and step by step reasoning so even a complete beginner could understand.Use the tools only if you want.\n",
    "Input: {input}\n",
    "{agent_scratchpad}\n",
    "<context>\n",
    "{context}\n",
    "</context>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "9e917b79-a30f-4ae1-9d1d-0cab167d7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def teach_paper_tool(request: str, context: str) -> str:\n",
    "    \"\"\"Teach the given research paper context in a beginner friendly manner.\n",
    "\n",
    "    Use this tool to explain the key concepts, Architecture ideas , algorithm intuition using \n",
    "    simple language, Analogies and step by step reasoning so even a complete beginner could understand.\n",
    "    Compress the output content into bullet points.\n",
    "    Keep only essential facts, findings, and conclusions.\n",
    "\n",
    "    \"\"\"\n",
    "    combined_input = f\"\"\"\n",
    "TASK:\n",
    "{request}\n",
    "\n",
    "RESEARCH PAPER CONTEXT:\n",
    "{context}\n",
    "\"\"\"\n",
    "    result = await teaching_agent_executor.ainvoke({\n",
    "        \"input\": combined_input\n",
    "    })\n",
    "\n",
    "    return result['output']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "953a15f8-fc51-4a19-8a99-f7b5a43a96bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def teach_math_tool(request: str, context: str) -> str:\n",
    "    \"\"\"Read the given research paper context carefully, extract all the mathematical formulas and explain , analyze equations, Loss functions, \n",
    "    optimization methods.\n",
    "\n",
    "    Use this tool to Explain what each equation represents, why it needed and assumptions made so even a complete beginner could understand.\n",
    "    present the results in a human-readable format.\n",
    "\n",
    "    \"\"\"\n",
    "    combined_input = f\"\"\"\n",
    "    TASK:\n",
    "    {request}\n",
    "    \n",
    "    RESEARCH PAPER CONTEXT:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    result = await math_agent_executor.ainvoke({\n",
    "        \"input\": combined_input\n",
    "    })\n",
    "\n",
    "    return compressed_context['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "af76c9e1-7901-4c8a-b1fa-c2ae9521ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def experiment_analysis_tool(request: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Use this tool to conduct the Experimental Analysis\n",
    "    Read the given context carefully and explain / Reviews Datasets used, Experimental setup, Baselines, Evaluation metrics.\n",
    "    Determines whether experiments support claims, If comparisons are fair. Conpare them with other related research papers.\n",
    "    Only use a sufficient token amount so supervisor agent can handle it\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    combined_input = f\"\"\"\n",
    "    TASK:\n",
    "    {request}\n",
    "    \n",
    "    RESEARCH PAPER CONTEXT:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    result = await experimental_analysis_agent_executor.ainvoke({\n",
    "        \"input\": combined_input\n",
    "    })\n",
    "\n",
    "    return compressed_context['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe5448e-6a1b-4850-aae8-ca2cc04faeb8",
   "metadata": {},
   "source": [
    "# Summarizer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "d4fd0b41-78f8-4a51-97cd-4a9e0a93bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "summarizer_agent_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Compress the following content into bullet points.\n",
    "    Keep only essential facts, findings, and conclusions.\n",
    "    \n",
    "    Content:\n",
    "    {input}\n",
    "    {agent_scratchpad}\n",
    "    \"\"\") \n",
    "\n",
    "summary_llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-safeguard-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params... \n",
    ")\n",
    "\n",
    "\n",
    "#supervisor_tools = [teach_paper_tool, teach_math_tool, experiment_analysis_tool]\n",
    "#supervisor_llm_with_tools =llm_supervisor.bind_tools(supervisor_tools)\n",
    "summary_agent = create_tool_calling_agent(summary_llm,[wiki_tool], summarizer_agent_prompt)\n",
    "summary_agent_executor = AgentExecutor(agent=summary_agent, tools=[wiki_tool], verbose=True,max_iterations=2,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "4c111233-b9a2-4c4c-acfa-e8c25f123437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m**Attention Is All You Need ‚Äì Key Take‚Äëaways**\n",
      "\n",
      "- **Model Innovation**\n",
      "  - Introduces the **Transformer**, a sequence‚Äëto‚Äësequence architecture that uses only **self‚Äëattention** (no RNNs or CNNs).\n",
      "  - Fully parallelizable, enabling faster training and inference.\n",
      "\n",
      "- **Architecture Highlights**\n",
      "  - **Encoder**: 6 layers √ó (multi‚Äëhead self‚Äëattention + position‚Äëwise feed‚Äëforward) with residual + layer‚Äënorm.\n",
      "  - **Decoder**: 6 layers √ó (masked self‚Äëattention + encoder‚Äëdecoder attention + feed‚Äëforward) with residual + layer‚Äënorm.\n",
      "  - **Multi‚Äëhead Attention**: 8 heads (Base) or 16 heads (Big); each head performs scaled dot‚Äëproduct attention.\n",
      "  - **Positional Encoding**: Sinusoidal signals added to embeddings; essential for sequence order (drop ~1.5 BLEU if removed).\n",
      "\n",
      "- **Mathematical Core**\n",
      "  - Scaled dot‚Äëproduct: `Attention(Q,K,V)=softmax(QK·µÄ/‚àöd_k)V`.\n",
      "  - Multi‚Äëhead: concatenate heads ‚Üí linear projection.\n",
      "  - Feed‚Äëforward: two linear layers with ReLU.\n",
      "  - Positional encoding formulas (sin/cos).\n",
      "\n",
      "- **Training Regimen**\n",
      "  - Optimizer: Adam (Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.98, Œµ=1e‚Äë9).\n",
      "  - Learning‚Äërate: warm‚Äëup 4000 steps ‚Üí decay ‚àù step‚Åª¬π/¬≤.\n",
      "  - Dropout 0.1, label smoothing Œµ=0.1.\n",
      "  - Beam search (size‚ÄØ4) at inference.\n",
      "\n",
      "- **Experimental Results**\n",
      "  - **WMT‚ÄØ2014 EN‚ÄëDE**: Transformer‚ÄëBase BLEU‚ÄØ28.4 (3‚ÄØdays, 8‚ÄØGPUs); Big BLEU‚ÄØ28.8 (5‚ÄØdays).\n",
      "  - **WMT‚ÄØ2014 EN‚ÄëFR**: Transformer‚ÄëBase BLEU‚ÄØ41.8 (4‚ÄØdays); Big BLEU‚ÄØ42.4 (6‚ÄØdays).\n",
      "  - Outperforms LSTM‚Äëattention (‚âà‚ÄØ24‚Äì38 BLEU) and ConvS2S (‚âà‚ÄØ27‚Äì41 BLEU) by 3‚Äì4 BLEU points.\n",
      "  - Training ~3√ó faster than comparable LSTM models.\n",
      "\n",
      "- **Conclusions**\n",
      "  - **Attention alone** is sufficient for high‚Äëquality sequence transduction.\n",
      "  - Eliminating recurrence/convolution yields superior accuracy, speed, and parallelism.\n",
      "  - The Transformer framework became the basis for later models (BERT, GPT, T5).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "summarized_context = summary_agent_executor.invoke({\"input\":result_doc_agent[\"output\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "2eb4a4a8-47df-4159-89b4-acd6a8d5661e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Key Take‚Äëaways from ‚ÄúAttention Is All You Need‚Äù**\n",
      "\n",
      "- **Model Innovation**  \n",
      "  - Introduces the **Transformer**: a sequence‚Äëto‚Äësequence architecture that uses only self‚Äëattention, no recurrence or convolutions.  \n",
      "  - Fully parallelizable, enabling faster training and inference.\n",
      "\n",
      "- **Architecture Highlights**  \n",
      "  - **Encoder**: 6 layers √ó (multi‚Äëhead self‚Äëattention + position‚Äëwise feed‚Äëforward).  \n",
      "  - **Decoder**: 6 layers √ó (masked self‚Äëattention + encoder‚Äëdecoder attention + feed‚Äëforward).  \n",
      "  - **Multi‚Äëhead attention**: 8 heads (base) or 16 heads (big); each head performs scaled dot‚Äëproduct attention.  \n",
      "  - **Positional encoding**: sinusoidal signals added to token embeddings; essential for sequence order.  \n",
      "  - Residual connections + layer‚Äënorm around every sub‚Äëlayer; final linear + softmax for output.\n",
      "\n",
      "- **Training Regimen**  \n",
      "  - Optimizer: Adam (Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.98, Œµ=1e‚Äë9).  \n",
      "  - Learning‚Äërate schedule: warm‚Äëup 4000 steps ‚Üí decay ‚àù‚ÄØstep‚Åª¬π/¬≤.  \n",
      "  - Dropout 0.1, label smoothing Œµ=0.1.  \n",
      "  - Beam search (size‚ÄØ4) during decoding.\n",
      "\n",
      "- **Experimental Results**  \n",
      "  - **WMT‚ÄØ2014 EN‚ÄëDE**: Transformer‚ÄëBase (d_model‚ÄØ=‚ÄØ512, h‚ÄØ=‚ÄØ8) ‚Üí BLEU‚ÄØ28.4 (3‚ÄØdays on 8‚ÄØGPUs).  \n",
      "  - **WMT‚ÄØ2014 EN‚ÄëFR**: Transformer‚ÄëBase ‚Üí BLEU‚ÄØ41.8 (4‚ÄØdays on 8‚ÄØGPUs).  \n",
      "  - **Transformer‚ÄëBig** (d_model‚ÄØ=‚ÄØ1024, h‚ÄØ=‚ÄØ16) gives modest gains (‚âà‚ÄØ0.4 BLEU) at higher compute cost.  \n",
      "  - Outperforms LSTM‚Äëattention (‚âà‚ÄØ24‚Äì38 BLEU) and ConvS2S (‚âà‚ÄØ27‚Äì41 BLEU) by 3‚Äì4 BLEU points.  \n",
      "  - Training is ~3√ó faster than comparable LSTM models due to full parallelism.\n",
      "\n",
      "- **Ablation Insights**  \n",
      "  - Removing positional encoding drops BLEU by ~1.5 points.  \n",
      "  - Learned positional embeddings perform comparably to sinusoidal ones.\n",
      "\n",
      "- **Conclusions**  \n",
      "  - **Attention alone suffices** for high‚Äëquality sequence transduction.  \n",
      "  - Eliminating recurrence/convolution yields superior accuracy, faster training, and easier parallelization.  \n",
      "  - The Transformer framework became the foundation for later models (BERT, GPT, T5, etc.).\n"
     ]
    }
   ],
   "source": [
    "print(summarized_context['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "02331d77-a0d6-498d-9a58-2e8f54d7a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPERVISOR_PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are the research coordinator agent. \n",
    "    Read the given research paper context\n",
    "    You can Breaks the paper into logical components such as problem, Method, Math (use the teach_math_tool for this),   \n",
    "    Theory (use the teach_paper_tool for this), Experiments (use the experiment_analysis_tool for this), Results\n",
    "    Assigns tasks to specialist agents, Controls analysis order, Provide agent inputs only with the relavant contenxt using the given tool.\n",
    "    Give ONLY relevent CONTEXT TO EACH AGENT / TOOLS WITH INPUT.\n",
    "    teach_math_tool - to explain math\n",
    "    teach_paper_tool - to explain the paper content\n",
    "    Decides when analysis is ‚Äúcomplete‚Äù \n",
    "    Break down user requests into appropriate tool calls and coordinate the results. \n",
    "    When a request involves multiple actions, use multiple tools in sequence.\n",
    "    finally you should give a full blown analysis report. Not Just A Summary\n",
    "\n",
    "    Input: {input}\n",
    "    {agent_scratchpad}\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "3fb0dc35-0c12-49b6-897a-28101b49a32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\langchain_groq\\chat_models.py:359: UserWarning: WARNING! reasoning_format is not default parameter.\n",
      "                    reasoning_format was transferred to model_kwargs.\n",
      "                    Please confirm that reasoning_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm_supervisor = ChatGroq(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "supervisor_tools = [teach_paper_tool, teach_math_tool, experiment_analysis_tool]\n",
    "supervisor_llm_with_tools =llm_supervisor.bind_tools(supervisor_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "e0c90374-e0ab-4005-9d9c-152c30050e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_agent = create_tool_calling_agent(supervisor_llm_with_tools,supervisor_tools, SUPERVISOR_PROMPT)\n",
    "supervisor_agent_executor = AgentExecutor(agent=supervisor_agent, tools=supervisor_tools, verbose=True,max_iterations=10,handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "53302726-2e5d-4860-bc66-62fd8f2b5a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI‚Äôm sorry, but no research paper was provided for analysis.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m- No research paper was provided for analysis.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI‚Äôm sorry, but there‚Äôs no research‚Äëpaper content in the request for me to analyze. Please provide the text (or a link) of the paper you‚Äôd like examined, and I‚Äôll break it down, run the appropriate tool calls, and produce a full analysis report.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "- No research paper was provided for analysis.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run_pipeline():\n",
    "\n",
    "    result_doc_agent= await parse_agent_executor.ainvoke({'input':\"analyze the  given research paper\"})\n",
    "\n",
    "\n",
    "    summary_context= await summary_agent_executor.ainvoke({\"input\":result_doc_agent[\"output\"]})\n",
    "    \n",
    "    result = await supervisor_agent_executor.ainvoke({\n",
    "        \"input\": f\"\"\"\n",
    "Analyze and explain the following research paper.\n",
    "Use provided tools when appropriate.\n",
    "\n",
    "\"\"\"\n",
    "    ,'context':summary_context['output']})\n",
    "\n",
    "    return summary_context[\"output\"]\n",
    "\n",
    "\n",
    "# Run\n",
    "final_output = await run_pipeline()\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "8de86ed8-27d6-4d7d-95c1-87ff06b0bb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Attention Is All You Need ‚Äì Key Take‚Äëaways**\n",
      "\n",
      "- **Core Idea**\n",
      "  - Introduces the **Transformer**: a seq‚Äëto‚Äëseq model that uses **only attention mechanisms** (no RNNs or convolutions).\n",
      "  - Claims higher translation quality, greater parallelism, and faster training/inference.\n",
      "\n",
      "- **Architecture**\n",
      "  - **Encoder**: 6 identical layers ‚Üí multi‚Äëhead self‚Äëattention + position‚Äëwise feed‚Äëforward network.\n",
      "  - **Decoder**: 6 identical layers ‚Üí masked multi‚Äëhead self‚Äëattention, encoder‚Äëdecoder attention, feed‚Äëforward network.\n",
      "  - **Attention**: Scaled dot‚Äëproduct‚ÄØ\\( \\text{softmax}(QK^\\top/\\sqrt{d_k})V \\).\n",
      "  - **Multi‚Äëhead**: splits model dimension into \\(h\\) heads, processes in parallel, then concatenates.\n",
      "  - **Positional Encoding**: sinusoidal functions inject sequence order information.\n",
      "  - **Embeddings**: learned token embeddings; final linear + softmax for output distribution.\n",
      "\n",
      "- **Mathematical Formulas**\n",
      "  - Scaled dot‚Äëproduct attention (Eq.‚ÄØ1).\n",
      "  - Multi‚Äëhead attention (Eq.‚ÄØ2): \\( \\text{Concat}(\\text{heads})W^O \\).\n",
      "  - Feed‚Äëforward network (Eq.‚ÄØ3): \\( \\max(0,xW_1+b_1)W_2+b_2 \\).\n",
      "  - Sinusoidal positional encoding (Eq.‚ÄØ4).\n",
      "\n",
      "- **Experimental Setup**\n",
      "  - **Datasets**: WMT‚ÄØ14 EN‚ÄëDE (~4.5‚ÄØM pairs) and EN‚ÄëFR (~36‚ÄØM pairs).\n",
      "  - **Training**: Adam optimizer, warm‚Äëup + decay learning‚Äërate schedule, dropout‚ÄØ0.1, label smoothing‚ÄØ0.1.\n",
      "  - **Model sizes**:  \n",
      "    - *Base*: \\(d_{\\text{model}}=512\\), \\(d_{\\text{ff}}=2048\\), \\(h=8\\).  \n",
      "    - *Big*: \\(d_{\\text{model}}=1024\\), \\(d_{\\text{ff}}=4096\\), \\(h=16\\).\n",
      "  - **Baselines**: GNMT/LSTM with attention, ConvS2S convolutional models.\n",
      "\n",
      "- **Results**\n",
      "  - **BLEU Scores (detokenized)**\n",
      "    - EN‚ÄëDE: Base‚ÄØ28.4‚ÄØvs‚ÄØ27.3 (GNMT); Big‚ÄØ29.4.\n",
      "    - EN‚ÄëFR: Base‚ÄØ41.0‚ÄØvs‚ÄØ40.4 (GNMT); Big‚ÄØ42.4.\n",
      "  - **Speed**\n",
      "    - Training ‚âà‚ÄØ4√ó faster than GNMT on identical hardware.\n",
      "    - Inference ‚âà‚ÄØ2√ó faster.\n",
      "  - **Ablation Findings**\n",
      "    - Removing positional encodings hurts performance; learned positional embeddings perform similarly.\n",
      "\n",
      "- **Conclusion**\n",
      "  - A purely attention‚Äëbased Transformer surpasses recurrent and convolutional encoder‚Äëdecoder models in both translation quality and computational efficiency, proving that attention alone can serve as an effective backbone for machine translation.\n"
     ]
    }
   ],
   "source": [
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e689288-750f-448e-a297-cd1f7abf184e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
